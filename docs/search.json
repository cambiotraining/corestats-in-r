[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform core data analysis techniques appropriately confidently using R.6 lecture-practicals6 lecture-practicalsOngoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey ‚Äúmindlessly use stats program‚Äù course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented arbitrary dataset e.g.Know data analysis techniques availableKnow ones allowableBe able carry understand results","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Simple hypothesis testingCategorical predictor variablesContinuous predictorsTwo predictor variablesMultiple predictor variablesPower analysis","code":""},{"path":"index.html","id":"practicals","chapter":"1 Overview","heading":"1.3 Practicals","text":"practical document divided various sections. section explanatory text help understand going ‚Äôre trying achieve.\nmay list commands relevant section displayed boxes like :Conditional operatorsTo set filtering conditions, use following relational operators:> greater >= greater equal < less <= less equal == equal != different %% contained combine conditions, use following logical operators:& | ","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.4 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save ‚Ä¶. Next unzip file copy working directory. data accessible via <working-directory-name>/data/tidy.","code":""},{},{"path":"cs1-intro.html","id":"cs1-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"cs1-intro.html","id":"objectives","chapter":"2 Introduction","heading":"2.1 Objectives","text":"Aim: carry basic one two sample statistical tests.end section practical participants able achieve following listed tests:Understand purpose test isPerform test RInterpret test outputUnderstand assumptions/conditions test appropriateCheck assumptionsThe tests covered practical :One-sample tests\nOne sample t-test\nOne-sample Wilcoxon signed-rank test\nOne sample t-testOne-sample Wilcoxon signed-rank testTwo-sample tests\nStudent‚Äôs t-test\nMann-Whitney U test\nPaired two-sample t-test\nWilcoxon signed-rank test\nStudent‚Äôs t-testMann-Whitney U testPaired two-sample t-testWilcoxon signed-rank test","code":""},{"path":"cs1-intro.html","id":"background","chapter":"2 Introduction","heading":"2.2 Background","text":"practical focus underlying mathematical theory tests although demonstrators happy answer questions.\ntest section explaining purpose, section explaining perform test R, section explaining results output screen, section covering assumptions required perform test.","code":""},{},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction","heading":"3 Introduction","text":"","code":""},{"path":"introduction.html","id":"cs1-one-sample-tests","chapter":"3 Introduction","heading":"3.1 One-sample tests","text":"","code":""},{"path":"introduction.html","id":"objectives-one-sample","chapter":"3 Introduction","heading":"3.1.1 Objectives (one-sample)","text":"QuestionsWhen perform one-sample test?one-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis single sample continuous dataBe able summarise visualise data RUnderstand assess underlying assumptions testsPerform one-sample t-test Wilcoxon signed-rank test RKnow test appropriate whenBe able interpret report results","code":""},{"path":"introduction.html","id":"purpose-and-aim","chapter":"3 Introduction","heading":"3.1.2 Purpose and aim","text":"tests used single sample continuous data. used find sample came parent distribution given mean (median). essentially boils finding sample mean (median) ‚Äúclose enough‚Äù hypothesised parent population mean (median).\n, figure , use tests see probability sample ten points comes distribution plotted .e.¬†population mean 20 mm.","code":""},{"path":"introduction.html","id":"choosing-a-test","chapter":"3 Introduction","heading":"3.1.3 Choosing a test","text":"two tests going look situation; one-sample t-test, one-sample Wilcoxon signed rank test. tests work sort data ‚Äôre considering , different assumptions.data normally distributed, one-sample t-test appropriate. data aren‚Äôt normally distributed, distribution symmetric, sample size small one-sample Wilcoxon signed rank test appropriate.statistical test consider five tasks. come back , pay extra close attention.Setting hypothesisSummarise visualisation dataAssessment assumptionsImplementation statistical testInterpreting output presentation resultsWe won‚Äôt always carry exactly order, always consider five tasks every test.","code":""},{"path":"introduction.html","id":"cs1-two-sample","chapter":"3 Introduction","heading":"3.2 Two-sample tests","text":"","code":""},{"path":"introduction.html","id":"objectives-two-sample","chapter":"3 Introduction","heading":"3.2.1 Objectives (two-sample)","text":"QuestionsWhen perform two-sample test?two-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis two-sample continuous dataDetermine correct data format perform two-sample test RSummarise visualise dataCheck underlying assumptions (normality, homogeneity variance)able choose appropriate two-sample test run RBe able interpret report results","code":""},{"path":"introduction.html","id":"purpose-and-aim-1","chapter":"3 Introduction","heading":"3.2.2 Purpose and aim","text":"tests used two samples continuous data trying find samples came parent distribution . essentially boils finding difference means two samples.","code":""},{"path":"introduction.html","id":"two-sample-choosing-a-test","chapter":"3 Introduction","heading":"3.2.3 Choosing a test","text":"five key tests can used deal two samples. Choosing test use depends upon key assumptions satisfied sample data effectively boils answering four questions samples:samples normally distributed? (Yes/)big samples? (<30 data points >30 data points)samples paired? (Yes/)samples variance? (Yes/)two sets tests consider depending answers questions 1 2. data normally distributed big samples need look parametric tests. data normally distributed sample size small, need look non-parametric tests (see Figure 3.1. Questions 3 4 help pick specific test use, summarised Figure 3.2.\nFigure 3.1: Category test\n\nFigure 3.2: test use\nTesting whether sample comes normal distribution covered One-sample tests. need visualise data /use Shapiro-Wilk test.size sample makes things easier. maths (specifically due something called central limit theorem even going attempt touch upon ) large samples can use tests assume normality parent population (Student‚Äôs t-test, Welch‚Äôs t-test paired t-test) even parent populations certainly normal. really want understand exactly works, rigorous mathematics. , moment ‚Äôm going say ‚Äôs OK take facts faith just trust .Paired samples mean every data point one sample matching data point sample linked inextricable way. typical example involve group 20 test subjects measured experiment. Providing experiment didn‚Äôt anything fatal test subjects data consist two samples; 20 pre-experiment measurements 20 post-experiment measurements. However, test subjects used pre-experiment data point can matched exactly one post-experiment data points. sense two samples said ‚Äúpaired.‚Äùcouple tests (Bartlett‚Äôs test Levene‚Äôs test) can used see two samples come distributions variance. covered later section.Resampling techniques aren‚Äôt covered course require mixture statistical understanding programming skill. Ask demonstrator (Google üòâ) want know .","code":""},{"path":"introduction.html","id":"cs1-tidy-data","chapter":"3 Introduction","heading":"3.3 Tidy data","text":"two samples data can stored one three formats R:two separate vectors,stacked data frame,unstacked data frame/list.Two separate vectors case (hopefully) obvious.using data frame different options organise data. best way formatting data R using tidy data format.Tidy data following properties:variable columnEach observation rowEach value cellStacked form (long format data) data arranged way variable (thing measured) column. consider dataset containing meerkat weights (g) two different countries stacked format data look like:unstacked (wide format) form variable (measured thing) present one column. example, let‚Äôs say measured meerkat weight two countries period years. organise data way year measured values split country:tidy data easiest way analyses R strongly encourage start adopting format standard data collection processing.","code":"## # A tibble: 6 √ó 2\n##   country  weight\n##   <chr>     <dbl>\n## 1 Botswana    514\n## 2 Botswana    568\n## 3 Botswana    519\n## 4 Uganda      624\n## 5 Uganda      662\n## 6 Uganda      633## # A tibble: 3 √ó 3\n##    year Botswana Uganda\n##   <dbl>    <dbl>  <dbl>\n## 1  1990      514    624\n## 2  1992      568    662\n## 3  1995      519    633"},{},{"path":"cs1-one-sample-t-test.html","id":"cs1-one-sample-t-test","chapter":"4 One-sample t-test","heading":"4 One-sample t-test","text":"","code":""},{"path":"cs1-one-sample-t-test.html","id":"section-commands","chapter":"4 One-sample t-test","heading":"4.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-one-sample-t-test.html","id":"data-and-hypotheses","chapter":"4 One-sample t-test","heading":"4.2 Data and hypotheses","text":"example, suppose measure body lengths male guppies (mm) collected Guanapo River Trinidad. want test whether data support hypothesis mean body actually 20 mm. form following null alternative hypotheses:\\(H_0\\): mean body length equal 20mm (\\(\\mu =\\) 20).\\(H_1\\): mean body length equal 20mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed t-test see reject null hypothesis .use one-sample test one sample.use two-tailed t-test want know data suggest true (population) mean different 20 mm either direction rather just see greater less 20 mm (case use one-tailed test).‚Äôre using t-test don‚Äôt know better yet ‚Äôm telling . ‚Äôll look precise assumptions/requirements need moment.Make sure downloaded data (see: Datasets) placed data/raw folder within working directory.First load relevant libraries:read data create vector containing data.first line reads data R creates object called tibble, type data frame. data frame contains 3 columns: unique id, river encoding river length measured guppy length.","code":"\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n# import the data\nfishlengthDF <- read_csv(\"data/tidy/CS1-onesample.csv\")\n\nfishlengthDF## # A tibble: 29 √ó 3\n##       id river   length\n##    <dbl> <chr>    <dbl>\n##  1     1 guanapo   19.1\n##  2     2 guanapo   23.3\n##  3     3 guanapo   18.2\n##  4     4 guanapo   16.4\n##  5     5 guanapo   19.7\n##  6     6 guanapo   16.6\n##  7     7 guanapo   17.5\n##  8     8 guanapo   19.9\n##  9     9 guanapo   19.1\n## 10    10 guanapo   18.8\n## # ‚Ä¶ with 19 more rows"},{"path":"cs1-one-sample-t-test.html","id":"summarise-and-visualise","chapter":"4 One-sample t-test","heading":"4.3 Summarise and visualise","text":"Summarise data visualise :data appear contain obvious errors, whilst mean median less 20 (18.3 18.8 respectively) absolutely certain sample mean sufficiently different value ‚Äústatistically significant,‚Äù although may anticipate result.","code":"\nsummary(fishlengthDF)##        id        river               length    \n##  Min.   : 1   Length:29          Min.   :11.2  \n##  1st Qu.: 8   Class :character   1st Qu.:17.5  \n##  Median :15   Mode  :character   Median :18.8  \n##  Mean   :15                      Mean   :18.3  \n##  3rd Qu.:22                      3rd Qu.:19.7  \n##  Max.   :29                      Max.   :23.3\nfishlengthDF %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()"},{"path":"cs1-one-sample-t-test.html","id":"assumptions","chapter":"4 One-sample t-test","heading":"4.4 Assumptions","text":"comes one-sample tests, two options:t-testWilcoxon signed-rank testFor us use t-test analysis (results valid) make two assumptions:parent distribution sample taken normally distributed (sample data normally distributed ).worth noting though t-test actually pretty robust situations sample data normal. sufficiently large sample sizes (guess good mine, conventionally means 30 data points), can use t-test without worrying whether underlying population normally distributed .data point sample independent others. general something can tested instead considered sampling procedure. example, taking repeated measurements individual generate data independent.second point know nothing ignore (issue needs considered experimental design), whereas first assumption can checked.\nthree ways checking normality:increasing order rigour, haveHistogramQuantile-quantile plotShapiro-Wilk test","code":""},{"path":"cs1-one-sample-t-test.html","id":"histogram-of-the-data","chapter":"4 One-sample t-test","heading":"4.4.1 Histogram of the data","text":"Plot histogram data, gives:distribution appears uni-modal symmetric, isn‚Äôt obviously non-normal. However, lot distributions simple properties aren‚Äôt normal, isn‚Äôt exactly rigorous. Thankfully , rigorous tests.NB. even looking distribution assess assumption normality already going far beyond anyone else ever . Nevertheless, continue.","code":"\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 15)"},{"path":"cs1-one-sample-t-test.html","id":"q-q-plot-of-the-data","chapter":"4 One-sample t-test","heading":"4.4.2 Q-Q plot of the data","text":"Q-Q plot short quantile-quantile plot. diagnostic plot (sometimes called) way comparing two distributions. Q-Q plots work won‚Äôt explained ask demonstrator really want know going .Construct Q-Q Plot quantiles data quantiles normal distribution:important know data normally distributed points lie (close ) diagonal line graph.case, points lie quite close line part sample quantiles (points) either end sample distribution either smaller (line left) larger (line right) expected supposed normally distributed. suggests sample distribution bit spread expected came normal distribution.important recognise isn‚Äôt simple unambiguous answer interpreting types graph, terms whether assumption normality well met instead often boils matter experience.rare situation indeed assumptions necessary test met unequivocally certain degree personal interpretation always needed. ask whether data normal ‚Äúenough‚Äù confident validity test.four examples QQ plots different types distributions:two graphs relate 200 data points drawn normal distribution. Even can see points lie perfectly diagonal line QQ plot, certain amount deviation top bottom graph can happen just chance (draw different set point graph look slightly different).two graphs relate 200 data points drawn uniform distribution. Uniform distributions condensed normal distributions, reflected QQ plot pronounced S-shaped pattern (colloquially known snaking).two graphs relate 200 data points drawn t distribution. t distributions spread normal distributions, reflected QQ plot pronounced S-shaped pattern , time snaking reflection observed uniform distribution.two graphs relate 200 data points drawn exponential distribution. Exponential distributions symmetric skewed compared normal distributions. significant right-skew distribution reflected QQ plot points curve away diagonal line ends (left-skew points line ends).four cases worth noting deviations ends plot.","code":"\n# pipe the data to ggplot()\n# then construct a Q-Q plot\nfishlengthDF %>%\n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-one-sample-t-test.html","id":"shapiro-wilk-test","chapter":"4 One-sample t-test","heading":"4.4.3 Shapiro-Wilk test","text":"one number formal statistical test assess whether given sample numbers come normal distribution. calculates probability getting sample data underlying distribution fact normal. easy carry R.Perform Shapiro-Wilk test data:variable indicated variable used perform test onstatistic gives calculated W-value (0.9493842)p gives calculated p-value (0.1764229)p-value bigger 0.05 (say) can say insufficient evidence reject null hypothesis sample came normal distribution.important recognise Shapiro-Wilk test without limitations. rather sensitive sample size considered. general, small sample sizes, test relaxed normality (nearly datasets considered normal), whereas large sample sizes test can overly strict, can fail recognise datasets nearly normal indeed.","code":"\nfishlengthDF %>% \n  shapiro_test(length)## # A tibble: 1 √ó 3\n##   variable statistic     p\n##   <chr>        <dbl> <dbl>\n## 1 length       0.949 0.176"},{"path":"cs1-one-sample-t-test.html","id":"assumptions-overview","chapter":"4 One-sample t-test","heading":"4.4.4 Assumptions overview","text":"terms assessing assumptions test always worth considering several methods, graphical analytic, just relying single method.fishlengthDF example, graphical Q-Q plot analysis especially conclusive suggestion snaking plots, Shapiro-Wilk test gave non-significant p-value (0.1764). Putting two together, along original histogram recognition 30 data points dataset personally happy assumptions t-test met well enough trust result t-test, may ‚Ä¶case consider alternative test less stringent assumptions (less powerful): one-sample Wilcoxon signed-rank test.","code":""},{"path":"cs1-one-sample-t-test.html","id":"implement-the-test","chapter":"4 One-sample t-test","heading":"4.5 Implement the test","text":"Perform one-sample, two-tailed t-test:t_test() function requires three arguments:formula, give length ~ 1 indicate one-sample test lengththe mu mean tested null hypothesis, 20the alternative argument gives type alternative hypothesis must one two.sided, greater less. prior assumptions whether alternative fish length greater less 20, choose two.sided.","code":"\nfishlengthDF %>% \n  t_test(length ~ 1,\n         mu = 20,\n         alternative = \"two.sided\")"},{"path":"cs1-one-sample-t-test.html","id":"interpreting-the-output-and-report-results","chapter":"4 One-sample t-test","heading":"4.6 Interpreting the output and report results","text":"output now see console window:statistic column gives us t-statistic -3.5492 (‚Äôll need reporting)df column tells us 28 degrees freedom (‚Äôll need reporting)p column gives us p-value 0.00139The p-value ‚Äôre mostly interested . gives probability us getting sample null hypothesis actually true.:high p-value means high probability observing sample null hypothesis probably true whereasa low p-value means low probability observing sample null hypothesis probably true.important realise p-value just indication absolute certainty interpretation.People, however like definite answers pick artificial probability threshold (called significance level) order able say something decisive. standard significance level 0.05 since p-value smaller choose say ‚Äúunlikely particular sample null hypothesis true.‚Äù Consequently, can reject null hypothesis state :one-sample t-test indicated mean body length male guppies (\\(\\mu\\) = 18.29mm) differs significantly 20 mm (t = -3.55, df = 28, p = 0.0014).sentence adequate concluding statement test write paper report. Note included (brackets) information actual mean value group(\\(\\mu\\) = 18.29mm), test statistic (t = -3.55), degrees freedom (df = 28), p-value (p = 0.0014). journals required report whether p-value less critical value (e.g.¬†p < 0.05) always recommend reporting actual p-value obtained.Please feel free ask demonstrator aspect section unclear form core classical hypothesis testing logic applies rest tests.","code":"## # A tibble: 1 √ó 7\n##   .y.    group1 group2         n statistic    df       p\n## * <chr>  <chr>  <chr>      <int>     <dbl> <dbl>   <dbl>\n## 1 length 1      null model    29     -3.55    28 0.00139"},{"path":"cs1-one-sample-t-test.html","id":"exercise","chapter":"4 One-sample t-test","heading":"4.7 Exercise","text":"Exercise 4.1  following data dissolving times (seconds) drug agitated gastric juice:42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6Do results provide evidence suggest dissolving time drug different 45 seconds?Create tidy data frame save .csv formatWrite null alternative hypotheses.Summarise visualise data perform appropriate one-sample t-test.\ncan say dissolving time? (sentence use report )\ncan say dissolving time? (sentence use report )Check assumptions test.\ntest valid?\ntest valid?\\(H_0\\) : mean \\(=\\) 45s\\(H_1\\) : mean \\(\\neq\\) 45sWe can create data frame Excel save .csv file, example CS1-gastric_juices.csv. contains two columns, id column dissolving_time column measured values.can look histogram boxplot data:8 data points, histogram rather uninformative. even reduce number bins get meaningful visualisation. Thankfully boxplot bit useful . can see:don‚Äôt appear major errors data entry aren‚Äôt huge outliersThe median value box-plot (thick black line) pretty close 45 wouldn‚Äôt surprised mean data isn‚Äôt significantly different 45. can confirm looking mean median values calculated using summary command earlier.data appear symmetric, whilst can‚Äôt tell ‚Äôre normal ‚Äôre least massively skewed.Normality:Shapiro test p-value 0.964 (given bigger 0.05) suggests data normal enough.Q-Q plot isn‚Äôt perfect, deviation points away line since points aren‚Äôt accelerating away line , since 8 points, can claim, slight reservations, assumption normality appears adequately well met.Overall, somewhat confident assumption normality well-enough met t-test appropriate method analysing data. Note ridiculous number caveats slightly political/slippery language ‚Äôm using. intentional reflects ambiguous nature assumption checking. important approach statistics need embrace.reality, found situation also try non-parametric test data (Wilcoxon signed-rank test) see whether get conclusion whether median dissolving time differs 45s. Technically, don‚Äôt know Wilcoxon test yet haven‚Äôt done section materials. Anyway, get conclusion confidence result test goes considerably; doesn‚Äôt matter well assumption met, get result. hand get completely different conclusion carrying non-parametric test bets ; now little confidence test result don‚Äôt know one believe (case assumptions test bit unclear). example Wilcoxon test also gives us non-significant result good.one-sample t-test indicated mean dissolving time drug significantly different 45s (t=0.366 , df=7 , p=0.725)","code":"\n# load the data\ndissolving <- read_csv(\"data/tidy/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving## # A tibble: 8 √ó 2\n##      id dissolving_time\n##   <dbl>           <dbl>\n## 1     1            42.7\n## 2     2            43.4\n## 3     3            44.6\n## 4     4            45.1\n## 5     5            45.6\n## 6     6            45.9\n## 7     7            46.8\n## 8     8            47.6\n# summarise the data\nsummary(dissolving)##        id       dissolving_time\n##  Min.   :1.00   Min.   :42.70  \n##  1st Qu.:2.75   1st Qu.:44.30  \n##  Median :4.50   Median :45.35  \n##  Mean   :4.50   Mean   :45.21  \n##  3rd Qu.:6.25   3rd Qu.:46.12  \n##  Max.   :8.00   Max.   :47.60\n# create a histogram\ndissolving %>% \n  ggplot(aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n# create a boxplot\ndissolving %>% \n  ggplot(aes(y = dissolving_time)) +\n  geom_boxplot()\n# perform Shapiro-Wilk test\ndissolving %>% \n  shapiro_test(dissolving_time)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 dissolving_time     0.980 0.964\n# create a Q-Q plot\ndissolving %>% \n  ggplot(aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")\n# perform one-sample t-test\ndissolving %>% \n  t_test(dissolving_time ~ 1,\n         mu = 45,\n         alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.             group1 group2         n statistic    df     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl> <dbl>\n## 1 dissolving_time 1      null model     8     0.366     7 0.725"},{"path":"cs1-one-sample-t-test.html","id":"hypotheses","chapter":"4 One-sample t-test","heading":"4.7.1 Hypotheses","text":"\\(H_0\\) : mean \\(=\\) 45s\\(H_1\\) : mean \\(\\neq\\) 45s","code":""},{"path":"cs1-one-sample-t-test.html","id":"data-summarise-visualise","chapter":"4 One-sample t-test","heading":"4.7.2 Data, summarise & visualise","text":"can create data frame Excel save .csv file, example CS1-gastric_juices.csv. contains two columns, id column dissolving_time column measured values.can look histogram boxplot data:8 data points, histogram rather uninformative. even reduce number bins get meaningful visualisation. Thankfully boxplot bit useful . can see:don‚Äôt appear major errors data entry aren‚Äôt huge outliersThe median value box-plot (thick black line) pretty close 45 wouldn‚Äôt surprised mean data isn‚Äôt significantly different 45. can confirm looking mean median values calculated using summary command earlier.data appear symmetric, whilst can‚Äôt tell ‚Äôre normal ‚Äôre least massively skewed.","code":"\n# load the data\ndissolving <- read_csv(\"data/tidy/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving## # A tibble: 8 √ó 2\n##      id dissolving_time\n##   <dbl>           <dbl>\n## 1     1            42.7\n## 2     2            43.4\n## 3     3            44.6\n## 4     4            45.1\n## 5     5            45.6\n## 6     6            45.9\n## 7     7            46.8\n## 8     8            47.6\n# summarise the data\nsummary(dissolving)##        id       dissolving_time\n##  Min.   :1.00   Min.   :42.70  \n##  1st Qu.:2.75   1st Qu.:44.30  \n##  Median :4.50   Median :45.35  \n##  Mean   :4.50   Mean   :45.21  \n##  3rd Qu.:6.25   3rd Qu.:46.12  \n##  Max.   :8.00   Max.   :47.60\n# create a histogram\ndissolving %>% \n  ggplot(aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n# create a boxplot\ndissolving %>% \n  ggplot(aes(y = dissolving_time)) +\n  geom_boxplot()"},{"path":"cs1-one-sample-t-test.html","id":"assumptions-1","chapter":"4 One-sample t-test","heading":"4.7.3 Assumptions","text":"Normality:Shapiro test p-value 0.964 (given bigger 0.05) suggests data normal enough.Q-Q plot isn‚Äôt perfect, deviation points away line since points aren‚Äôt accelerating away line , since 8 points, can claim, slight reservations, assumption normality appears adequately well met.Overall, somewhat confident assumption normality well-enough met t-test appropriate method analysing data. Note ridiculous number caveats slightly political/slippery language ‚Äôm using. intentional reflects ambiguous nature assumption checking. important approach statistics need embrace.reality, found situation also try non-parametric test data (Wilcoxon signed-rank test) see whether get conclusion whether median dissolving time differs 45s. Technically, don‚Äôt know Wilcoxon test yet haven‚Äôt done section materials. Anyway, get conclusion confidence result test goes considerably; doesn‚Äôt matter well assumption met, get result. hand get completely different conclusion carrying non-parametric test bets ; now little confidence test result don‚Äôt know one believe (case assumptions test bit unclear). example Wilcoxon test also gives us non-significant result good.","code":"\n# perform Shapiro-Wilk test\ndissolving %>% \n  shapiro_test(dissolving_time)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 dissolving_time     0.980 0.964\n# create a Q-Q plot\ndissolving %>% \n  ggplot(aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-one-sample-t-test.html","id":"implement-test","chapter":"4 One-sample t-test","heading":"4.7.4 Implement test","text":"one-sample t-test indicated mean dissolving time drug significantly different 45s (t=0.366 , df=7 , p=0.725)","code":"\n# perform one-sample t-test\ndissolving %>% \n  t_test(dissolving_time ~ 1,\n         mu = 45,\n         alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.             group1 group2         n statistic    df     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl> <dbl>\n## 1 dissolving_time 1      null model     8     0.366     7 0.725"},{},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"cs1-onesample-wilcoxon-signed-rank","chapter":"5 Wilcoxon signed-rank test","heading":"5 Wilcoxon signed-rank test","text":"test also considers single sample, however test (contrast one sample t-test) don‚Äôt assume parent distribution normally distributed. still need parent distribution (consequently sample) symmetric though. test look see median parent distributions differs significantly given hypothesised value (contrast t-test looks mean).","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"section-commands-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.1 Section commands","text":"New commands used section:, use fishlengthDF dataset. one-sample Wilcoxon signed-rank test allows see median body length different specified value. want test whether data support hypothesis median body actually 20 mm. following null alternative hypotheses similar used one sample t-test:\\(H_0\\): median body length equal 20 mm (\\(\\mu =\\) 20).\\(H_1\\): median body length equal 20 mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed Wilcoxon signed-rank test see reject null hypothesis .","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.2 Summarise and visualise","text":"previous section, nothing really changed now (‚Äôre good start practical!)","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"assumptions-2","chapter":"5 Wilcoxon signed-rank test","heading":"5.3 Assumptions","text":"order use one-sample Wilcoxon rank-sum test analysis (results strictly valid) make two assumptions:parent distribution sample symmetricEach data point sample independent others. t-test common feature nearly statistical tests. Lack independence data really tough deal (impossible) large part proper experimental design ensuring .Whilst formal statistical tests symmetry opt simple visual inspection using boxplot histogram.Plot histogram boxplot data:get following plots (‚Äôll learn create panels like later!):can see whilst distribution isn‚Äôt perfectly symmetric, neither heavily skewed left right can make call distribution symmetric enough us happy results test.","code":"\n# create a histogram\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 10)\n\n# create boxplot\nfishlengthDF %>% \n  ggplot(aes(y = length)) +\n  geom_boxplot()"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"implement-the-test-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.4 Implement the test","text":"Perform one-sample, two-tailed Wilcoxon signed-rank test:syntax identical one-sample t-test carried earlier.formula, give length ~ 1 indicate one-sample test lengththe mu median tested null hypothesis, 20the alternative argument gives type alternative hypothesis must one two.sided, greater less. prior assumptions whether alternative median fish length greater less 20, choose two.sided.","code":"\nfishlengthDF %>% \n  wilcox_test(length ~ 1,\n              mu = 20,\n              alternative = \"two.sided\")"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"interpreting-the-output-and-report-results-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.5 Interpreting the output and report results","text":"output now see console windowthe statistic column gives us t-statistic 67.5 (‚Äôll need reporting)n column gives us sample size 29the p column gives us p-value 0.00122Again, p-value ‚Äôre interested . gives probability us getting sample null hypothesis actually true.\n, case since p-value less 0.05 can reject null hypothesis state :one-sample Wilcoxon signed-rank test indicated median body length male guppies (\\(\\mu\\) = 18.8 mm) differs significantly 20 mm (V = 67.5, n = 29, p = 0.0012).sentence adequate concluding statement test write paper report. Note included (brackets) information median value group (\\(\\mu\\) = 18.8 mm), test statistic (V = 67.5), number observations (n = 29), p-value (p = 0.0012).","code":"## # A tibble: 1 √ó 6\n##   .y.    group1 group2         n statistic       p\n## * <chr>  <chr>  <chr>      <int>     <dbl>   <dbl>\n## 1 length 1      null model    29      67.5 0.00122"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"exercise-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.6 Exercise","text":"Exercise 5.1  Performing Wilcoxon signed-rank test:Analyse drug dataset using one-sample Wilcoxon signed-rank testDiscuss (virtual) neighbour two tests feel best suited data.matter case?\\(H_0\\) : median \\(=\\) 45s\\(H_1\\) : median \\(\\neq\\) 45sFrom box-plot previous exercise already know data symmetric enough test valid.one-sample Wilcoxon-signed rank test indicated median dissolving time drug significantly different 45 s (V=22, n=8 , p=0.64)terms choosing two test can see meet respective assumptions tests valid. case tests also agree terms conclusions .e.¬†average dissolving time (either mean median) doesn‚Äôt differ significantly proposed value 45 s.one answer doesn‚Äôt matter test use.Another answer pick test measures quantity ‚Äôre interested .e. care medians use Wilcoxon test, whereas care means use t-test.final answer , since test valid prefer use test greater power. t-tests always power Wilcoxon tests (long ‚Äôre valid) report one. (‚Äôll talk last session power effectively capacity test detect significant difference - power better).","code":"\ndissolving %>% \n  wilcox_test(dissolving_time ~ 1,\n              mu = 45,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 6\n##   .y.             group1 group2         n statistic     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl>\n## 1 dissolving_time 1      null model     8        22 0.641"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"hypotheses-1","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.1 Hypotheses","text":"\\(H_0\\) : median \\(=\\) 45s\\(H_1\\) : median \\(\\neq\\) 45s","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"assumptions-3","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.2 Assumptions","text":"box-plot previous exercise already know data symmetric enough test valid.","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"wilcoxon-signed-rank-test","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.3 Wilcoxon signed-rank test","text":"one-sample Wilcoxon-signed rank test indicated median dissolving time drug significantly different 45 s (V=22, n=8 , p=0.64)","code":"\ndissolving %>% \n  wilcox_test(dissolving_time ~ 1,\n              mu = 45,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 6\n##   .y.             group1 group2         n statistic     p\n## * <chr>           <chr>  <chr>      <int>     <dbl> <dbl>\n## 1 dissolving_time 1      null model     8        22 0.641"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"discussion","chapter":"5 Wilcoxon signed-rank test","heading":"5.6.4 Discussion","text":"terms choosing two test can see meet respective assumptions tests valid. case tests also agree terms conclusions .e.¬†average dissolving time (either mean median) doesn‚Äôt differ significantly proposed value 45 s.one answer doesn‚Äôt matter test use.Another answer pick test measures quantity ‚Äôre interested .e. care medians use Wilcoxon test, whereas care means use t-test.final answer , since test valid prefer use test greater power. t-tests always power Wilcoxon tests (long ‚Äôre valid) report one. (‚Äôll talk last session power effectively capacity test detect significant difference - power better).","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"key-points","chapter":"5 Wilcoxon signed-rank test","heading":"5.7 Key points","text":"One-sample tests used single sample continuous dataWe can summarise data using summary() function visualise geom_boxplot()t-test assumes data normally distributed independent otherThe Wilcoxon signed-rank test assume normal distribution, require independent samplesThe t_test() compares mean parent distribution differs hypothesised value, whereas wilcox_test() compares median.good way assessing assumptions visually check looking distribution geom_histogram() quantile-quantile plots stat_qq() stat_qqline()","code":""},{},{"path":"cs1-students-t-test.html","id":"cs1-students-t-test","chapter":"6 Student‚Äôs t-test","heading":"6 Student‚Äôs t-test","text":"test assume sample data sets normally distributed equal variance. test see means two samples differ significantly .language used section slightly different used section 3.1. Although language used section 3.1 technically correct, sentences somewhat onerous read. ‚Äôve opted easier reading style expense technical accuracy. Please feel free re-write section (leisure).","code":""},{"path":"cs1-students-t-test.html","id":"section-commands-2","chapter":"6 Student‚Äôs t-test","heading":"6.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-students-t-test.html","id":"data-and-hypotheses-1","chapter":"6 Student‚Äôs t-test","heading":"6.2 Data and hypotheses","text":"example, suppose now measure body lengths male guppies (mm) collected two rivers Trinidad; Aripo Guanapo. want test whether mean body length differs samples. form following null alternative hypotheses:\\(H_0\\): mean body length differ two groups \\((\\mu = \\mu G)\\)\\(H_1\\): mean body length differ two groups \\((\\mu \\neq \\mu G)\\)use two-sample, two-tailed t-test see can reject null hypothesis.use two-sample test now two samples.use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller .‚Äôre using Student‚Äôs t-test sample sizes big ‚Äôre assuming parent populations equal variance (can check later).data stored file data/tidy/CS1-twosample.csv.Read R:","code":"\nrivers <- read_csv(\"data/tidy/CS1-twosample.csv\")"},{"path":"cs1-students-t-test.html","id":"cs1-students-sumvisual","chapter":"6 Student‚Äôs t-test","heading":"6.3 Summarise and visualise","text":"Let‚Äôs summarise data‚Ä¶visualise :boxplot appear suggest two samples different means, moreover guppies Guanapo may smaller guppies Aripo. isn‚Äôt immediately obvious two populations don‚Äôt equal variances though, plough .","code":"\n# get common summary stats for the length column\nrivers %>% \n  select(-id) %>% \n  group_by(river) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   river   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n## 2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\nrivers %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()"},{"path":"cs1-students-t-test.html","id":"assumptions-4","chapter":"6 Student‚Äôs t-test","heading":"6.4 Assumptions","text":"order use Student‚Äôs t-test (results strictly valid) make three assumptions:parent distributions samples taken normally distributed (lead sample data normally distributed ).data point samples independent others.parent distributions variance.example first assumption can ignored sample sizes large enough (maths, Aripo containing 39 Guanapo 29 samples). samples smaller use tests previous section.second point can nothing unless know data collected, ignore .third point regarding equality variance can tested using either Bartlett‚Äôs test (samples normally distributed) Levene‚Äôs test (samples normally distributed).\ngets bit trickier. Although don‚Äôt care samples normally distributed t-test valid (sample size big enough compensate), need know normally distributed order decide variance test use.perform Shapiro-Wilk test samples separately.can see whilst Guanapo data probably normally distributed (p = 0.1764 > 0.05), Aripo data unlikely normally distributed (p = 0.02802 < 0.05). Remember p-value gives probability observing sample parent population actually normally distributed.Shapiro-Wilk test quite sensitive sample size. means large sample even small deviations normality cause sample fail test, whereas smaller samples allowed pass much larger deviations. Aripo data nearly 40 points compared Guanapo data much easier Aripo sample fail compared Guanapo data.","code":"\n# group data by river and perform test\nrivers %>% \n  group_by(river) %>% \n  shapiro_test(length)## # A tibble: 2 √ó 4\n##   river   variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Aripo   length       0.936 0.0280\n## 2 Guanapo length       0.949 0.176"},{"path":"cs1-students-t-test.html","id":"exercise-qq-rivers","chapter":"6 Student‚Äôs t-test","heading":"6.5 Exercise: Q-Q plots rivers","text":"Exercise 6.1  Q-Q plots rivers dataCreate Q-Q plots two samples discuss neighbour see light results Shapiro-Wilk test.Q-Q plots mirror found Shapiro-Wilk tests: data Aripo pretty normally distributed, whereas assumption normality Guanapo data less certain.","code":"\n# we group the data by river\n# then create a panel per river\n# containing the Q-Q plot for that river\nrivers %>% \n  group_by(river) %>%\n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(river))"},{"path":"cs1-students-t-test.html","id":"equality-of-variance","chapter":"6 Student‚Äôs t-test","heading":"6.6 Equality of variance","text":"Remember statistical tests provide answers, merely suggest patterns. Human interpretation still crucial aspect .Shapiro-Wilk test shown data normal enough order test equality variance use Levene‚Äôs test.key bit information p column. p-value (0.1876) test. tells us probability observing two samples come distributions variance. probability greater arbitrary significance level 0.05 can somewhat confident necessary assumptions carrying Student‚Äôs t-test two samples valid. (woohoo!)","code":"\nrivers %>% \n  levene_test(length ~ river)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    66      1.77 0.188"},{"path":"cs1-students-t-test.html","id":"bartletts-test","chapter":"6 Student‚Äôs t-test","heading":"6.6.1 Bartlett‚Äôs test","text":"wanted carry Bartlett‚Äôs test (.e.¬†data sufficiently normally distributed) command :relevant p-value given 3rd line.use bartlett.test() base R. Surprisingly, rstatix package built-equivalent.wanted get output Bartlett test tidy format, following, take rivers data set pipe bartlett.test() function. Note need define data using dot (.), first input bartlett.test() data. pipe output tidy() function, part broom library, kindly converts output tidy format. Handy!","code":"\nbartlett.test(length ~ river, data = rivers)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  length by river\n## Bartlett's K-squared = 4.4734, df = 1, p-value = 0.03443\n# load the broom package\nlibrary(broom)\n\n# perform Bartlett's test on the data and tidy\nrivers %>% \n  bartlett.test(length ~ river,\n                data = .) %>% \n  tidy()## # A tibble: 1 √ó 4\n##   statistic p.value parameter method                                   \n##       <dbl>   <dbl>     <dbl> <chr>                                    \n## 1      4.47  0.0344         1 Bartlett test of homogeneity of variances"},{"path":"cs1-students-t-test.html","id":"implement-test-1","chapter":"6 Student‚Äôs t-test","heading":"6.7 Implement test","text":"case ‚Äôre ignoring fact data normal enough, according Shapiro-Wilk test. However, sample sizes pretty large t-test also pretty robust case, can perform t-test. Remember, allowed variances two groups (Aripo Guanapo) equal.Perform two-sample, two-tailed, t-test:following:take data set pipe t_test() functionThe t_test() function takes formula format variable ~ categoryAgain alternative two.sided prior knowledge whether alternative greater lessThe last argument says whether variance two samples can assumed equal (Student‚Äôs t-test) unequal (Welch‚Äôs t-test)","code":"\n# two-sample, two-tailed t-test\nrivers %>% \n  t_test(length ~ river,\n         alternative = \"two.sided\",\n         var.equal = TRUE)"},{"path":"cs1-students-t-test.html","id":"interpret-output-and-report-results","chapter":"6 Student‚Äôs t-test","heading":"6.8 Interpret output and report results","text":"Let‚Äôs look results t-test performed original (stacked) data frame:first 5 columns give information variable (.y.), groups sample size groupThe statistic column gives t-value 3.8433 (need reporting)df column tell us 66 degrees freedom (need reporting)p column gives us p-value 0.0002754Again, p-value ‚Äôre interested . Since p-value small (much smaller standard significance level) choose say ‚Äúunlikely two samples came parent distribution can reject null hypothesis‚Äù state :Student‚Äôs t-test indicated mean body length male guppies Guanapo river (18.29 mm) differs significantly mean body length male guppies Aripo river (20.33 mm) (t = 3.8433, df = 66, p = 0.0003).Now ‚Äôs conversation starter.","code":"## # A tibble: 1 √ó 8\n##   .y.    group1 group2     n1    n2 statistic    df        p\n## * <chr>  <chr>  <chr>   <int> <int>     <dbl> <dbl>    <dbl>\n## 1 length Aripo  Guanapo    39    29      3.84    66 0.000275"},{"path":"cs1-students-t-test.html","id":"exercise-turtles","chapter":"6 Student‚Äôs t-test","heading":"6.9 Exercise: Turtles","text":"Exercise 6.2  Serum cholesterol concentrations turtlesUsing following data, test null hypothesis male female turtles mean serum cholesterol concentrations.Create tidy data frame save .csv fileWrite null alternative hypothesesImport data RSummarise visualise dataCheck assumptions (normality variance) using appropriate tests plotsPerform two-sample t-testWrite sentence summarises results foundHere data tidy format, variable column, row observation (unique identifier observation).\\(H_0\\) : male mean \\(=\\) female mean\\(H_1\\) : male mean \\(\\neq\\) female meanI‚Äôd always recommend storing data tidy, stacked format (fact can‚Äôt think situation want store data untidy, unstacked format!) example manually input data Excel following layout, saving data CSV file reading :Let‚Äôs summarise data (although visualisation probably much easier work ):visualise data:always use plot summary assess three things:look like ‚Äôve loaded data correctly?\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .think difference two groups?\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.think assumptions?\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.\nHomogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.Homogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.NormalityLet‚Äôs look normality groups separately. several ways getting serum values Males Females separately. ‚Äôll use unstacking method, use Shapiro-Wilk followed qqplots.p-values Shapiro-Wilk tests non-significant suggests data normal enough. bit surprising given saw boxplot two bits information can use reassure us.p-value Female group smaller Male group (suggesting Female group closer non-normal Male group) makes sense.Shapiro-Wilk test generally quite relaxed normality small sample sizes (notoriously strict large sample sizes). group 6 data points , data actually really, really skewed distribution. Given Female group 6 data points , ‚Äôs surprising Shapiro-Wilk test came back saying everything OK.results Q-Q plots echo ‚Äôve already seen Shapiro-Wilk analyses. Male group doesn‚Äôt look bad whereas Female group looks somewhat dodgy.Overall, assumption normality data doesn‚Äôt appear well met , bear mind data points group might just seeing pattern data due random chance rather underlying populations actually normally distributed. Personally, though ‚Äôd edge towards non-normal .Homogeneity VarianceIt‚Äôs clear whether data normal , isn‚Äôt clear test use . sensible approach hope agree (fingers crossed!)Bartlett‚Äôs test gives us:Levene‚Äôs test gives us:good news Levene Bartlett agree homogeneity variance two groups (thank goodness!).Overall, means ‚Äôre sure normality, homogeneity variance pretty good.result Bartlett test know can carry two-sample Student‚Äôs t-test (opposed two-sample Welch‚Äôs t-test, ‚Äôre confused, see Figure 3.2)p-value 0.544, test tells insufficient evidence suggest means two groups different. suitable summary sentence :Student‚Äôs two-sample t-test indicated mean serum cholesterol level differ significantly Male Female turtles (t = 0.627, df = 11, p = 0.544).reality, ambiguous normality assumption assessment, dataset actually carry two different tests; two-sample t-test equal variance Mann-Whitney U test. agreed wouldn‚Äôt matter much one reported (‚Äôd personally report short sentence say ‚Äôm wasn‚Äôt clear whether assumption normality met), acceptable report just one.","code":"\n# load the data\nturtle <- read_csv(\"data/tidy/CS1-turtle.csv\")\n\n# and have a look\nturtle## # A tibble: 13 √ó 3\n##       id serum sex   \n##    <dbl> <dbl> <chr> \n##  1     1  220. Male  \n##  2     2  219. Male  \n##  3     3  230. Male  \n##  4     4  229. Male  \n##  5     5  222  Male  \n##  6     6  224. Male  \n##  7     7  226. Male  \n##  8     8  223. Female\n##  9     9  222. Female\n## 10    10  230. Female\n## 11    11  224. Female\n## 12    12  224. Female\n## 13    13  231. Female\n# create summary statistics for each group\nturtle %>% \n  select(-id) %>% \n  group_by(sex) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   sex    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n## 2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n# visualise the data\nturtle %>% \n  ggplot(aes(x = sex, y = serum)) +\n  geom_boxplot()\n# perform Shapiro-Wilk test on each group\nturtle %>% \n  group_by(sex) %>% \n  shapiro_test(serum)## # A tibble: 2 √ó 4\n##   sex    variable statistic     p\n##   <chr>  <chr>        <dbl> <dbl>\n## 1 Female serum        0.842 0.135\n## 2 Male   serum        0.944 0.674\n# create Q-Q plots for both groups\nturtle %>% \n  ggplot(aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(sex))\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  serum by sex\n## Bartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n# perform Levene's test\nturtle %>% \n  levene_test(serum ~ sex)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    11     0.243 0.631\n# perform two-sample t-test\nturtle %>% \n  t_test(serum ~ sex,\n         alternative = \"two.sided\",\n         var.equal = TRUE)## # A tibble: 1 √ó 8\n##   .y.   group1 group2    n1    n2 statistic    df     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n## 1 serum Female Male       6     7     0.627    11 0.544"},{"path":"cs1-students-t-test.html","id":"data","chapter":"6 Student‚Äôs t-test","heading":"6.9.1 Data","text":"data tidy format, variable column, row observation (unique identifier observation).","code":""},{"path":"cs1-students-t-test.html","id":"hypotheses-2","chapter":"6 Student‚Äôs t-test","heading":"6.9.2 Hypotheses","text":"\\(H_0\\) : male mean \\(=\\) female mean\\(H_1\\) : male mean \\(\\neq\\) female mean","code":""},{"path":"cs1-students-t-test.html","id":"load-summarise-and-visualise-data","chapter":"6 Student‚Äôs t-test","heading":"6.9.3 Load, summarise and visualise data","text":"‚Äôd always recommend storing data tidy, stacked format (fact can‚Äôt think situation want store data untidy, unstacked format!) example manually input data Excel following layout, saving data CSV file reading :Let‚Äôs summarise data (although visualisation probably much easier work ):visualise data:always use plot summary assess three things:look like ‚Äôve loaded data correctly?\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .\ntwo groups extreme values plots seem match dataset, ‚Äôm happy haven‚Äôt done anything massively wrong .think difference two groups?\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.\nneed result formal test make sense given data, ‚Äôs important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn‚Äôt case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.think assumptions?\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.\nHomogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ‚Äôll look carefully formal checks decided whether think data normal enough us use t-test.Homogeneity variance. stage spread data within group looks similar, potential skew Female group ‚Äôll want check assumptions carefully.","code":"\n# load the data\nturtle <- read_csv(\"data/tidy/CS1-turtle.csv\")\n\n# and have a look\nturtle## # A tibble: 13 √ó 3\n##       id serum sex   \n##    <dbl> <dbl> <chr> \n##  1     1  220. Male  \n##  2     2  219. Male  \n##  3     3  230. Male  \n##  4     4  229. Male  \n##  5     5  222  Male  \n##  6     6  224. Male  \n##  7     7  226. Male  \n##  8     8  223. Female\n##  9     9  222. Female\n## 10    10  230. Female\n## 11    11  224. Female\n## 12    12  224. Female\n## 13    13  231. Female\n# create summary statistics for each group\nturtle %>% \n  select(-id) %>% \n  group_by(sex) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   sex    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n## 2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n# visualise the data\nturtle %>% \n  ggplot(aes(x = sex, y = serum)) +\n  geom_boxplot()"},{"path":"cs1-students-t-test.html","id":"assumptions-5","chapter":"6 Student‚Äôs t-test","heading":"6.9.4 Assumptions","text":"NormalityLet‚Äôs look normality groups separately. several ways getting serum values Males Females separately. ‚Äôll use unstacking method, use Shapiro-Wilk followed qqplots.p-values Shapiro-Wilk tests non-significant suggests data normal enough. bit surprising given saw boxplot two bits information can use reassure us.p-value Female group smaller Male group (suggesting Female group closer non-normal Male group) makes sense.Shapiro-Wilk test generally quite relaxed normality small sample sizes (notoriously strict large sample sizes). group 6 data points , data actually really, really skewed distribution. Given Female group 6 data points , ‚Äôs surprising Shapiro-Wilk test came back saying everything OK.results Q-Q plots echo ‚Äôve already seen Shapiro-Wilk analyses. Male group doesn‚Äôt look bad whereas Female group looks somewhat dodgy.Overall, assumption normality data doesn‚Äôt appear well met , bear mind data points group might just seeing pattern data due random chance rather underlying populations actually normally distributed. Personally, though ‚Äôd edge towards non-normal .Homogeneity VarianceIt‚Äôs clear whether data normal , isn‚Äôt clear test use . sensible approach hope agree (fingers crossed!)Bartlett‚Äôs test gives us:Levene‚Äôs test gives us:good news Levene Bartlett agree homogeneity variance two groups (thank goodness!).Overall, means ‚Äôre sure normality, homogeneity variance pretty good.","code":"\n# perform Shapiro-Wilk test on each group\nturtle %>% \n  group_by(sex) %>% \n  shapiro_test(serum)## # A tibble: 2 √ó 4\n##   sex    variable statistic     p\n##   <chr>  <chr>        <dbl> <dbl>\n## 1 Female serum        0.842 0.135\n## 2 Male   serum        0.944 0.674\n# create Q-Q plots for both groups\nturtle %>% \n  ggplot(aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(sex))\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  serum by sex\n## Bartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n# perform Levene's test\nturtle %>% \n  levene_test(serum ~ sex)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    11     0.243 0.631"},{"path":"cs1-students-t-test.html","id":"implement-two-sample-t-test","chapter":"6 Student‚Äôs t-test","heading":"6.9.5 Implement two-sample t-test","text":"result Bartlett test know can carry two-sample Student‚Äôs t-test (opposed two-sample Welch‚Äôs t-test, ‚Äôre confused, see Figure 3.2)p-value 0.544, test tells insufficient evidence suggest means two groups different. suitable summary sentence :Student‚Äôs two-sample t-test indicated mean serum cholesterol level differ significantly Male Female turtles (t = 0.627, df = 11, p = 0.544).","code":"\n# perform two-sample t-test\nturtle %>% \n  t_test(serum ~ sex,\n         alternative = \"two.sided\",\n         var.equal = TRUE)## # A tibble: 1 √ó 8\n##   .y.   group1 group2    n1    n2 statistic    df     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n## 1 serum Female Male       6     7     0.627    11 0.544"},{"path":"cs1-students-t-test.html","id":"discussion-1","chapter":"6 Student‚Äôs t-test","heading":"6.9.6 Discussion","text":"reality, ambiguous normality assumption assessment, dataset actually carry two different tests; two-sample t-test equal variance Mann-Whitney U test. agreed wouldn‚Äôt matter much one reported (‚Äôd personally report short sentence say ‚Äôm wasn‚Äôt clear whether assumption normality met), acceptable report just one.","code":""},{},{"path":"cs1-mannwhitney-u-test.html","id":"cs1-mannwhitney-u-test","chapter":"7 Mann-Whitney U test","heading":"7 Mann-Whitney U test","text":"test also compares two samples, however test (contrast Student‚Äôs t-test) don‚Äôt assume parent distributions normally distributed. order compare medians two groups still need parent distributions (consequently samples) shape variance. test look see medians two parent distributions differ significantly .","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"section-commands-3","chapter":"7 Mann-Whitney U test","heading":"7.1 Section commands","text":"new commands used section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"data-and-hypotheses-2","chapter":"7 Mann-Whitney U test","heading":"7.2 Data and hypotheses","text":", use rivers dataset. want test whether median body length male guppies differs samples. form following null alternative hypotheses:\\(H_0\\): difference median body length two groups 0 \\((\\mu - \\mu G = 0)\\)\\(H_1\\): difference median body length two groups 0 \\((\\mu - \\mu G \\neq 0)\\)use two-tailed Mann-Whitney U test see can reject null hypothesis.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"summarise-and-visualise-2","chapter":"7 Mann-Whitney U test","heading":"7.3 Summarise and visualise","text":"previous section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"assumptions-6","chapter":"7 Mann-Whitney U test","heading":"7.4 Assumptions","text":"checked previously.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"implement-test-2","chapter":"7 Mann-Whitney U test","heading":"7.5 Implement test","text":"Perform two-tailed, Mann-Whitney U test:first argument must formula format: variable ~ categoryThe second argument gives type alternative hypothesis must one two.sided, greater less","code":"\nrivers %>% \n  wilcox_test(length ~ river,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.    group1 group2     n1    n2 statistic        p\n## * <chr>  <chr>  <chr>   <int> <int>     <dbl>    <dbl>\n## 1 length Aripo  Guanapo    39    29       841 0.000646"},{"path":"cs1-mannwhitney-u-test.html","id":"interpret-output-and-report-results-1","chapter":"7 Mann-Whitney U test","heading":"7.6 Interpret output and report results","text":"may get warning message console stating compute exact p-value ties. just means data points exactly value affects internal mathematics slightly. However, given p-value small, something need worry .first 5 columns give information variable (.y.), groups sample size groupThe statistic column gives t-value 841 (need reporting)p column gives us p-value 0.0006464.Given p-value less 0.05 can reject null hypothesis confidence level.\n, p-value 3rd line ‚Äôre interested . Since p-value small (much smaller standard significance level) choose say ‚Äúunlikely two samples came parent distribution can reject null hypothesis.‚Äùput completely, can state :Mann-Whitney test indicated median body length male guppies Guanapo river (18.8 mm) differs significantly median body length male guppies Aripo river (20.1 mm) (W = 841, p = 0.0006).","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"exercise-2","chapter":"7 Mann-Whitney U test","heading":"7.7 Exercise","text":"Exercise 7.1  Analyse turtle dataset using Mann Whitney test.follow process Student‚Äôs t-test.\\(H_0\\) : male median \\(=\\) female median\\(H_1\\) : male median \\(\\neq\\) female medianThis .‚Äôve already checked variances two groups similar, ‚Äôre OK . Whilst Mann-Whitney test doesn‚Äôt require normality symmetry distributions require distributions shape. example, just handful data points group, ‚Äôs quite hard make call one way another. advice case say unless ‚Äôs obvious distributions different can just allow assumption pass, ‚Äôre going see obvious differences distribution shape considerably data points .gives us exactly conclusion got two-sample t-test .e. isn‚Äôt significant difference two groups.Mann-Whitney test indicated wasn‚Äôt significant difference median Serum Cholesterol levels male female turtles (W = 26, p = 0.534)","code":"\nturtle %>% \n  wilcox_test(serum ~ sex,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.   group1 group2    n1    n2 statistic     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>\n## 1 serum Female Male       6     7        26 0.534"},{"path":"cs1-mannwhitney-u-test.html","id":"hypotheses-3","chapter":"7 Mann-Whitney U test","heading":"7.7.1 Hypotheses","text":"\\(H_0\\) : male median \\(=\\) female median\\(H_1\\) : male median \\(\\neq\\) female median","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"summarise-and-visualise-3","chapter":"7 Mann-Whitney U test","heading":"7.7.2 Summarise and visualise","text":".","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"assumptions-7","chapter":"7 Mann-Whitney U test","heading":"7.7.3 Assumptions","text":"‚Äôve already checked variances two groups similar, ‚Äôre OK . Whilst Mann-Whitney test doesn‚Äôt require normality symmetry distributions require distributions shape. example, just handful data points group, ‚Äôs quite hard make call one way another. advice case say unless ‚Äôs obvious distributions different can just allow assumption pass, ‚Äôre going see obvious differences distribution shape considerably data points .","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"carry-out-a-mann-whitney-test","chapter":"7 Mann-Whitney U test","heading":"7.7.4 Carry out a Mann-Whitney test","text":"gives us exactly conclusion got two-sample t-test .e. isn‚Äôt significant difference two groups.Mann-Whitney test indicated wasn‚Äôt significant difference median Serum Cholesterol levels male female turtles (W = 26, p = 0.534)","code":"\nturtle %>% \n  wilcox_test(serum ~ sex,\n              alternative = \"two.sided\")## # A tibble: 1 √ó 7\n##   .y.   group1 group2    n1    n2 statistic     p\n## * <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>\n## 1 serum Female Male       6     7        26 0.534"},{},{"path":"cs1-paired-two-sample-t-test.html","id":"cs1-paired-two-sample-t-test","chapter":"8 Paired two-sample t-test","heading":"8 Paired two-sample t-test","text":"paired t-test used two samples continuous data can paired (examples sort data weights individuals diet). test applicable number paired points within samples large (>30) , number points small, test also works parent distributions normally distributed.","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"section-commands-4","chapter":"8 Paired two-sample t-test","heading":"8.1 Section commands","text":"","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"data-and-hypotheses-3","chapter":"8 Paired two-sample t-test","heading":"8.2 Data and hypotheses","text":"example, suppose measure cortisol levels 20 adult females (nmol/l) first thing morning evening. want test whether cortisol levels differs two measurement times. initially form following null alternative hypotheses:\\(H_0\\): difference cortisol level times (\\(\\mu M = \\mu E\\))\\(H_1\\): difference cortisol levels times (\\(\\mu M \\neq \\mu E\\))use two-sample, two-tailed paired t-test see can reject null hypothesis.use two-sample test now two samplesWe use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller otherWe use paired test data point first sample can linked another data point second sample connecting factorWe‚Äôre using t-test ‚Äôre assuming parent populations normal equal variance (‚Äôll check bit)data stored tidy format file data/tidy/CS1-twopaired.csv.\nRead R:can see data frame consists three columns:patient_id, unique ID patienttime cortisol level measuredcortisol, contains measured value.patient_id two measurements: one morning one afternoon.","code":"\n# load the data\ncortisol <- read_csv(\"data/tidy/CS1-twopaired.csv\")\n\n# have a look at the data\ncortisol## # A tibble: 40 √ó 3\n##    patient_id time    cortisol\n##         <dbl> <chr>      <dbl>\n##  1          1 morning     311.\n##  2          2 morning     146.\n##  3          3 morning     297 \n##  4          4 morning     271.\n##  5          5 morning     268.\n##  6          6 morning     264.\n##  7          7 morning     358.\n##  8          8 morning     316.\n##  9          9 morning     336.\n## 10         10 morning     221.\n## # ‚Ä¶ with 30 more rows"},{"path":"cs1-paired-two-sample-t-test.html","id":"summarise-and-visualise-4","chapter":"8 Paired two-sample t-test","heading":"8.3 Summarise and visualise","text":"use also visualise actual data points, get sense data spread . avoid overlapping data points (try using geom_point() instead geom_jitter()), jitter data points. geom_jitter() add small amount variation point.However, plot capture cortisol level individual subject changed though. can explore individual changes morning evening creating boxplot differences two times measurement., need put data wide format, using pivot_wider().differences cortisol levels appear much less zero, (meaning evening cortisol levels appear much lower morning ones). expect test give pretty significant result.alternative representation plot data points evening morning connect patient:gives similar picture boxplot telling us, patients cortisol levels higher morning evening.","code":"\n# create a boxplot\ncortisol %>% \n  ggplot(aes(x = time, y = cortisol)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2) +\n  ylab(\"Cortisol level (nmol/l)\")\n# calculate the difference between evening and morning values\ncortisol_diff <- cortisol %>%\n  pivot_wider(names_from = time, values_from = cortisol) %>% \n  mutate(cortisol_change = evening - morning)\n\n# plot the data\n  ggplot(cortisol_diff, aes(y = cortisol_change)) +\n  geom_boxplot() +\n  ylab(\"Change in cortisol (nmol/l)\")\n# plot cortisol levels by patient\ncortisol %>% \n  ggplot(aes(x = time, y = cortisol, group = patient_id)) +\n  geom_point() +\n  geom_line()"},{"path":"cs1-paired-two-sample-t-test.html","id":"assumptions-8","chapter":"8 Paired two-sample t-test","heading":"8.4 Assumptions","text":"exercise!","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"implement-test-3","chapter":"8 Paired two-sample t-test","heading":"8.5 Implement test","text":"Perform two-sample, two-tailed, paired t-test:first argument gives formulaThe second argument gives type alternative hypothesis must one two.sided, greater lessThe third argument says data paired","code":"\n# perform the test\ncortisol %>% \n  t_test(cortisol ~ time,\n         alternative = \"two.sided\",\n         paired = TRUE)"},{"path":"cs1-paired-two-sample-t-test.html","id":"interpret-output-and-report-results-2","chapter":"8 Paired two-sample t-test","heading":"8.6 Interpret output and report results","text":"perspective value interested p column (p-value = 5.29 \\(\\cdot\\) 10-5). Given substantially less 0.05 can reject null hypothesis state:two-tailed, paired t-test indicated cortisol level adult females differed significantly morning (313.5 nmol/l) evening (197.4 nmol/l) (t = -5.1, df = 19, p = 5.3 * 10-5).","code":"## # A tibble: 1 √ó 8\n##   .y.      group1  group2     n1    n2 statistic    df         p\n## * <chr>    <chr>   <chr>   <int> <int>     <dbl> <dbl>     <dbl>\n## 1 cortisol evening morning    20    20     -5.18    19 0.0000529"},{"path":"cs1-paired-two-sample-t-test.html","id":"exercise-assumptions","chapter":"8 Paired two-sample t-test","heading":"8.7 Exercise: Assumptions","text":"Exercise 8.1  Checking assumptionsCheck assumptions necessary paired t-test.\npaired t-test appropriate test?paired test really just one-sample test disguise. actually don‚Äôt care much distributions individual groups. Instead care properties differences. paired t-test valid dataset, need differences morning evening values normally distributed.Let‚Äôs check Shapiro-Wilk Q-Q plots using cortisol_diff variable created earlier.Shapiro-Wilk test says data normal enough whilst Q-Q plot mostly fine, suggestion snaking bottom left. ‚Äôm actually OK suggestion snaking actually due single point (last point left). cover point thumb (finger choice) remaining points Q-Q plot look pretty good, suggestion snaking actually driven single point (can happen chance). ‚Äôm actually happy assumption normality well met case. single point check useful thing remember assessing diagnostic plots., yep, paired t-test appropriate dataset.","code":"\n# perform Shapiro-Wilk test on cortisol differences\ncortisol_diff %>% \n  shapiro_test(cortisol_change)## # A tibble: 1 √ó 3\n##   variable        statistic     p\n##   <chr>               <dbl> <dbl>\n## 1 cortisol_change     0.924 0.116\n# and create the Q-Q plot\ncortisol_diff %>% \n  ggplot(aes(sample = cortisol_change)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"cs1-twosample-wilcoxon-signed-rank","chapter":"9 Wilcoxon signed-rank test","heading":"9 Wilcoxon signed-rank test","text":"Wilcoxon signed-rank test alternative paired t-test. require data drawn normal distributions, require distribution differences symmetric. ‚Äôre effectively testing see median differences two samples differs significantly zero.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"section-commands-5","chapter":"9 Wilcoxon signed-rank test","heading":"9.1 Section commands","text":"new commands section.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"data-and-hypotheses-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.2 Data and hypotheses","text":"Using cortisol dataset form following null alternative hypotheses:\\(H_0\\): median difference cortisol levels two groups 0 \\((\\mu M = \\mu E)\\)\\(H_1\\): median difference cortisol levels two groups 0 \\((\\mu M \\neq \\mu E)\\)use two-tailed Wilcoxon signed-rank test see can reject null hypothesis.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-5","chapter":"9 Wilcoxon signed-rank test","heading":"9.3 Summarise and visualise","text":"Already implemented previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"assumptions-9","chapter":"9 Wilcoxon signed-rank test","heading":"9.4 Assumptions","text":"checked previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"implement-test-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.5 Implement test","text":"Perform two-tailed, Wilcoxon signed-rank test:first argument gives formulaThe second argument gives type alternative hypothesis must one two.sided, greater lessThe third argument says data paired","code":"\n# perform the test\ncortisol %>% \n  wilcox_test(cortisol ~ time,\n              alternative = \"two.sided\",\n              paired = TRUE)"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"interpret-output-and-report-results-3","chapter":"9 Wilcoxon signed-rank test","heading":"9.6 Interpret output and report results","text":"p-value given p column (p-value = 0.000168). Given less 0.05 can still reject null hypothesis.two-tailed, Wilcoxon signed-rank test indicated median cortisol level adult females differed significantly morning (320.5 nmol/l) evening (188.9 nmol/l) (V = 197, p = 0.00017).","code":"## # A tibble: 1 √ó 7\n##   .y.      group1  group2     n1    n2 statistic        p\n## * <chr>    <chr>   <chr>   <int> <int>     <dbl>    <dbl>\n## 1 cortisol evening morning    20    20        13 0.000168"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"exercise-deer-legs","chapter":"9 Wilcoxon signed-rank test","heading":"9.7 Exercise: Deer legs","text":"Exercise 9.1  Deer legsUsing following data, test null hypothesis fore hind legs deer length.results provide evidence suggest fore- hind-leg length differ deer?Write null alternative hypothesesChoose tidy representation data create csv file (‚Äôll stop asking now ‚Ä¶)Import data RSummarise visualise dataCheck assumptions (normality variance) using appropriate testsDiscuss (virtual) neighbour test appropriate?Perform testWrite sentence summarises results found\\(H_0\\) : foreleg average (mean median) \\(=\\) hindleg average (mean median)\\(H_1\\) : foreleg average \\(\\neq\\) hindleg averageFirst , need get data tidy format (every variable column, observation row). Excel, adding ID gives us:ordering data important ; first hindleg row corresponds first foreleg row, second second . indicate use id column, observation unique ID.Let‚Äôs look data see can see.suggests might difference legs, hindlegs longer forelegs. However, representation obscures fact paired data. really need look difference leg length deer data observation:Additionally, can also plot data observation:gives us much clearer picture. looks though hindlegs 4 cm longer forelegs, average. also suggests leg differences might normally distributed (data look bit skewed boxplot).need consider distribution difference leg lengths rather individual distributions.Shapiro-Wilk test Q-Q plot suggest difference data aren‚Äôt normally distributed, rules paired t-test. therefore consider paired Wilcoxon test next. Remember test requires distribution differences symmetric, whereas box-plot suggested data much skewed., frustratingly, neither tests appropriate dataset. differences foreleg hindleg lengths neither normal enough paired t-test symmetric enough Wilcoxon test don‚Äôt enough data just use t-test (‚Äôd need 30 points ). situation? Well answer aren‚Äôt actually traditional statistical tests valid dataset stands!two options available someone:try transforming raw data (take logs, square root, reciprocals) hope one leads modified dataset satisfies assumptions one tests ‚Äôve covered, oruse permutation test approach (work beyond scope course).reason included example first practical purely illustrate simple dataset apparently clear message (leg lengths differ within deer) can intractable. don‚Äôt need complex datasets go beyond capabilities classical statistics.Jeremy Clarkson put :bombshell, ‚Äôs time end. Goodnight!","code":"## # A tibble: 10 √ó 2\n##    hindleg foreleg\n##      <dbl>   <dbl>\n##  1     142     138\n##  2     140     136\n##  3     144     147\n##  4     144     139\n##  5     142     143\n##  6     146     141\n##  7     149     143\n##  8     150     145\n##  9     142     136\n## 10     148     146\n# load the data\ndeer <- read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer## # A tibble: 20 √ó 3\n##       id leg     length\n##    <dbl> <chr>    <dbl>\n##  1     1 hindleg    142\n##  2     2 hindleg    140\n##  3     3 hindleg    144\n##  4     4 hindleg    144\n##  5     5 hindleg    142\n##  6     6 hindleg    146\n##  7     7 hindleg    149\n##  8     8 hindleg    150\n##  9     9 hindleg    142\n## 10    10 hindleg    148\n## 11     1 foreleg    138\n## 12     2 foreleg    136\n## 13     3 foreleg    147\n## 14     4 foreleg    139\n## 15     5 foreleg    143\n## 16     6 foreleg    141\n## 17     7 foreleg    143\n## 18     8 foreleg    145\n## 19     9 foreleg    136\n## 20    10 foreleg    146\n# summarise the data\ndeer %>% \n  select(-id) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 1 √ó 10\n##   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 length      20   136   150    143  5.25  143.  4.01 0.896  1.88\n# or even summarise by leg type\ndeer %>% \n  select(-id) %>% \n  group_by(leg) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   leg     variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n## 2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n# we can also visualise the data\ndeer %>% \n  ggplot(aes(x = leg, y = length)) +\n  geom_boxplot()\n# create a data set that contains the difference in leg length\nleg_diff <- deer %>% \n  pivot_wider(names_from = leg, values_from = length) %>% \n  mutate(leg_diff = hindleg - foreleg)\n# plot the difference in leg length\nleg_diff %>% \n  ggplot(aes(y = leg_diff)) +\n  geom_boxplot()\n# plot the data by observation\ndeer %>% \n  ggplot(aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()\n# perform Shapiro-Wilk test on leg differences\nleg_diff %>% \n  shapiro_test(leg_diff)## # A tibble: 1 √ó 3\n##   variable statistic      p\n##   <chr>        <dbl>  <dbl>\n## 1 leg_diff     0.814 0.0212\n# and create a Q-Q plot\nleg_diff %>% \n  ggplot(aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"hypotheses-4","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.1 Hypotheses","text":"\\(H_0\\) : foreleg average (mean median) \\(=\\) hindleg average (mean median)\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"import-data-summarise-and-visualise","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.2 Import data, summarise and visualise","text":"First , need get data tidy format (every variable column, observation row). Excel, adding ID gives us:ordering data important ; first hindleg row corresponds first foreleg row, second second . indicate use id column, observation unique ID.Let‚Äôs look data see can see.suggests might difference legs, hindlegs longer forelegs. However, representation obscures fact paired data. really need look difference leg length deer data observation:Additionally, can also plot data observation:gives us much clearer picture. looks though hindlegs 4 cm longer forelegs, average. also suggests leg differences might normally distributed (data look bit skewed boxplot).","code":"\n# load the data\ndeer <- read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer## # A tibble: 20 √ó 3\n##       id leg     length\n##    <dbl> <chr>    <dbl>\n##  1     1 hindleg    142\n##  2     2 hindleg    140\n##  3     3 hindleg    144\n##  4     4 hindleg    144\n##  5     5 hindleg    142\n##  6     6 hindleg    146\n##  7     7 hindleg    149\n##  8     8 hindleg    150\n##  9     9 hindleg    142\n## 10    10 hindleg    148\n## 11     1 foreleg    138\n## 12     2 foreleg    136\n## 13     3 foreleg    147\n## 14     4 foreleg    139\n## 15     5 foreleg    143\n## 16     6 foreleg    141\n## 17     7 foreleg    143\n## 18     8 foreleg    145\n## 19     9 foreleg    136\n## 20    10 foreleg    146\n# summarise the data\ndeer %>% \n  select(-id) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 1 √ó 10\n##   variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 length      20   136   150    143  5.25  143.  4.01 0.896  1.88\n# or even summarise by leg type\ndeer %>% \n  select(-id) %>% \n  group_by(leg) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 2 √ó 11\n##   leg     variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n## 2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n# we can also visualise the data\ndeer %>% \n  ggplot(aes(x = leg, y = length)) +\n  geom_boxplot()\n# create a data set that contains the difference in leg length\nleg_diff <- deer %>% \n  pivot_wider(names_from = leg, values_from = length) %>% \n  mutate(leg_diff = hindleg - foreleg)\n# plot the difference in leg length\nleg_diff %>% \n  ggplot(aes(y = leg_diff)) +\n  geom_boxplot()\n# plot the data by observation\ndeer %>% \n  ggplot(aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"assumptions-10","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.3 Assumptions","text":"need consider distribution difference leg lengths rather individual distributions.Shapiro-Wilk test Q-Q plot suggest difference data aren‚Äôt normally distributed, rules paired t-test. therefore consider paired Wilcoxon test next. Remember test requires distribution differences symmetric, whereas box-plot suggested data much skewed.","code":"\n# perform Shapiro-Wilk test on leg differences\nleg_diff %>% \n  shapiro_test(leg_diff)## # A tibble: 1 √ó 3\n##   variable statistic      p\n##   <chr>        <dbl>  <dbl>\n## 1 leg_diff     0.814 0.0212\n# and create a Q-Q plot\nleg_diff %>% \n  ggplot(aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"conclusions","chapter":"9 Wilcoxon signed-rank test","heading":"9.7.4 Conclusions","text":", frustratingly, neither tests appropriate dataset. differences foreleg hindleg lengths neither normal enough paired t-test symmetric enough Wilcoxon test don‚Äôt enough data just use t-test (‚Äôd need 30 points ). situation? Well answer aren‚Äôt actually traditional statistical tests valid dataset stands!two options available someone:try transforming raw data (take logs, square root, reciprocals) hope one leads modified dataset satisfies assumptions one tests ‚Äôve covered, oruse permutation test approach (work beyond scope course).reason included example first practical purely illustrate simple dataset apparently clear message (leg lengths differ within deer) can intractable. don‚Äôt need complex datasets go beyond capabilities classical statistics.Jeremy Clarkson put :bombshell, ‚Äôs time end. Goodnight!","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"key-points-1","chapter":"9 Wilcoxon signed-rank test","heading":"9.8 Key points","text":"use two-sample tests see two samples continuous data come parent distributionThis essentially boils testing mean median differs two samplesThere 5 key two-sample tests: Student‚Äôs t-test, Welch‚Äôs t-test, Mann-Whitney U test, paired t-test Wilcoxon signed-rank testWhich one use depends normality distribution, sample size, paired unpaired data variance samplesParametric tests used data normally distributed sample size largeNon-parametric tests used data normally distributed sample size smallEquality variance determines test appropriateYou can ask 3 questions determine test:\ndata paired?\nneed parametric non-parametric test\ncan assume equality variance?\ndata paired?need parametric non-parametric testcan assume equality variance?","code":""},{},{"path":"cs2-intro.html","id":"cs2-intro","chapter":"10 Introduction","heading":"10 Introduction","text":"","code":""},{"path":"cs2-intro.html","id":"objectives-1","chapter":"10 Introduction","heading":"10.1 Objectives","text":"Aim: introduce R commands analysing single categorical predictors.end practical participants able perform following statistical analyses:One-way Analysis Variance (ANOVA)Kruskal-Wallis testFor , participants able :Perform test RInterpret outputCheck assumptions testCarry post-hoc test appropriateThe tests covered practical :One-way ANOVAKruskall-Wallis test","code":""},{"path":"cs2-intro.html","id":"background-1","chapter":"10 Introduction","heading":"10.2 Background","text":"practical focuses implementation various statistical tests relating categorical predictors. boil ANOVA Kruskal-Wallis (non-parametric alternative).\n, focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-1.html","id":"introduction-1","chapter":"11 Introduction","heading":"11 Introduction","text":"practical introducing can compare data different groups.","code":""},{"path":"introduction-1.html","id":"cs2-datasets","chapter":"11 Introduction","heading":"11.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"cs2-anova.html","id":"cs2-anova","chapter":"12 ANOVA","heading":"12 ANOVA","text":"","code":""},{"path":"cs2-anova.html","id":"objectives-2","chapter":"12 ANOVA","heading":"12.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data?ANOVA?check differences groups?ObjectivesBe able perform ANOVA RUnderstand ANOVA output evaluate assumptionsUnderstand post-hoc testing R","code":""},{"path":"cs2-anova.html","id":"purpose-and-aim-2","chapter":"12 ANOVA","heading":"12.2 Purpose and aim","text":"Analysis variance ANOVA test can used multiple samples continuous data. Whilst possible use ANOVA two samples, generally used three groups. used find samples came parent distributions mean. can thought generalisation two-sample Student‚Äôs t-test.","code":""},{"path":"cs2-anova.html","id":"section-commands-6","chapter":"12 ANOVA","heading":"12.3 Section commands","text":"New commands used section.also use functionality new library, ggResidPanel:","code":"\n# load ggResidPanel, for ggplot-friendly diagnostics plots\nlibrary(ggResidpanel)"},{"path":"cs2-anova.html","id":"data-and-hypotheses-5","chapter":"12 ANOVA","heading":"12.4 Data and hypotheses","text":"example, suppose measure feeding rate oyster catchers (shellfish per hour) three sites characterised degree shelter wind, imaginatively called exposed (E), partially sheltered (P) sheltered (S). want test whether data support hypothesis feeding rates don‚Äôt differ locations. form following null alternative hypotheses:\\(H_0\\): mean feeding rates three sites \\(\\mu E = \\mu P = \\mu S\\)\\(H_1\\): mean feeding rates equal.use one-way ANOVA test check .use one-way ANOVA test one predictor variable (categorical variable location).‚Äôre using ANOVA two groups don‚Äôt know better yet respect exact assumptions.data stored file CS2-oystercatcher.csv.","code":""},{"path":"cs2-anova.html","id":"summarise-and-visualise-6","chapter":"12 ANOVA","heading":"12.5 Summarise and visualise","text":"First read data.oystercatcher data set contains three columns:unique ID column ida feeding column containing feeding ratesa site column information amount shelter feeding locationFirst, get basic descriptive statistics:Next, plot data site:Looking data, appears noticeable difference feeding rates three sites. probably expect reasonably significant statistical result .","code":"\n# load data\noystercatcher <- read_csv(\"data/tidy/CS2-oystercatcher.csv\")\n\n# and have a look\noystercatcher## # A tibble: 15 √ó 3\n##       id feeding site     \n##    <dbl>   <dbl> <chr>    \n##  1     1    14.2 Exposed  \n##  2     2    16.5 Exposed  \n##  3     3     9.3 Exposed  \n##  4     4    15.1 Exposed  \n##  5     5    13.4 Exposed  \n##  6     6    18.4 Partial  \n##  7     7    13   Partial  \n##  8     8    17.4 Partial  \n##  9     9    20.4 Partial  \n## 10    10    16.5 Partial  \n## 11    11    24.1 Sheltered\n## 12    12    22.2 Sheltered\n## 13    13    25.3 Sheltered\n## 14    14    25.1 Sheltered\n## 15    15    21.5 Sheltered\n# get some basic descriptive statistics\noystercatcher %>% \n  select(-id) %>% \n  group_by(site) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   site      variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>     <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Exposed   feeding      5   9.3  16.5   14.2   1.7  13.7  2.72 1.21   3.37\n## 2 Partial   feeding      5  13    20.4   17.4   1.9  17.1  2.73 1.22   3.39\n## 3 Sheltered feeding      5  21.5  25.3   24.1   2.9  23.6  1.71 0.767  2.13\n# plot the data\noystercatcher %>% \n  ggplot(aes(x = site, y = feeding)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"assumptions-11","chapter":"12 ANOVA","heading":"12.6 Assumptions","text":"use ANOVA test, make three assumptions:parent distributions samples taken normally distributedEach data point samples independent othersThe parent distributions varianceIn similar way two-sample tests consider normality equality variance assumptions using tests graphical inspection (ignore independence assumption).","code":""},{"path":"cs2-anova.html","id":"normality","chapter":"12 ANOVA","heading":"12.6.1 Normality","text":"First perform Shapiro-Wilk test site separately.can see three groups appear normally distributed good.ANOVA however, considering group turn often considered quite excessive , cases, sufficient consider normality combined set residuals data. ‚Äôll explain residuals properly next session effectively difference data point group mean. residuals can obtained directly linear model fitted data., create linear model, extract residuals check normality:, can see combined residuals three groups appear normally distributed (expected given normally distributed individually!)","code":"\n# Shapiro-Wilk test on each site\noystercatcher %>% \n  select(-id) %>% \n  group_by(site) %>% \n  shapiro_test(feeding)\n# define the model\nlm_oystercatcher <- lm(feeding ~ site,\n                       data = oystercatcher)\n\n# extract the residuals\nresid_oyster <- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nresid_oyster %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.936   0.334"},{"path":"cs2-anova.html","id":"equality-of-variance-1","chapter":"12 ANOVA","heading":"12.6.2 Equality of Variance","text":"now test equality variance using Bartlett‚Äôs test (since ‚Äôve just found individual groups normally distributed).Perform Bartlett‚Äôs test data:relevant p-value given 3rd line. see group appear variance.","code":"\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  feeding by site\n## Bartlett's K-squared = 0.90632, df = 2, p-value = 0.6356"},{"path":"cs2-anova.html","id":"graphical-interpretation-and-diagnostic-plots","chapter":"12 ANOVA","heading":"12.6.3 Graphical interpretation and diagnostic Plots","text":"R provides convenient set graphs allow us assess assumptions graphically. simply ask R plot lm object created, can see diagnostic plots.variety ways can create diagnostic plots. right wrong - just preference.functionality base R really helpful, ‚Äôs easy create diagnostic plots built-functionality. example, can create set basic diagnostic plots using:first session already created diagnostic Q-Q plots directly data, using stat_qq() stat_qq_line(). specific plots becomes bit cumbersome. option create ggplot-friendly diagnostic plots, using ggResidPanel package. consistency tidyverse syntax used, use now , equally valid use base R functions.Create standard set diagnostic plots using ggResidPanel:Residual Plot: creates plot residuals versus predicted values. want points spread symmetrically around blue line.Q-Q Plot: creates normal quantile plot residualsIndex Plot: creates plot residuals versus observation numbers. solid blue horizontal line 0 included reference. plot can used look patterns dataHistogram: creates histogram residuals, using bins = 30 defaultThe default equivalent base R follows:second line creates three diagnostic plots (actually tries create four plots can‚Äôt dataset ‚Äôll also see warning text output screen (something hat values). ‚Äôll go next session ‚Äôs easier explain).example, two plots (top-left bottom-left) show effectively thing: distribution data group look like. allow informal check equality variance assumption.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).\nbottom-left graph, look red line want approximately horizontal.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).bottom-left graph, look red line want approximately horizontal.top-right graph familiar Q-Q plot used previously assess normality, looks combined residuals groups (much way looked Shapiro-Wilk test combined residuals).can see graphs much line ‚Äôve just looked using test, reassuring. groups appear spread data, whilst Q-Q plot isn‚Äôt perfect, appears assumption normality alright.stage, point nearly always stick graphical method assessing assumptions test. Assumptions rarely either completely met met always degree personal assessment.Whilst formal statistical tests (like Shapiro) technically fine, can often create false sense things absolutely right wrong spite fact still probabilistic statistical tests. exercises using approaches whilst gain confidence experience interpreting graphical output whilst absolutely fine use future strongly recommend don‚Äôt rely solely statistical tests isolation.","code":"# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(model_name)\n# and return the window back to normal\npar(mfrow = c(1,1))\nlm_oystercatcher %>% \n  resid_panel()\n# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(lm_oystercatcher)## hat values (leverages) are all = 0.2\n##  and there are no factor predictors; no plot no. 5\n# and return the window back to normal\npar(mfrow = c(1,1))"},{"path":"cs2-anova.html","id":"implement-test-5","chapter":"12 ANOVA","heading":"12.7 Implement test","text":"Perform ANOVA test data:first line fits linear model data (.e.¬†finds means three groups calculates load intermediary data need statistical analysis) stores information R object (‚Äôve called lm_oystercatchers, can call like). second line actually carries ANOVA analysis.first argument must formula format: response ~ predictorIf data stored stacked format, second argument must name data frameThe anova() command takes linear model object main argument","code":"\nanova(lm_oystercatcher)"},{"path":"cs2-anova.html","id":"interpret-output-and-report-results-4","chapter":"12 ANOVA","heading":"12.8 Interpret output and report results","text":"output now see console window:1st line just tells ANOVA testThe 2nd line tells response variable (case feeding)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 2 12 (‚Äôll need reporting)\nF value column contains F statistic, 21.508 (‚Äôll need reporting).\np-value 0.0001077 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) columns used calculate F statistic don‚Äôt need know .\nDf column contains degrees freedom values row, 2 12 (‚Äôll need reporting)F value column contains F statistic, 21.508 (‚Äôll need reporting).p-value 0.0001077 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) columns used calculate F statistic don‚Äôt need know .6th line symbolic codes represent big (small) p-value ; , p-value smaller 0.001 *** symbol next (). Whereas p-value 0.01 0.05 simply * character next , etc. Thankfully can cope actual numbers don‚Äôt need short-hand code determine reporting experiments (please tell ‚Äôs true‚Ä¶!), p-value ‚Äôre interested shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say ‚Äúunlikely three samples came parent distribution‚Äù can reject null hypothesis state :one-way ANOVA showed mean feeding rate oystercatchers differed significantly locations (F = 21.51, df = 2, 12, p = 0.00011).Note included (brackets) information test statistic (F = 21.51), degrees freedom (df = 2, 12), p-value (p = 0.00011).","code":"## Analysis of Variance Table\n## \n## Response: feeding\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## site       2 254.812 127.406  21.508 0.0001077 ***\n## Residuals 12  71.084   5.924                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cs2-anova.html","id":"post-hoc-testing-tukeys-rank-test","chapter":"12 ANOVA","heading":"12.9 Post-hoc testing (Tukey‚Äôs rank test)","text":"One drawback using ANOVA test tests see means , get significant result using ANOVA can say means , rather anything pairs groups differ. example, consider following boxplot three samples.group random sample 20 points normal distribution variance 1. Groups 1 2 come parent population mean 0 whereas group 3 come parent population mean 2. data clearly satisfy assumptions ANOVA test.","code":""},{"path":"cs2-anova.html","id":"read-in-the-data-and-plot","chapter":"12 ANOVA","heading":"12.9.1 Read in the data and plot","text":"","code":"\n# load the data\ntukey <- read_csv(\"data/tidy/CS2-tukey.csv\")\n\n# have a look at the data\ntukey## # A tibble: 60 √ó 3\n##       id response group  \n##    <dbl>    <dbl> <chr>  \n##  1     1    1.58  sample1\n##  2     2    0.380 sample1\n##  3     3   -0.997 sample1\n##  4     4   -0.771 sample1\n##  5     5    0.169 sample1\n##  6     6   -0.698 sample1\n##  7     7   -0.167 sample1\n##  8     8    1.38  sample1\n##  9     9   -0.839 sample1\n## 10    10   -1.05  sample1\n## # ‚Ä¶ with 50 more rows\n# plot the data\ntukey %>% \n  ggplot(aes(x = group, y = response)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"test-for-a-significant-difference-in-group-means","chapter":"12 ANOVA","heading":"12.9.2 Test for a significant difference in group means","text":"p-value 2.39 \\(\\cdot\\) 10-7 test conclusively rejected hypothesis means equal.However, due sample means different, rather just one groups different others. order drill investigate use new test called Tukey‚Äôs range test (Tukey‚Äôs honest significant difference test ‚Äì always makes think terrible cowboy/western dialogue). compare groups pairwise fashion reports whether significant difference exists.","code":"\n# create a linear model\nlm_tukey <- lm(response ~ group,\n               data = tukey)\n\n# perform an ANOVA\nanova(lm_tukey)## Analysis of Variance Table\n## \n## Response: response\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## group      2 33.850 16.9250   20.16 2.392e-07 ***\n## Residuals 57 47.854  0.8395                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cs2-anova.html","id":"performing-tukeys-test","chapter":"12 ANOVA","heading":"12.9.3 Performing Tukey‚Äôs test","text":"tukey_hsd() function takes linear model (lm_tukey) input. output pair--pair comparison different groups (samples 1 3). interested p.adj column, gives us adjusted p-value. null hypothesis case difference mean two groups. can see first row shows isn‚Äôt significant difference sample1 sample2 2nd 3rd rows show significant difference sample1 sample3, well sample2 sample3. matches expected based boxplot.","code":"\n# perform Tukey's range test on linear model\nlm_tukey %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high       p.adj\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl>       <dbl>\n## 1 group sample1 sample2          0    0.304   -0.393      1.00 0.55       \n## 2 group sample1 sample3          0    1.72     1.03       2.42 0.000000522\n## 3 group sample2 sample3          0    1.42     0.722      2.12 0.0000246  \n## # ‚Ä¶ with 1 more variable: p.adj.signif <chr>"},{"path":"cs2-anova.html","id":"assumptions-12","chapter":"12 ANOVA","heading":"12.9.4 Assumptions","text":"use Tukey‚Äôs range test matter debate (strangely enough lot statistical analysis techniques currently matters opinion rather mathematical fact ‚Äì explain little whole field appears bloody confusing!)people claim perform Tukey‚Äôs range test (post-hoc tests) preceding ANOVA test showed significant difference groups ANOVA test shown significant differences groups stop .people say rubbish can hell like, like long tell people .background rather involved one reasons debate prevent -called data-dredging p-hacking. scientists/analysts fixated getting ‚Äúsignificant‚Äù result perform huge variety statistical techniques find one shows data significant (particular problem psychological studies ‚Äì point fingers though, working hard sort stuff . Kudos!).Whether use post-hoc testing depend experimental design questions ‚Äôre attempting answer.Tukey‚Äôs range test, decide use , requires three assumptions ANOVA test:Normality distributionsEquality variance groupsIndependence observations","code":""},{"path":"cs2-anova.html","id":"exercise-lobster-weight","chapter":"12 ANOVA","heading":"12.10 Exercise: Lobster weight","text":"Exercise 12.1  Juvenile lobster weightJuvenile lobsters aquaculture grown three different diets (fresh mussels, semi-dry pellets dry flakes). nine weeks, wet weight :evidence diet affects growth rate lobsters?Write null alternative hypothesesImport data R\ndata stored tidy format data/tidy/CS2-lobsters.csv\ndata stored tidy format data/tidy/CS2-lobsters.csvSummarise visualise dataCheck assumptions using appropriate tests graphical analysesPerform ANOVA testWrite sentence summarise results foundPerform post-hoc test report findings\\(H_0\\) : means equal\\(H_1\\) : means equalThe data stored .csv file columns called id, weight diet.Next, visualise data:always use plot summary assess three things:load data properly?see three groups reasonable values. aren‚Äôt data points obviously wrong (negative, zero massively big) right number groups. looks didn‚Äôt anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ‚Äôs quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ‚Äôre immediately massively worried lack normality. , Flakes Mussels appear similar variances ‚Äôs bit hard decide ‚Äôs going Pellets. ‚Äôs hard say ‚Äôs going assumptions ‚Äôll wait see tests say.NormalityWe‚Äôll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ‚Äôll use Q-Q plot.First, perform Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let‚Äôs look Shapiro-Wilk test data together:hand says everything fine. Let‚Äôs look Q-Q plot., ‚Äôve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots. can tell resid_panel() plot specific one, using plots = arguments. want know look help documentation using ?resid_panel.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ‚Äôd happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn‚Äôt confident.Equality VarianceWe‚Äôll consider Bartlett test ‚Äôll look diagnostic plots .code ‚Äôve specified diagnostic plots wanted. also added loess smoother line (smoother = TRUE) plotsThe Residuals Plot. ‚Äôre looking points evenly spread either side line. Looks good.Location-Scale Plot (displayed default base R‚Äôs diagnostic plots). ‚Äôre looking red line. line less horizontal, equality variance assumption met.three methods agree isn‚Äôt issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.assumptions normality equality variance met can confident one-way ANOVA appropriate test.one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging ‚Äúsignificant‚Äù results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey‚Äôs range test, important point result one-way ANOVA test doesn‚Äôt preclude carrying Tukey test.","code":"\n# load the data\nlobsters <- read_csv(\"data/tidy/CS2-lobsters.csv\")\n\n# look at the data\nlobsters## # A tibble: 18 √ó 3\n##       id weight diet   \n##    <dbl>  <dbl> <chr>  \n##  1     1  152.  Mussels\n##  2     2  132.  Mussels\n##  3     3  104.  Mussels\n##  4     4  154.  Mussels\n##  5     5  132   Mussels\n##  6     6  119   Mussels\n##  7     7  162.  Mussels\n##  8     8  118.  Pellets\n##  9     9  111.  Pellets\n## 10    10  129.  Pellets\n## 11    11  110.  Pellets\n## 12    12  175.  Pellets\n## 13    13  102.  Flakes \n## 14    14  103.  Flakes \n## 15    15   90.4 Flakes \n## 16    16  133.  Flakes \n## 17    17  129.  Flakes \n## 18    18  129.  Flakes\n# create some summary statistics\nlobsters %>% \n  select(-id) %>% \n  group_by(diet) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   diet    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n## 2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n## 3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n# Shapiro-Wilk on lobster groups\nlobsters %>% \n  group_by(diet) %>% \n  shapiro_test(weight)## # A tibble: 3 √ó 4\n##   diet    variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Flakes  weight       0.844 0.140 \n## 2 Mussels weight       0.948 0.710 \n## 3 Pellets weight       0.767 0.0425\n# create a linear model\nlm_lobsters <- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters <- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.948   0.391\n# Q-Q plots\nlm_lobsters %>% \n  resid_panel(plots = \"qq\")\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n# plot the residuals and scale-location plots\nlm_lobsters %>% \n  resid_panel(plots = c(\"resid\", \"ls\"),\n              smoother = TRUE)\nanova(lm_lobsters)## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87\nlm_lobsters %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high p.adj p.adj.signif\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl> <dbl> <chr>       \n## 1 diet  Flakes  Mussels          0    21.9     -9.66      53.5 0.202 ns          \n## 2 diet  Flakes  Pellets          0    14.0    -20.3       48.4 0.551 ns          \n## 3 diet  Mussels Pellets          0    -7.85   -41.1       25.4 0.815 ns"},{"path":"cs2-anova.html","id":"hypotheses-5","chapter":"12 ANOVA","heading":"12.10.1 Hypotheses","text":"\\(H_0\\) : means equal\\(H_1\\) : means equal","code":""},{"path":"cs2-anova.html","id":"import-data-summarise-and-visualise-1","chapter":"12 ANOVA","heading":"12.10.2 Import Data, summarise and visualise","text":"data stored .csv file columns called id, weight diet.Next, visualise data:always use plot summary assess three things:load data properly?see three groups reasonable values. aren‚Äôt data points obviously wrong (negative, zero massively big) right number groups. looks didn‚Äôt anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ‚Äôs quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ‚Äôre immediately massively worried lack normality. , Flakes Mussels appear similar variances ‚Äôs bit hard decide ‚Äôs going Pellets. ‚Äôs hard say ‚Äôs going assumptions ‚Äôll wait see tests say.","code":"\n# load the data\nlobsters <- read_csv(\"data/tidy/CS2-lobsters.csv\")\n\n# look at the data\nlobsters## # A tibble: 18 √ó 3\n##       id weight diet   \n##    <dbl>  <dbl> <chr>  \n##  1     1  152.  Mussels\n##  2     2  132.  Mussels\n##  3     3  104.  Mussels\n##  4     4  154.  Mussels\n##  5     5  132   Mussels\n##  6     6  119   Mussels\n##  7     7  162.  Mussels\n##  8     8  118.  Pellets\n##  9     9  111.  Pellets\n## 10    10  129.  Pellets\n## 11    11  110.  Pellets\n## 12    12  175.  Pellets\n## 13    13  102.  Flakes \n## 14    14  103.  Flakes \n## 15    15   90.4 Flakes \n## 16    16  133.  Flakes \n## 17    17  129.  Flakes \n## 18    18  129.  Flakes\n# create some summary statistics\nlobsters %>% \n  select(-id) %>% \n  group_by(diet) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   diet    variable     n   min   max median   iqr  mean    sd    se    ci\n##   <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n## 2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n## 3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\nlobsters %>% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()"},{"path":"cs2-anova.html","id":"explore-assumptions","chapter":"12 ANOVA","heading":"12.10.3 Explore Assumptions","text":"NormalityWe‚Äôll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ‚Äôll use Q-Q plot.First, perform Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let‚Äôs look Shapiro-Wilk test data together:hand says everything fine. Let‚Äôs look Q-Q plot., ‚Äôve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots. can tell resid_panel() plot specific one, using plots = arguments. want know look help documentation using ?resid_panel.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ‚Äôd happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn‚Äôt confident.Equality VarianceWe‚Äôll consider Bartlett test ‚Äôll look diagnostic plots .code ‚Äôve specified diagnostic plots wanted. also added loess smoother line (smoother = TRUE) plotsThe Residuals Plot. ‚Äôre looking points evenly spread either side line. Looks good.Location-Scale Plot (displayed default base R‚Äôs diagnostic plots). ‚Äôre looking red line. line less horizontal, equality variance assumption met.three methods agree isn‚Äôt issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.","code":"\n# Shapiro-Wilk on lobster groups\nlobsters %>% \n  group_by(diet) %>% \n  shapiro_test(weight)## # A tibble: 3 √ó 4\n##   diet    variable statistic      p\n##   <chr>   <chr>        <dbl>  <dbl>\n## 1 Flakes  weight       0.844 0.140 \n## 2 Mussels weight       0.948 0.710 \n## 3 Pellets weight       0.767 0.0425\n# create a linear model\nlm_lobsters <- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters <- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %>% \n  shapiro_test()## # A tibble: 1 √ó 3\n##   variable statistic p.value\n##   <chr>        <dbl>   <dbl>\n## 1 .            0.948   0.391\n# Q-Q plots\nlm_lobsters %>% \n  resid_panel(plots = \"qq\")\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n# plot the residuals and scale-location plots\nlm_lobsters %>% \n  resid_panel(plots = c(\"resid\", \"ls\"),\n              smoother = TRUE)"},{"path":"cs2-anova.html","id":"carry-out-one-way-anova","chapter":"12 ANOVA","heading":"12.10.4 Carry out one-way ANOVA","text":"assumptions normality equality variance met can confident one-way ANOVA appropriate test.","code":"\nanova(lm_lobsters)## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87"},{"path":"cs2-anova.html","id":"result","chapter":"12 ANOVA","heading":"12.10.5 Result","text":"one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).","code":""},{"path":"cs2-anova.html","id":"post-hoc-testing-with-tukey","chapter":"12 ANOVA","heading":"12.10.6 Post-hoc testing with Tukey","text":"can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging ‚Äúsignificant‚Äù results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey‚Äôs range test, important point result one-way ANOVA test doesn‚Äôt preclude carrying Tukey test.","code":"\nlm_lobsters %>% \n  tukey_hsd()## # A tibble: 3 √ó 9\n##   term  group1  group2  null.value estimate conf.low conf.high p.adj p.adj.signif\n## * <chr> <chr>   <chr>        <dbl>    <dbl>    <dbl>     <dbl> <dbl> <chr>       \n## 1 diet  Flakes  Mussels          0    21.9     -9.66      53.5 0.202 ns          \n## 2 diet  Flakes  Pellets          0    14.0    -20.3       48.4 0.551 ns          \n## 3 diet  Mussels Pellets          0    -7.85   -41.1       25.4 0.815 ns"},{"path":"cs2-anova.html","id":"key-points-2","chapter":"12 ANOVA","heading":"12.11 Key points","text":"use ANOVA test difference means multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform ANOVA linear model anova()check assumptions diagnostic plots check residuals normally distributedWe use post-hoc testing check significant differences group means, example using Tukey‚Äôs range test","code":""},{},{"path":"kruskal-wallis-test.html","id":"kruskal-wallis-test","chapter":"13 Kruskal-Wallis test","heading":"13 Kruskal-Wallis test","text":"","code":""},{"path":"kruskal-wallis-test.html","id":"objectives-3","chapter":"13 Kruskal-Wallis test","heading":"13.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data data normally distributed?Kruskal-Wallis test?check differences groups?ObjectivesBe able perform Kruskal-Wallis test RUnderstand output test evaluate assumptionsBe able perform post-hoc testing Kruskal-Wallis test","code":""},{"path":"kruskal-wallis-test.html","id":"purpose-and-aim-3","chapter":"13 Kruskal-Wallis test","heading":"13.2 Purpose and aim","text":"Kruskal-Wallis one-way analysis variance test analogue ANOVA can used assumption normality met. way extension Mann-Whitney test two groups.","code":""},{"path":"kruskal-wallis-test.html","id":"section-commands-7","chapter":"13 Kruskal-Wallis test","heading":"13.3 Section commands","text":"New commands used section:","code":""},{"path":"kruskal-wallis-test.html","id":"data-and-hypotheses-6","chapter":"13 Kruskal-Wallis test","heading":"13.4 Data and hypotheses","text":"example, suppose behavioural ecologist records rate spider monkeys behaved aggressively towards one another function closely related two monkeys . familiarity two monkeys involved interaction classified high, low none. want test data support hypothesis aggression rates differ according strength relatedness. form following null alternative hypotheses:\\(H_0\\): median aggression rates types familiarity \\(H_1\\): median aggression rates equalWe use Kruskal-Wallis test check .data stored file data/raw/CS2-spidermonkey.csv.First read data :","code":"\nspidermonkey <- read_csv(\"data/tidy/CS2-spidermonkey.csv\")"},{"path":"kruskal-wallis-test.html","id":"summarise-and-visualise-7","chapter":"13 Kruskal-Wallis test","heading":"13.5 Summarise and visualise","text":"data appear show significant difference aggression rates three types familiarity. probably expect reasonably significant result .","code":"\n# look at the data\nspidermonkey## # A tibble: 21 √ó 3\n##       id aggression familiarity\n##    <dbl>      <dbl> <chr>      \n##  1     1        0.2 high       \n##  2     2        0.1 high       \n##  3     3        0.4 high       \n##  4     4        0.8 high       \n##  5     5        0.3 high       \n##  6     6        0.5 high       \n##  7     7        0.2 high       \n##  8     8        0.5 low        \n##  9     9        0.4 low        \n## 10    10        0.3 low        \n## # ‚Ä¶ with 11 more rows\n# summarise the data\nspidermonkey %>% \n  select(-id) %>% \n  group_by(familiarity) %>% \n  get_summary_stats(type = \"common\")## # A tibble: 3 √ó 11\n##   familiarity variable       n   min   max median   iqr  mean    sd    se    ci\n##   <chr>       <chr>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 high        aggression     7   0.1   0.8    0.3  0.25 0.357 0.237 0.09  0.219\n## 2 low         aggression     7   0.3   1.2    0.5  0.3  0.629 0.315 0.119 0.291\n## 3 none        aggression     7   0.9   1.6    1.2  0.25 1.26  0.23  0.087 0.213\n# create boxplot\nspidermonkey %>% \n  ggplot(aes(x = familiarity, y = aggression)) +\n  geom_boxplot()"},{"path":"kruskal-wallis-test.html","id":"assumptions-13","chapter":"13 Kruskal-Wallis test","heading":"13.6 Assumptions","text":"use Kruskal-Wallis test make three assumptions:parent distributions samples drawn shape (‚Äôre normal use one-way ANOVA)data point samples independent othersThe parent distributions varianceIndependence ‚Äôll ignore usual. Similar shape best assessed earlier visualisation data. means need check equality variance.","code":""},{"path":"kruskal-wallis-test.html","id":"equality-of-variance-2","chapter":"13 Kruskal-Wallis test","heading":"13.6.1 Equality of variance","text":"test equality variance using Levene‚Äôs test (since can‚Äôt assume normal parent distributions rules Bartlett‚Äôs test).relevant p-value given p column (0.893). quite large see group appear variance.also warning group coerced factor. need worry - Levene‚Äôs test needs compare different groups aggression encoded numeric value, converts categorical one running test.","code":"\n# perform Levene's test\nspidermonkey %>% \n  levene_test(aggression ~ familiarity)## Warning in leveneTest.default(y = y, group = group, ...): group coerced to\n## factor.## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    18     0.114 0.893"},{"path":"kruskal-wallis-test.html","id":"implement-test-6","chapter":"13 Kruskal-Wallis test","heading":"13.7 Implement test","text":"Perform Kruskal-Wallis test data:kruskal_test() takes formula following format: variable ~ category","code":"\n# implement Kruskal-Wallis test\nspidermonkey %>% \n  kruskal_test(aggression ~ familiarity)\n\nkruskal.test(aggression ~ familiarity, data = spidermonkey)"},{"path":"kruskal-wallis-test.html","id":"interpret-output-and-report-results-5","chapter":"13 Kruskal-Wallis test","heading":"13.8 Interpret output and report results","text":"output now see console window:p-value given p column. shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say ‚Äúunlikely three samples came parent distribution can reject null hypothesis‚Äù state :one-way Kruskal-Wallis rank sum test showed aggression rates spidermonkeys depends upon degree familiarity (KW = 13.597, df = 2, p = 0.0011).","code":"## # A tibble: 1 √ó 6\n##   .y.            n statistic    df       p method        \n## * <chr>      <int>     <dbl> <int>   <dbl> <chr>         \n## 1 aggression    21      13.6     2 0.00112 Kruskal-Wallis## \n##  Kruskal-Wallis rank sum test\n## \n## data:  aggression by familiarity\n## Kruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115"},{"path":"kruskal-wallis-test.html","id":"post-hoc-testing-dunns-test","chapter":"13 Kruskal-Wallis test","heading":"13.9 Post-hoc testing (Dunn‚Äôs test)","text":"equivalent Tukey‚Äôs range test non-normal data Dunn‚Äôs test.Dunn‚Äôs test used check significant differences group medians:give following output:dunn_test() function performs Kruskal-Wallis test data, followed post-hoc pairwise multiple comparison.comparison pairs groups reported table bottom. row contains single comparison. interested p p.adj columns, contain p-values want. table shows isn‚Äôt significant difference high low groups, p-value (0.1598) high. two comparisons high familiarity familiarity groups low groups significant though.dunn_test() function several arguments, p.adjust.method likely interest. can define method needs used account multiple comparisons. default \"holm\". ‚Äôll cover chapter Power analysis.","code":"\n# perform Dunn's test\nspidermonkey %>% \n  dunn_test(aggression ~ familiarity)## # A tibble: 3 √ó 9\n##   .y.        group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n## * <chr>      <chr>  <chr>  <int> <int>     <dbl>    <dbl>    <dbl> <chr>       \n## 1 aggression high   low        7     7      1.41 0.160    0.160    ns          \n## 2 aggression high   none       7     7      3.66 0.000257 0.000771 ***         \n## 3 aggression low    none       7     7      2.25 0.0245   0.0490   *"},{"path":"kruskal-wallis-test.html","id":"exercise-lobster-weight-1","chapter":"13 Kruskal-Wallis test","heading":"13.10 Exercise: Lobster weight","text":"Exercise 13.1  Kruskal-Wallis Dunn‚Äôs test lobster dataPerform Kruskal-Wallis test post-hoc test lobster data set.\\(H_0\\) : medians equal\\(H_1\\) : medians equalAll done previously., since data normal enough definitely similar enough Kruskal-Wallis test equality variance assessment diagnostic plots. completeness though look Levene‚Äôs testGiven p-value high, agrees previous assessment equality variance assumption well met. Rock .Kruskal-Wallis test indicated median weight juvenile lobsters differ significantly diets (KW = 3.26, df = 2, p = 0.20).Although rather unnecessary (likely unwanted, since don‚Äôt want p-hacking), detect significant differences diets, can perform non-parametric equivalent Tukey‚Äôs range test: Dunn‚Äôs test.can see none comparisons significant, either based uncorrected p-values (p) p-values adjusted multiple comparisons (p.adj). consistent found previously.","code":"\nlobsters %>% \n  levene_test(weight ~ diet)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    15   0.00280 0.997\n# implement Kruskal-Wallis test\nlobsters %>% \n  kruskal_test(weight ~ diet)## # A tibble: 1 √ó 6\n##   .y.        n statistic    df     p method        \n## * <chr>  <int>     <dbl> <int> <dbl> <chr>         \n## 1 weight    18      3.26     2 0.196 Kruskal-Wallis\n# perform Dunn's test\nlobsters %>% \n  dunn_test(weight ~ diet)## # A tibble: 3 √ó 9\n##   .y.    group1  group2     n1    n2 statistic      p p.adj p.adj.signif\n## * <chr>  <chr>   <chr>   <int> <int>     <dbl>  <dbl> <dbl> <chr>       \n## 1 weight Flakes  Mussels     6     7     1.79  0.0738 0.221 ns          \n## 2 weight Flakes  Pellets     6     5     0.670 0.503  0.629 ns          \n## 3 weight Mussels Pellets     7     5    -1.01  0.315  0.629 ns"},{"path":"kruskal-wallis-test.html","id":"hypotheses-6","chapter":"13 Kruskal-Wallis test","heading":"13.10.1 Hypotheses","text":"\\(H_0\\) : medians equal\\(H_1\\) : medians equal","code":""},{"path":"kruskal-wallis-test.html","id":"import-data-summarise-and-visualise-2","chapter":"13 Kruskal-Wallis test","heading":"13.10.2 Import data, summarise and visualise","text":"done previously.","code":""},{"path":"kruskal-wallis-test.html","id":"assumptions-14","chapter":"13 Kruskal-Wallis test","heading":"13.10.3 Assumptions","text":", since data normal enough definitely similar enough Kruskal-Wallis test equality variance assessment diagnostic plots. completeness though look Levene‚Äôs testGiven p-value high, agrees previous assessment equality variance assumption well met. Rock .","code":"\nlobsters %>% \n  levene_test(weight ~ diet)## # A tibble: 1 √ó 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     2    15   0.00280 0.997"},{"path":"kruskal-wallis-test.html","id":"kruskal-wallis-test-1","chapter":"13 Kruskal-Wallis test","heading":"13.10.4 Kruskal-Wallis test","text":"Kruskal-Wallis test indicated median weight juvenile lobsters differ significantly diets (KW = 3.26, df = 2, p = 0.20).","code":"\n# implement Kruskal-Wallis test\nlobsters %>% \n  kruskal_test(weight ~ diet)## # A tibble: 1 √ó 6\n##   .y.        n statistic    df     p method        \n## * <chr>  <int>     <dbl> <int> <dbl> <chr>         \n## 1 weight    18      3.26     2 0.196 Kruskal-Wallis"},{"path":"kruskal-wallis-test.html","id":"post-hoc-dunns-test","chapter":"13 Kruskal-Wallis test","heading":"13.10.5 Post-hoc Dunn‚Äôs test","text":"Although rather unnecessary (likely unwanted, since don‚Äôt want p-hacking), detect significant differences diets, can perform non-parametric equivalent Tukey‚Äôs range test: Dunn‚Äôs test.can see none comparisons significant, either based uncorrected p-values (p) p-values adjusted multiple comparisons (p.adj). consistent found previously.","code":"\n# perform Dunn's test\nlobsters %>% \n  dunn_test(weight ~ diet)## # A tibble: 3 √ó 9\n##   .y.    group1  group2     n1    n2 statistic      p p.adj p.adj.signif\n## * <chr>  <chr>   <chr>   <int> <int>     <dbl>  <dbl> <dbl> <chr>       \n## 1 weight Flakes  Mussels     6     7     1.79  0.0738 0.221 ns          \n## 2 weight Flakes  Pellets     6     5     0.670 0.503  0.629 ns          \n## 3 weight Mussels Pellets     7     5    -1.01  0.315  0.629 ns"},{"path":"kruskal-wallis-test.html","id":"key-points-3","chapter":"13 Kruskal-Wallis test","heading":"13.11 Key points","text":"use Kruskal-Wallis test see difference medians multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform Kruskal-Wallis test linear model kruskal_test()assume parent distributions shape; data point independent parent distributions varianceWe test equality variance using levene_test()Post-hoc testing check significant differences group medians done dunn_test()","code":""},{},{"path":"cs3-intro.html","id":"cs3-intro","chapter":"14 Introduction","heading":"14 Introduction","text":"","code":""},{"path":"cs3-intro.html","id":"objectives-4","chapter":"14 Introduction","heading":"14.1 Objectives","text":"Aim: introduce R commands analysing simple linear modelsBy end practical participants able perform following statistical analyses:Simple Linear RegressionCorrelationFor , participants able :Perform test RInterpret outputCheck assumptions test","code":""},{"path":"cs3-intro.html","id":"background-2","chapter":"14 Introduction","heading":"14.2 Background","text":"practical focuses implementation various statistical tests relating simple linear regression correlation., focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-2.html","id":"introduction-2","chapter":"15 Introduction","heading":"15 Introduction","text":"practical introducing can compare data different continuous variables.","code":""},{"path":"introduction-2.html","id":"cs3-datasets","chapter":"15 Introduction","heading":"15.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"correlation-coefficients.html","id":"correlation-coefficients","chapter":"16 Correlation coefficients","heading":"16 Correlation coefficients","text":"","code":""},{"path":"correlation-coefficients.html","id":"objectives-5","chapter":"16 Correlation coefficients","heading":"16.1 Objectives","text":"QuestionsWhat correlation coefficients?kind correlation coefficients use ?ObjectivesBe able calculate correlation coefficients RUse visual tools explore correlations variablesKnow limitations correlation coefficients","code":""},{"path":"correlation-coefficients.html","id":"purpose-and-aim-4","chapter":"16 Correlation coefficients","heading":"16.2 Purpose and aim","text":"Correlation refers relationship two variables (datasets) one another. Two datasets said correlated independent one another. Correlations can useful can indicate predictive relationship may exist. However just two datasets correlated mean causally related.","code":""},{"path":"correlation-coefficients.html","id":"section-commands-8","chapter":"16 Correlation coefficients","heading":"16.3 Section commands","text":"New commands used section:","code":""},{"path":"correlation-coefficients.html","id":"data-and-hypotheses-7","chapter":"16 Correlation coefficients","heading":"16.4 Data and hypotheses","text":"use USArrests dataset example. rather bleak dataset contains statistics arrests per 100,000 residents assault, murder robbery 50 US states 1973, alongside proportion population lived urban areas time. USArrests data frame 50 observations five variables: state, murder, assault, urban_pop robbery.data stored file data/tidy/CS3-usarrests.csv.First read data:","code":"\n# load the data\nUSArrests <- read_csv(\"data/tidy/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests## # A tibble: 50 √ó 5\n##    state       murder assault urban_pop robbery\n##    <chr>        <dbl>   <dbl>     <dbl>   <dbl>\n##  1 Alabama       13.2     236        58    21.2\n##  2 Alaska        10       263        48    44.5\n##  3 Arizona        8.1     294        80    31  \n##  4 Arkansas       8.8     190        50    19.5\n##  5 California     9       276        91    40.6\n##  6 Colorado       7.9     204        78    38.7\n##  7 Connecticut    3.3     110        77    11.1\n##  8 Delaware       5.9     238        72    15.8\n##  9 Florida       15.4     335        80    31.9\n## 10 Georgia       17.4     211        60    25.8\n## # ‚Ä¶ with 40 more rows"},{"path":"correlation-coefficients.html","id":"pearsons-product-moment-correlation-coefficient","chapter":"16 Correlation coefficients","heading":"16.5 Pearson‚Äôs product moment correlation coefficient","text":"Pearson‚Äôs r (quantity also known) measure linear correlation two variables. value -1 +1, +1 means perfect positive correlation, -1 means perfect negative correlation 0 means correlation .can look correlations need reformat data little bit. functions ‚Äôre going use require data frames contain numbers input. want keep state information linked data, need define state column name rows.need update original USArrests data frame, ‚Äôre just piping displaying output can see ‚Äôs going .","code":"\n# convert the state column to row names\nUSArrests %>% \n  column_to_rownames(var = \"state\")"},{"path":"correlation-coefficients.html","id":"summarise-and-visualise-8","chapter":"16 Correlation coefficients","heading":"16.6 Summarise and visualise","text":"Knowing reformatting works, can first visualise data:argument lower.panel tells R add redundant reflected lower set plots, diagonalFrom visual inspection scatter plots can see appears slight positive correlation pairs variables, although may weak cases (murder urban_pop example).","code":"\n# create correlation plot\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"implement-test-7","chapter":"16 Correlation coefficients","heading":"16.7 Implement test","text":"can calculate Pearson‚Äôs correlation coefficients pair variables (e.g.¬†coefficient murder assault). several functions allow . cor() function base R cor_mat() rstatix package, spit results matrix (grid) format. ‚Äôll use cor_mat() can keep using tibble data sets.First create matrix, keeping state data linked row namesThe method argument tells R correlation coefficient use (pearson (default), kendall, spearman)","code":"\n# calculate Pearson's correlation coefficients\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-6","chapter":"16 Correlation coefficients","heading":"16.8 Interpret output and report results","text":"give following output:table gives correlation coefficient pair variables data frame. correlated variables murder assault r value 0.80. appears agree well set scatter plots produced earlier.","code":"## # A tibble: 4 √ó 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.8       0.07    0.56\n## 2 assault     0.8     1         0.26    0.67\n## 3 urban_pop   0.07    0.26      1       0.41\n## 4 robbery     0.56    0.67      0.41    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-pearson","chapter":"16 Correlation coefficients","heading":"16.9 Exercise: State data (Pearson)","text":"Exercise 16.1  Pearson‚Äôs correlation USA state dataWe use data file data/tidy/CS3-statedata.csv dataset exercise. rather benign dataset contains information general properties US state, population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (‚Äôs getting away ), percentage population high-school graduates, average number days minimum temperature freezing 1931 1960, state area square miles. dataset contains 50 rows 8 columns, column names: population, income, illiteracy, life_exp, murder, hs_grad, frost area.Load data (remembering tell R first column CSV file used specify row names dataset) use pairs() command visually identify 3 different pairs variables appear bethe positively correlatedthe negatively correlatednot correlated allCalculate Pearson‚Äôs r variable pairs see well able identify correlation visually.get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 √ó 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # ‚Ä¶ with 40 more rows\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 √ó 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # ‚Ä¶ with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 √ó 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # ‚Ä¶ with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"read-in-the-data","chapter":"16 Correlation coefficients","heading":"16.9.1 Read in the data","text":"","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 √ó 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # ‚Ä¶ with 40 more rows"},{"path":"correlation-coefficients.html","id":"pair-wise-comparisons-visual","chapter":"16 Correlation coefficients","heading":"16.9.2 Pair-wise comparisons (visual)","text":"","code":"\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"calculate-the-correlation-coefficients","chapter":"16 Correlation coefficients","heading":"16.9.3 Calculate the correlation coefficients","text":"get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 √ó 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # ‚Ä¶ with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 √ó 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # ‚Ä¶ with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"spearmans-rank-correlation-coefficient","chapter":"16 Correlation coefficients","heading":"16.10 Spearman‚Äôs rank correlation coefficient","text":"test first calculates rank numerical data (.e.¬†position smallest (negative) largest (positive)) two variables calculates Pearson‚Äôs product moment correlation coefficient using ranks. consequence, test less sensitive outliers distribution.","code":""},{"path":"correlation-coefficients.html","id":"implement-test-8","chapter":"16 Correlation coefficients","heading":"16.11 Implement test","text":"using USArrests data set , run command:Remember cor_mat() requires matrix, use state column row namesThe argument method tells R correlation coefficient use","code":"\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-7","chapter":"16 Correlation coefficients","heading":"16.12 Interpret output and report results","text":"gives following output:table gives correlation coefficient pair variables data frame. Slightly annoyingly, pair occurs twice opposite direction.","code":"## # A tibble: 4 √ó 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.82      0.11    0.68\n## 2 assault     0.82    1         0.28    0.71\n## 3 urban_pop   0.11    0.28      1       0.44\n## 4 robbery     0.68    0.71      0.44    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-spearman","chapter":"16 Correlation coefficients","heading":"16.13 Exercise: State data (Spearman)","text":"Exercise 16.2  Spearman‚Äôs correlation USA state dataCalculate Spearman‚Äôs correlation coefficient data/tidy/CS3-statedata.csv dataset.variable‚Äôs correlations affected use Spearman‚Äôs rank compared Pearson‚Äôs r?reference scatter plot produced earlier, can explain might ?Remember use column_to_rownames(var = \"state\") argument load data matrixInstead eye-balling differences, think can determine difference two correlation matricesThe cor_plot() function can useful visualise matricesIn order determine variables affected choice Spearman vs Pearson just plot matrices side side try spot going , one reasons ‚Äôre using R can bit programmatic things. Also, eyes aren‚Äôt good processing parsing sort information display. better way somehow visualise data.Let‚Äôs calculate difference two correlation matrices. create correlation matrix using cor_mat(). Next remove rowname, ‚Äôre left just data frame containing numbers. way can subtract values two data frames.Lastly, use cor_plot() function plot heatmap differences.one cases using tidyverse actually necessarily easiest way. similar thing using base R syntax:plot coloured blue red, indicating biggest positive differences correlation coefficients blue. biggest negative differences coloured red, whereas least difference indicated white.plot symmetric along leading diagonal (hopefully obvious reasons) can see majority squares light blue light red colour, means isn‚Äôt much difference Spearman Pearson vast majority variables. squares appear darkest look along area row/column suggesting ‚Äôs big difference correlation coefficients .can now revisit pairwise scatter plot see makes sense:can see clearly correspond plots noticeable outliers. example, Alaska twice big next biggest state, Texas. Big outliers data can large impact Pearson coefficient, whereas Spearman coefficient robust effects outliers. can see detail look area vs income graph coefficients. Pearson gives value 0.36, slight positive correlation, whereas Spearman gives value 0.057, basically uncorrelated. single outlier (Alaska) top-right scatter plot big effect Pearson practically ignored Spearman.Well done, Mr.¬†Spearman.","code":"\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")## # A tibble: 8 √ó 9\n##   rowname    population income illiteracy life_exp murder hs_grad frost   area\n## * <chr>           <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n## 1 population       1     0.12        0.31    -0.1    0.35   -0.38 -0.46 -0.12 \n## 2 income           0.12  1          -0.31     0.32  -0.22    0.51  0.2   0.057\n## 3 illiteracy       0.31 -0.31        1       -0.56   0.67   -0.65 -0.68 -0.25 \n## 4 life_exp        -0.1   0.32       -0.56     1     -0.78    0.52  0.3   0.13 \n## 5 murder           0.35 -0.22        0.67    -0.78   1      -0.44 -0.54  0.11 \n## 6 hs_grad         -0.38  0.51       -0.65     0.52  -0.44    1     0.4   0.44 \n## 7 frost           -0.46  0.2        -0.68     0.3   -0.54    0.4   1     0.11 \n## 8 area            -0.12  0.057      -0.25     0.13   0.11    0.44  0.11  1\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_pear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\") %>% \n  # remove the row names\n  select(-rowname)\n\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_spear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\") %>% \n  # remove the row names\n  select(-rowname)\n\n# calculate the difference between Pearson's and Spearman's\nUSAstate_diff <- USAstate_pear - USAstate_spear\n\n# use the column names of the data set as rownames\nrownames(USAstate_diff) <- names(USAstate_diff)\n\nUSAstate_diff %>%\n  cor_plot()\n# read in the data with the base R read.csv function\n# and assign the first column as row names\nUSAstate_base <- read.csv(\"data/tidy/CS3-statedata.csv\", row.names = 1)\n\n# calculate a correlation matrix using Pearson's\ncorPear <- cor(USAstate_base, method = \"pearson\")\n\n# calculate a correlation matrix using Spearman\ncorSpea <- cor(USAstate_base, method = \"spearman\")\n\n# calculate the difference between the two matrices\ncorDiff <- corPear - corSpea\n\n# and plot it, like before\ncorDiff %>% \n  cor_plot()\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"key-points-4","chapter":"16 Correlation coefficients","heading":"16.14 Key points","text":"Correlation degree two variables linearly relatedCorrelation imply causationWe can visualise correlations using pairs() cor_plot() functionsUsing cor_mat() cor_test() functions can calculate correlation matricesTwo main correlation coefficients Pearson‚Äôs r Spearman‚Äôs rank, Spearman‚Äôs rank less sensitive outliers","code":""},{},{"path":"linear-regression.html","id":"linear-regression","chapter":"17 Linear regression","heading":"17 Linear regression","text":"","code":""},{"path":"linear-regression.html","id":"objectives-6","chapter":"17 Linear regression","heading":"17.1 Objectives","text":"QuestionsWhen use linear regression?interpret results?ObjectivesBe able perform linear regression RUse ANOVA check slope regression differs zeroUnderstand underlying assumptions linear regression analysisUse diagnostic plots check assumptions","code":""},{"path":"linear-regression.html","id":"purpose-and-aim-5","chapter":"17 Linear regression","heading":"17.2 Purpose and aim","text":"Regression analysis tests association two variables, also allows one investigate quantitatively nature relationship present, thus determine whether one variable may used predict values another.\nSimple linear regression essentially models dependence scalar dependent variable (y) independent (explanatory) variable (x) according relationship:\\[\\begin{equation*} \ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\\(\\beta_0\\) value intercept \\(\\beta_1\\) slope fitted line. aim simple linear regression analysis assess whether coefficient slope, \\(\\beta_1\\), actually different zero. different zero can say \\(x\\) significant effect \\(y\\) (since changing \\(x\\) leads predicted change \\(y\\)), whereas isn‚Äôt significantly different zero, say isn‚Äôt sufficient evidence relationship. course, order assess whether slope significantly different zero first need calculate values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"section-commands-9","chapter":"17 Linear regression","heading":"17.3 Section commands","text":"new commands used section.","code":""},{"path":"linear-regression.html","id":"data-and-hypotheses-8","chapter":"17 Linear regression","heading":"17.4 Data and hypotheses","text":"perform simple linear regression analysis two variables Murder Assault USArrests dataset. wish determine whether Assault variable significant predictor Murder variable. means need find coefficients \\(\\beta_0\\) \\(\\beta_1\\) best fit following macabre equation:\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 Assault\n\\end{equation*}\\]testing following null alternative hypotheses:\\(H_0\\): Assault significant predictor Murder, \\(\\beta_1 = 0\\)\\(H_1\\): Assault significant predictor Murder, \\(\\beta_1 \\neq 0\\)","code":""},{"path":"linear-regression.html","id":"summarise-and-visualise-9","chapter":"17 Linear regression","heading":"17.5 Summarise and visualise","text":"can visualise data :appears relatively strong positive relationship two variables whilst reasonable scatter points around trend line, probably expect significant result case.","code":"\nplot(Murder ~ Assault, data = USArrests)"},{"path":"linear-regression.html","id":"assumptions-15","chapter":"17 Linear regression","heading":"17.6 Assumptions","text":"order linear regression analysis valid 4 key assumptions need met:data must linear (entirely possible calculate straight line data straight - doesn‚Äôt mean though!)residuals must normally distributedThe residuals must correlated fitted valuesThe fit depend overly much single point (point high leverage).Whether assumptions met can easily checked visually producing four key diagnostic plots.First need define linear model:first argument lm formula saying Murder depends Assaults. seen , syntax generally dependent variable ~ independent variable.second argument specifies dataset useNext, can create diagnostic plots model:top left graph plots residuals fitted values. data best explained straight line uniform distribution points horizontal grey dotted line (sufficient points red line, moving average, top grey dotted line). plot pretty good.top right graph shows Q-Q plot allows visual inspection normality. residuals normally distributed, points lie diagonal dotted line. isn‚Äôt bad slight snaking towards upper end Georgia appears outlier .bottom left scale-location graph allows us investigate whether correlation residuals fitted values whether variance residuals changes significantly. , red line horizontal. correlation change variance red line horizontal. plot fine.last graph shows Cook‚Äôs distance tests one point unnecessarily large effect fit. important aspect see points lie beyond red dashed contour line top right corner plot. , point undue influence. plot good.Formally, concern looking diagnostic plots, linear regression valid. However, disappointingly, people ever check whether linear regression assumptions met quoting results.Let‚Äôs change leading example!","code":"\nlm_1 <- lm(Murder ~ Assault, data = USArrests)\n# create a 2 x 2 output window\npar(mfrow = c(2,2))\n\n# and create the diagnostic plots for our model\nplot(lm_1)"},{"path":"linear-regression.html","id":"implement-test-9","chapter":"17 Linear regression","heading":"17.7 Implement test","text":"already defined linear model, can closer look :function lm returns linear model (lm) object essentially list containing everything necessary understand analyse linear model. However, just type model name () just prints screen actual coefficients model .e.¬†intercept slope line.found line best fit given :\\[\\begin{equation*}\nMurder = 0.63 + 0.042 Assault\n\\end{equation*}\\]Assess whether slope significantly different zero:, use anova() command assess significance. shouldn‚Äôt surprising stage introductory lectures made sense. mathematical perspective, one-way ANOVA simple linear regression exactly makes sense use command analyse R.","code":"\n# show the linear model\nlm_1## \n## Call:\n## lm(formula = Murder ~ Assault, data = USArrests)\n## \n## Coefficients:\n## (Intercept)      Assault  \n##     0.63168      0.04191\nanova(lm_1)"},{"path":"linear-regression.html","id":"interpret-output-and-report-results-8","chapter":"17 Linear regression","heading":"17.8 Interpret output and report results","text":"exactly format table saw one-way ANOVA:1st line just tells ANOVA testThe 2nd line tells response variable (case Murder)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 1 48 (‚Äôll need reporting)\nF value column contains F statistic, 86.454 (‚Äôll need reporting).\np-value 2.596e-12 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) column used calculate F statistic don‚Äôt need know .\nDf column contains degrees freedom values row, 1 48 (‚Äôll need reporting)F value column contains F statistic, 86.454 (‚Äôll need reporting).p-value 2.596e-12 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) column used calculate F statistic don‚Äôt need know ., p-value ‚Äôre interested shows us probability getting data null hypothesis actually true slope line actually zero.\nSince p-value excruciatingly tiny can reject null hypothesis state :simple linear regression showed assault rate US states significant predictor number murders (F = 86.45, df = 1,48, p = 2.59x10-12).Plotting regression lineIt can helpful plot regression line original data see far data predicted linear values. can :first command creates scatter plot dataThe second command uses results linear model fitting (object lm_1) add line best fit plot (colour red).","code":"## Analysis of Variance Table\n## \n## Response: Murder\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Assault    1 597.70  597.70  86.454 2.596e-12 ***\n## Residuals 48 331.85    6.91                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nplot(Murder ~ Assault, data = USArrests)\n\n# add the regression line\nabline(lm_1, col = \"red\")"},{"path":"linear-regression.html","id":"exercise-3","chapter":"17 Linear regression","heading":"17.9 Exercise","text":"Exercise 17.1  Linear regressionCalculate two simple linear regressions using data/raw/CS3-statedata.csv dataset, first variable LifeExp variable Murder variable HSGrad Frost.following cases:Find value slope intercept coefficients regressionsDetermine slope significantly different zero (.e.¬†relationship two variables)Produce scatter plot data line best fit superimposed top.Produce diagnostic plots discuss (virtual) neighbour carried simple linear regression caseMurder Life ExpectancyLet‚Äôs see Murder variable can used predict Life.Exp variable. Let‚Äôs plot first ., ‚Äôve fit linear model (second line) time plotting raw data (first line) just can add line best fit (third line). visualise reasons:check data aren‚Äôt obviously wrong. sensible values life expectancy (nothing massively large small), plausible values murder rates (‚Äôm au fait US murder rates 1976 small positive numbers seem plausible).check see expect statistical analysis. appear reasonable downward trend data. surprised didn‚Äôt get significant result given amount data spread data lineWe check assumptions (roughly though ‚Äôll properly minute). Nothing immediately gives cause concern; data appear linear, spread data around line appears homogeneous symmetrical. outliers either.Now, let‚Äôs check assumptions diagnostic plots.residuals vs fitted plot appears symmetric enough (similar distribution points horizontal grey dotted line) happy linearity. Similarly red line scale-location plot looks horizontal enough happy homogeneity variance. aren‚Äôt influential points residuals vs leverage. plot give bit concern Normal Q-Q graph. see clear evidence snaking, although degree snaking isn‚Äôt actually bad. just means can pretty certain distribution residuals isn‚Äôt normal, also isn‚Äôt non-normal. situation? Well, three possible options:Appeal Central Limit Theorem. states large enough sample size don‚Äôt worry whether distribution residuals normally distributed. Large enough bit moving target honest depends non-normal underlying data . data little bit non-normal can get away using smaller sample data massively skewed (example). exact science, anything 30 data points considered lot mild moderate non-normality (case). data skewed looking data points (50-100). , example can legitimately just carry analysis without worrying.Try transforming data. try applying mathematical functions response variable (LifeExp) hope repeating analysis transformed variable make things better. honest might work won‚Äôt know try. Dealing transformed variables legitimate approach can make interpreting model bit challenging. particular example none traditional transformations (log, square-root, reciprocal) anything fix slight lack normality (can take word try ; plot(lm(log(LifeExp ~ Murder, data = USAstate))) example.Go permutation methods / bootstrapping. approach definitely work. don‚Äôt time explain (‚Äôs subject entire practical). approach also requires us reasonably large sample size work well assume distribution sample good approximation distribution entire dataset.case, large enough sample size deviation normality isn‚Äôt bad, can just crack standard analysis., let‚Äôs actually analysis:find Murder rate statistically significant predictor life expectancy US states. Woohoo!High School Graduation Frosty DaysNow let‚Äôs investigate relationship proportion High School Graduates state (HSGrad) mean number days freezing (Frost) within state., look data.doesn‚Äôt appear ridiculous errors data; High School graduation proportions 0-100% range mean number sub-zero days state 0 365, numbers plausible.Whilst trend upwards, wouldn‚Äôt surprise came back significant, ‚Äôm bit concerned ‚Ä¶assumptions. ‚Äôm mainly concerned data aren‚Äôt linear. appears noticeable pattern data sort minimum around 50-60 Frost days. means ‚Äôs hard assess assumptions.Let‚Äôs check properlyNow, let‚Äôs check assumptions diagnostic plots.can see suspected backed residual vs fitted graph. data aren‚Äôt linear appears sort odd -pattern . Given lack linearity just isn‚Äôt worth worrying plots model misspecified: straight line just doesn‚Äôt represent data .Just reference, practice looking diagnostic plots, ignore lack linearity can say thatNormality pretty good Normal Q-Q plotHomogeneity variance isn‚Äôt good appears noticeable drop variance go left right (consideration Scale-Location plot)don‚Äôt appear influential points (looking residuals vs leverage graph)However, none relevant particular case since data aren‚Äôt linear straight line wrong model fit.situation?Well actually, bit tricky aren‚Äôt easy fixes . two broad solutions dealing misspecified model.common solution need predictor variables model. ‚Äôre trying explain/predict high school graduation using number frost days. Obviously many things affect proportion high school graduates just cold State (weird potential predictor think ) need statistical approach allows us look multiple predictor variables. ‚Äôll cover approach next two sessions.potential solution say high school graduation can fact predicted number frost days relationship isn‚Äôt linear. need specify relationship (curve basically) try fit data new, non-linear, curve. process called, unsurprisingly, non-linear regression don‚Äôt cover course. process best used already strong theoretical reason non-linear relationship two variables (sigmoidal dose-response curves pharmacology exponential relationships cell growth). case don‚Äôt preconceived notions wouldn‚Äôt really appropriate case.Neither solutions can tackled knowledge far course can definitely say based upon data set, isn‚Äôt linear relationship (significant otherwise) frosty days high school graduation rates.","code":"\n# plot the data\nplot(LifeExp ~ Murder, data = USAstate)\n\n# create a linear model\nlm1 <- lm(LifeExp ~ Murder, data = USAstate)\n\n# and add a regression line\nabline(lm1, col = \"red\")\npar(mfrow = c(2,2))\nplot(lm1)\nanova(lm1)## Analysis of Variance Table\n## \n## Response: LifeExp\n##           Df Sum Sq Mean Sq F value   Pr(>F)    \n## Murder     1 53.838  53.838  74.989 2.26e-11 ***\n## Residuals 48 34.461   0.718                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nplot(HSGrad ~ Frost, data = USAstate)\n\n# create a linear model\nlm2<-lm(HSGrad ~ Frost, data=USAstate)\n\n# and add a regression line\nabline(lm2, col = \"red\")\npar(mfrow = c(2,2))\nplot(lm2)"},{"path":"linear-regression.html","id":"key-points-5","chapter":"17 Linear regression","heading":"17.10 Key points","text":"Linear regression tests linear relationship exists two variablesIf , can use one variable predict anotherA linear model intercept slope test slope differs zeroWe create linear models R lm() function use anova() assess slope coefficientWe can use linear regression four assumptions met:\ndata linear\nResiduals normally distributed\nResiduals correlated fitted values\nsingle point large influence linear model\ndata linearResiduals normally distributedResiduals correlated fitted valuesNo single point large influence linear modelWe use plot(model_name) get four diagnostic plots R, help evaluate assumptions","code":""}]
