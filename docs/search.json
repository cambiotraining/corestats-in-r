[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform core data analysis techniques appropriately confidently using R.6 lecture-practicals6 lecture-practicalsOngoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented arbitrary dataset e.g.Know data analysis techniques availableKnow ones allowableBe able carry understand results","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Simple hypothesis testingCategorical predictor variablesContinuous predictorsTwo predictor variablesMultiple predictor variablesPower analysis","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data/raw.","code":""},{},{"path":"cs1-intro.html","id":"cs1-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"cs1-intro.html","id":"objectives","chapter":"2 Introduction","heading":"2.1 Objectives","text":"Aim: carry basic one two sample statistical tests.end section practical participants able achieve following listed tests:Understand purpose test isPerform test RInterpret test outputUnderstand assumptions/conditions test appropriateCheck assumptionsThe tests covered practical :One-sample tests\nOne sample t-test\nOne-sample Wilcoxon signed-rank test\nOne sample t-testOne-sample Wilcoxon signed-rank testTwo-sample tests\nStudent’s t-test\nMann-Whitney U test\nPaired two-sample t-test\nWilcoxon signed-rank test\nStudent’s t-testMann-Whitney U testPaired two-sample t-testWilcoxon signed-rank test","code":""},{"path":"cs1-intro.html","id":"background","chapter":"2 Introduction","heading":"2.2 Background","text":"practical focus underlying mathematical theory tests although demonstrators happy answer questions.\ntest section explaining purpose, section explaining perform test R, section explaining results output screen, section covering assumptions required perform test.","code":""},{},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction","heading":"3 Introduction","text":"practical document divided various sections. section explanatory text help understand going ’re trying achieve.\nmay list commands relevant section displayed boxes like :Conditional operatorsTo set filtering conditions, use following relational operators:> greater >= greater equal < less <= less equal == equal != different %% contained combine conditions, use following logical operators:& | ","code":""},{"path":"introduction.html","id":"cs1-datasets","chapter":"3 Introduction","heading":"3.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"cs1-one-sample-tests.html","id":"cs1-one-sample-tests","chapter":"4 One-sample tests","heading":"4 One-sample tests","text":"","code":""},{"path":"cs1-one-sample-tests.html","id":"objectives-1","chapter":"4 One-sample tests","heading":"4.1 Objectives","text":"QuestionsWhen perform one-sample test?one-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis single sample continuous dataBe able summarise visualise data RUnderstand assess underlying assumptions testsPerform one-sample t-test Wilcoxon signed-rank test RKnow test appropriate whenBe able interpret report results","code":""},{"path":"cs1-one-sample-tests.html","id":"purpose-and-aim","chapter":"4 One-sample tests","heading":"4.2 Purpose and aim","text":"tests used single sample continuous data. used find sample came parent distribution given mean (median). essentially boils finding sample mean (median) “close enough” hypothesised parent population mean (median).\n, figure , use tests see probability sample ten points comes distribution plotted .e. population mean 20 mm.","code":""},{"path":"cs1-one-sample-tests.html","id":"choosing-a-test","chapter":"4 One-sample tests","heading":"4.3 Choosing a test","text":"two tests going look situation; one-sample t-test, one-sample Wilcoxon signed rank test. tests work sort data ’re considering , different assumptions.data normally distributed, one-sample t-test appropriate. data aren’t normally distributed, distribution symmetric, sample size small one-sample Wilcoxon signed rank test appropriate.statistical test consider five tasks. come back , pay extra close attention.Setting hypothesisSummarise visualisation dataAssessment assumptionsImplementation statistical testInterpreting output presentation resultsWe won’t always carry exactly order, always consider five tasks every test.","code":""},{},{"path":"cs1-one-sample-t-test.html","id":"cs1-one-sample-t-test","chapter":"5 One-sample t-test","heading":"5 One-sample t-test","text":"","code":""},{"path":"cs1-one-sample-t-test.html","id":"section-commands","chapter":"5 One-sample t-test","heading":"5.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-one-sample-t-test.html","id":"data-and-hypotheses","chapter":"5 One-sample t-test","heading":"5.2 Data and hypotheses","text":"example, suppose measure body lengths male guppies (mm) collected Guanapo River Trinidad. want test whether data support hypothesis mean body actually 20 mm. form following null alternative hypotheses:\\(H_0\\): mean body length equal 20mm (\\(\\mu =\\) 20).\\(H_1\\): mean body length equal 20mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed t-test see reject null hypothesis .use one-sample test one sample.use two-tailed t-test want know data suggest true (population) mean different 20 mm either direction rather just see greater less 20 mm (case use one-tailed test).’re using t-test don’t know better yet ’m telling . ’ll look precise assumptions/requirements need moment.Make sure downloaded data (see: Datasets) placed data/raw folder within working directory.read data create vector containing data.first line reads data R creates object called data frame. data frame contains single column numbers called “Guanapo” (name river). situations, statistical analyses, data stored data frame exactly ’d want. However, one sample tests actually need data stored vector. , second line extracts values Guanapo column fishlengthDF data frame creates simple vector numbers called fishlength. step necessary one-sample tests look complex datasets, won’t need second step .","code":"\n# import the data\nfishlengthDF <- read.csv(\"data/raw/CS1-onesample.csv\")\n\n# create a vector containing the data\nfishlength <- fishlengthDF$Guanapo"},{"path":"cs1-one-sample-t-test.html","id":"summarise-and-visualise","chapter":"5 One-sample t-test","heading":"5.3 Summarise and visualise","text":"Summarise data visualise :data appear contain obvious errors, whilst mean median less 20 (18.3 18.8 respectively) absolutely certain sample mean sufficiently different value “statistically significant,” although may anticipate result.","code":"\nsummary(fishlength)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    11.2    17.5    18.8    18.3    19.7    23.3\nboxplot(fishlength, main = \"Male guppies\", ylab = \"Length (mm)\")"},{"path":"cs1-one-sample-t-test.html","id":"implement-the-test","chapter":"5 One-sample t-test","heading":"5.4 Implement the test","text":"Perform one-sample, two-tailed t-test:first argument must numerical vector data values.second argument must number mean tested null hypothesis.third argument gives type alternative hypothesis must one two.sided, greater less.","code":"\nt.test(fishlength, mu = 20, alternative = \"two.sided\")## \n##  One Sample t-test\n## \n## data:  fishlength\n## t = -3.5492, df = 28, p-value = 0.001387\n## alternative hypothesis: true mean is not equal to 20\n## 95 percent confidence interval:\n##  17.31341 19.27969\n## sample estimates:\n## mean of x \n##  18.29655"},{"path":"cs1-one-sample-t-test.html","id":"interpreting-the-output-and-report-results","chapter":"5 One-sample t-test","heading":"5.5 Interpreting the output and report results","text":"output now see console window:1st line gives name test 2nd line reminds dataset calledThe 3rd line contains three key outputs test:\ncalculated t-value -3.5492 (’ll need reporting)\n28 degrees freedom (’ll need reporting)\np-value 0.001387.\ncalculated t-value -3.5492 (’ll need reporting)28 degrees freedom (’ll need reporting)p-value 0.001387.4th line simply states alternative hypothesisThe 5th 6th lines give 95th confidence interval (don’t need know )7th, 8th 9th lines give sample mean (18.29655).p-value 3rd line ’re interested . gives probability us getting sample null hypothesis actually true.:high p-value means high probability observing sample null hypothesis probably true whereasa low p-value means low probability observing sample null hypothesis probably true.important realise p-value just indication absolute certainty interpretation.People, however like definite answers pick artificial probability threshold (called significance level) order able say something decisive. standard significance level 0.05 since p-value smaller choose say “unlikely particular sample null hypothesis true.” Consequently, can reject null hypothesis state :one-sample t-test indicated mean body length male guppies (\\(\\mu\\) = 18.29mm) differs significantly 20 mm (t = -3.55, df = 28, p = 0.0014).sentence adequate concluding statement test write paper report. Note included (brackets) information actual mean value group(\\(\\mu\\) = 18.29mm), test statistic (t = -3.55), degrees freedom (df = 28), p-value (p = 0.0014). journals required report whether p-value less critical value (e.g. p < 0.05) always recommend reporting actual p-value obtained.Please feel free ask demonstrator aspect section unclear form core classical hypothesis testing logic applies rest tests.","code":"## \n##  One Sample t-test\n## \n## data:  fishlength\n## t = -3.5492, df = 28, p-value = 0.001387\n## alternative hypothesis: true mean is not equal to 20\n## 95 percent confidence interval:\n##  17.31341 19.27969\n## sample estimates:\n## mean of x \n##  18.29655"},{"path":"cs1-one-sample-t-test.html","id":"assumptions","chapter":"5 One-sample t-test","heading":"5.6 Assumptions","text":"order use t-test analysis (results strictly valid) make two assumptions:parent distribution sample taken normally distributed (sample data normally distributed ).worth noting though t-test actually pretty robust situations sample data normal. sufficiently large sample sizes (guess good mine, conventionally means 30 data points), can use t-test without worrying whether underlying population normally distributed .data point sample independent others. general something can tested instead considered sampling procedure. example, taking repeated measurements individual generate data independent.second point know nothing ignore (issue needs considered experimental design), whereas first assumption can checked.\nthree ways checking normality:increasing order rigour, haveHistogramQuantile-quantile plotShapiro-Wilk test","code":""},{"path":"cs1-one-sample-t-test.html","id":"histogram-of-the-data","chapter":"5 One-sample t-test","heading":"5.6.1 Histogram of the data","text":"Plot histogram data, gives:distribution appears uni-modal symmetric, isn’t obviously non-normal. However, lot distributions simple properties aren’t normal, isn’t exactly rigorous. Thankfully , rigorous tests.NB. even looking distribution assess assumption normality already going far beyond anyone else ever . Nevertheless, continue.","code":"\nhist(fishlength, breaks = 15)"},{"path":"cs1-one-sample-t-test.html","id":"q-q-plot-of-the-data","chapter":"5 One-sample t-test","heading":"5.6.2 Q-Q plot of the data","text":"Q-Q plot short quantile-quantile plot. diagnostic plot (sometimes called) way comparing two distributions. Q-Q plots work won’t explained ask demonstrator really want know going .Construct Q-Q Plot quantiles data quantiles normal distribution:important know data normally distributed points lie (close ) diagonal line graph.case, points lie quite close line part sample quantiles (points) either end sample distribution either smaller (line left) larger (line right) expected supposed normally distributed. suggests sample distribution bit spread expected came normal distribution.important recognise isn’t simple unambiguous answer interpreting types graph, terms whether assumption normality well met instead often boils matter experience.rare situation indeed assumptions necessary test met unequivocally certain degree personal interpretation always needed. ask whether data normal “enough” confident validity test.four examples QQ plots different types distributions:two graphs relate 200 data points drawn normal distribution. Even can see points lie perfectly diagonal line QQ plot, certain amount deviation top bottom graph can happen just chance (draw different set point graph look slightly different).two graphs relate 200 data points drawn uniform distribution. Uniform distributions condensed normal distributions, reflected QQ plot pronounced S-shaped pattern (colloquially known snaking).two graphs relate 200 data points drawn t distribution. t distributions spread normal distributions, reflected QQ plot pronounced S-shaped pattern , time snaking reflection observed uniform distribution.two graphs relate 200 data points drawn exponential distribution. Exponential distributions symmetric skewed compared normal distributions. significant right-skew distribution reflected QQ plot points curve away diagonal line ends (left-skew points line ends).four cases worth noting deviations ends plot.","code":"\n# plot the Q-Q plot\nqqnorm(fishlength)\n\n# and add a comparison line\nqqline(fishlength)"},{"path":"cs1-one-sample-t-test.html","id":"shapiro-wilk-test","chapter":"5 One-sample t-test","heading":"5.6.3 Shapiro-Wilk test","text":"one number formal statistical test assess whether given sample numbers come normal distribution. calculates probability getting sample data underlying distribution fact normal. easy carry R.Perform Shapiro-Wilk test data:1st line gives name test 2nd line reminds dataset calledThe 3rd line contains two key outputs test:\ncalculated w-value 0.9494 (don’t need know )\np-value 0.1764\ncalculated w-value 0.9494 (don’t need know )p-value 0.1764As p-value bigger 0.05 (say) can say insufficient evidence reject null hypothesis sample came normal distribution.important recognise Shapiro-Wilk test without limitations. rather sensitive sample size considered. general, small sample sizes, test relaxed normality (nearly datasets considered normal), whereas large sample sizes test can overly strict, can fail recognise datasets nearly normal indeed.","code":"\nshapiro.test(fishlength)## \n##  Shapiro-Wilk normality test\n## \n## data:  fishlength\n## W = 0.94938, p-value = 0.1764"},{"path":"cs1-one-sample-t-test.html","id":"assumptions-overview","chapter":"5 One-sample t-test","heading":"5.6.4 Assumptions overview","text":"terms assessing assumptions test always worth considering several methods, graphical analytic, just relying single method.fishlength example, graphical Q-Q plot analysis especially conclusive suggestion snaking plots, Shapiro-Wilk test gave non-significant p-value (0.1764). Putting two together, along original histogram recognition 30 data points dataset personally happy assumptions t-test met well enough trust result t-test, may …case consider alternative test less stringent assumptions (less powerful): one-sample Wilcoxon signed rank test.","code":""},{"path":"cs1-one-sample-t-test.html","id":"exercise","chapter":"5 One-sample t-test","heading":"5.7 Exercise","text":"Exercise 5.1  following data dissolving times (seconds) drug agitated gastric juice:42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6Do results provide evidence suggest dissolving time drug different 45 seconds?Write null alternative hypotheses.Summarise visualise data perform appropriate one-sample t-test.\ncan say dissolving time? (sentence use report )\ncan say dissolving time? (sentence use report )Check assumptions test.\ntest valid?\ntest valid?Hypotheses\n\\(H_0\\) : mean = 45s\\(H_1\\) : mean \\(\\neq\\) 45sCreate data, summarise visualiseThere 8 data points, default histogram looks bit rubbish / uninformative. Thankfully box-plot bit useful . can see:don’t appear major errors data entry aren’t huge outliersThe median value box-plot (thick black line) pretty close 45 wouldn’t surprised mean data isn’t significantly different 45. can confirm looking mean median values calculated using summary command earlier.data appear symmetric, whilst can’t tell ’re normal ’re least massively skewed.Carry t-testA one-sample t-test indicated mean dissolving time drug significantly different 45s (t=0.366 , df=7 , p=0.725),Explore Assumptions\nNormality:Shapiro test p-value 0.964 (given bigger 0.05) suggests data normal enough.qq-plot isn’t perfect, deviation points away line since points aren’t accelerating away line , since 8 points, can claim, slight reservations, assumption normality appears adequately well met.Overall, somewhat confident assumption normality well-enough met t-test appropriate method analysing data. Note ridiculous number caveats slightly political/slippery language ’m using. intentional reflects ambiguous nature assumption checking. important approach statistics need embrace.reality, found situation also try non-parametric test data (Wilcoxon-signed Rank test) see whether get conclusion whether median dissolving time differs 45s. Technically, don’t know Wilcoxon test yet haven’t done section handout. Anyway, get conclusion confidence result test goes considerably; doesn’t matter well assumption met , get result. hand get completely different conclusion carrying non-parametric test bets ; now little confidence test result don’t know one believe (case assumptions test bit unclear). example Wilcoxon test also gives us non-significant result good.","code":"\ndissolving<-c(42.7 , 43.4 , 44.6 , 45.1 , 45.6 , 45.9 , 46.8 , 47.6)\nsummary(dissolving)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   42.70   44.30   45.35   45.21   46.12   47.60\nhist(dissolving)\nboxplot(dissolving)\nt.test(dissolving , mu=45 , alternative = \"two.sided\")## \n##  One Sample t-test\n## \n## data:  dissolving\n## t = 0.36647, df = 7, p-value = 0.7248\n## alternative hypothesis: true mean is not equal to 45\n## 95 percent confidence interval:\n##  43.84137 46.58363\n## sample estimates:\n## mean of x \n##   45.2125\nshapiro.test(dissolving)## \n##  Shapiro-Wilk normality test\n## \n## data:  dissolving\n## W = 0.98023, p-value = 0.9641\nqqnorm(dissolving)\nqqline(dissolving)"},{},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"cs1-onesample-wilcoxon-signed-rank","chapter":"6 Wilcoxon signed-rank test","heading":"6 Wilcoxon signed-rank test","text":"test also considers single sample, however test (contrast one sample t-test) don’t assume parent distribution normally distributed. still need parent distribution (consequently sample) symmetric though. test look see median parent distributions differs significantly given hypothesised value (contrast t-test looks mean).","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"section-commands-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.1 Section commands","text":"New commands used section:, use fishlength dataset. one-sample Wilcoxon signed-rank test allows see median body length different specified value. want test whether data support hypothesis median body actually 20 mm. following null alternative hypotheses similar used one sample t-test:\\(H_0\\): median body length equal 20 mm (\\(\\mu =\\) 20).\\(H_1\\): median body length equal 20 mm (\\(\\mu \\neq\\) 20).use one-sample, two-tailed Wilcoxon signed-rank test see reject null hypothesis .","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.2 Summarise and visualise","text":"previous section, nothing really changed now (’re good start practical!)","code":""},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"implement-the-test-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.3 Implement the test","text":"Perform one-sample, two-tailed Wilcoxon signed-rank test:syntax identical one-sample t-test carried earlier.first argument must numerical vector data values.second argument must number median tested null hypothesis.third argument gives type alternative hypothesis must one two.sided, greater less.","code":"\nwilcox.test(fishlength, mu = 20, alternative = \"two.sided\")"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"interpreting-the-output-and-report-results-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.4 Interpreting the output and report results","text":"output now see console windowThe first two lines give warning (error) message regarding implementation test. can safely ignored case p-value small, essentially, ’s letting know data values identical . supposed happen dealing continuous data test, practice ’s something need worry .3rd line gives name test 4th line reminds dataset calledThe 5th line contains two key outputs test:\ncalculated statistic 67.5 (’ll need reporting)\np-value 0.001222.\ncalculated statistic 67.5 (’ll need reporting)p-value 0.001222.6th line simply states alternative hypothesisAgain, p-value ’re interested . gives probability us getting sample null hypothesis actually true.\n, case since p-value less 0.05 can reject null hypothesis state :one-sample Wilcoxon signed-rank test indicated median body length male guppies (\\(\\mu\\) = 18.8 mm) differs significantly 20 mm (V = 67.5, n = 29, p = 0.0012).sentence adequate concluding statement test write paper report. Note included (brackets) information median value group (\\(\\mu\\) = 18.8 mm), test statistic (V = 67.5), number observations (n = 29), p-value (p = 0.0012).","code":"## Warning in wilcox.test.default(fishlength, mu = 20, alternative = \"two.sided\"):\n## cannot compute exact p-value with ties## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  fishlength\n## V = 67.5, p-value = 0.001222\n## alternative hypothesis: true location is not equal to 20"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"assumptions-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.5 Assumptions","text":"order use one-sample Wilcoxon rank-sum test analysis (results strictly valid) make two assumptions:parent distribution sample symmetricEach data point sample independent others. t-test common feature nearly statistical tests. Lack independence data really tough deal (impossible) large part proper experimental design ensuring .Whilst formal statistical tests symmetry opt simple visual inspection using boxplot histogram.Plot histogram boxplot data:get following plots:can see whilst distribution isn’t perfectly symmetric, neither heavily skewed left right can make call distribution symmetric enough us happy results test.","code":"\nhist(fishlength, breaks = 10)\n\nboxplot(fishlength)"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"exercise-1","chapter":"6 Wilcoxon signed-rank test","heading":"6.6 Exercise","text":"Exercise 6.1  Performing Wilcoxon signed-rank test:Analyse drug dataset using one-sample Wilcoxon signed-rank testDiscuss (virtual) neighbour two tests feel best suited data.matter case?Hypotheses\n\\(H_0\\) : median = 45s\\(H_1\\) : median \\(\\neq\\) 45sWilcoxon signed-rank testA one-sample Wilcoxon-signed rank test indicated median dissolving time drug significantly different 45 s (V=22, n=8 , p=0.64)Assumptions\nbox-plot previous exercise already know data symmetric enough test valid.Discussion\nterms choosing two test can see meet respective assumptions tests valid. case tests also agree terms conclusions .e. average dissolving time (either mean median) doesn’t differ significantly proposed value 45 s.one answer doesn’t matter test use.Another answer pick test measures quantity ’re interested .e. care medians use Wilcoxon test, whereas care means use t-test.final answer , since test valid prefer use test greater power. t-tests always power Wilcoxon tests (long ’re valid) report one. (’ll talk last session power effectively capacity test detect significant difference - power better).","code":"\nwilcox.test(dissolving , mu=45 , alternative = \"two.sided\")## \n##  Wilcoxon signed rank exact test\n## \n## data:  dissolving\n## V = 22, p-value = 0.6406\n## alternative hypothesis: true location is not equal to 45"},{"path":"cs1-onesample-wilcoxon-signed-rank.html","id":"key-points","chapter":"6 Wilcoxon signed-rank test","heading":"6.7 Key points","text":"One-sample tests used single sample continuous dataWe can summarise data using summary() function visualise boxplot()t-test assumes data normally distributed independent otherThe Wilcoxon signed-rank test assume normal distribution, require independent samplesThe t.test() compares mean parent distribution differs hypothesised value, whereas wilcox.test() compares median.good way assessing assumptions visually check looking distribution hist() quantile-quantile plots qqnorm() qqline()","code":""},{},{"path":"cs1-two-sample.html","id":"cs1-two-sample","chapter":"7 Two-sample tests","heading":"7 Two-sample tests","text":"","code":""},{"path":"cs1-two-sample.html","id":"objectives-2","chapter":"7 Two-sample tests","heading":"7.1 Objectives","text":"QuestionsWhen perform two-sample test?two-sample tests assumptions?interpret present results tests?ObjectivesSet hypothesis two-sample continuous dataDetermine correct data format perform two-sample test RSummarise visualise dataCheck underlying assumptions (normality, homogeneity variance)able choose appropriate two-sample test run RBe able interpret report results","code":""},{"path":"cs1-two-sample.html","id":"purpose-and-aim-1","chapter":"7 Two-sample tests","heading":"7.2 Purpose and aim","text":"tests used two samples continuous data trying find samples came parent distribution . essentially boils finding difference means two samples.","code":""},{"path":"cs1-two-sample.html","id":"two-sample-choosing-a-test","chapter":"7 Two-sample tests","heading":"7.3 Choosing a test","text":"five key tests can used deal two samples. Choosing test use depends upon key assumptions satisfied sample data effectively boils answering four questions samples:samples normally distributed? (Yes/)big samples? (<30 data points >30 data points)samples paired? (Yes/)samples variance? (Yes/)two sets tests consider depending answers questions 1 2. data normally distributed big samples need look parametric tests. data normally distributed sample size small, need look non-parametric tests (see Figure 7.1. Questions 3 4 help pick specific test use, summarised Figure 7.2.\nFigure 7.1: Category test\n\nFigure 7.2: test use\nTesting whether sample comes normal distribution covered One-sample tests. need visualise data /use Shapiro-Wilk test.size sample makes things easier. maths (specifically due something called central limit theorem even going attempt touch upon ) large samples can use tests assume normality parent population (Student’s t-test, Welch’s t-test paired t-test) even parent populations certainly normal. really want understand exactly works, rigorous mathematics. , moment ’m going say ’s OK take facts faith just trust .Paired samples mean every data point one sample matching data point sample linked inextricable way. typical example involve group 20 test subjects measured experiment. Providing experiment didn’t anything fatal test subjects data consist two samples; 20 pre-experiment measurements 20 post-experiment measurements. However, test subjects used pre-experiment data point can matched exactly one post-experiment data points. sense two samples said “paired.”couple tests (Bartlett’s test Levene’s test) can used see two samples come distributions variance. covered later section.Resampling techniques aren’t covered course require mixture statistical understanding programming skill. Ask demonstrator (Google 😉) want know .","code":""},{"path":"cs1-two-sample.html","id":"tidy-data","chapter":"7 Two-sample tests","heading":"7.4 Tidy data","text":"two samples data can stored one three formats R:two separate vectors,stacked data frame,unstacked data frame/list.Two separate vectors case (hopefully) obvious.using data frame different options organise data. best way formatting data R using tidy data format.Tidy data following properties:variable columnEach observation rowEach value cellStacked form (long format data) data arranged way variable (thing measured) column. consider dataset containing meerkat weights (g) two different countries stacked format data look like:unstacked (wide format) form variable (measured thing) present one column. example, let’s say measured meerkat weight two countries period years. organise data way year measured values split country:tidy data easiest way analyses R strongly encourage start adopting format standard data collection processing.","code":"## # A tibble: 6 × 2\n##   country  weight\n##   <chr>     <dbl>\n## 1 Botswana    514\n## 2 Botswana    568\n## 3 Botswana    519\n## 4 Uganda      624\n## 5 Uganda      662\n## 6 Uganda      633## # A tibble: 3 × 3\n##    year Botswana Uganda\n##   <dbl>    <dbl>  <dbl>\n## 1  1990      514    624\n## 2  1992      568    662\n## 3  1995      519    633"},{},{"path":"cs1-students-t-test.html","id":"cs1-students-t-test","chapter":"8 Student’s t-test","heading":"8 Student’s t-test","text":"test assume sample data sets normally distributed equal variance. test see means two samples differ significantly .language used section slightly different used section 4. Although language used section 4 technically correct, sentences somewhat onerous read. ’ve opted easier reading style expense technical accuracy. Please feel free re-write section (leisure).","code":""},{"path":"cs1-students-t-test.html","id":"section-commands-2","chapter":"8 Student’s t-test","heading":"8.1 Section commands","text":"New commands used section:","code":""},{"path":"cs1-students-t-test.html","id":"data-and-hypotheses-1","chapter":"8 Student’s t-test","heading":"8.2 Data and hypotheses","text":"example, suppose now measure body lengths male guppies (mm) collected two rivers Trinidad; Aripo Guanapo. want test whether mean body length differs samples. form following null alternative hypotheses:\\(H_0\\): mean body length differ two groups \\((\\mu = \\mu G)\\)\\(H_1\\): mean body length differ two groups \\((\\mu \\neq \\mu G)\\)use two-sample, two-tailed t-test see can reject null hypothesis.use two-sample test now two samples.use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller .’re using Student’s t-test sample sizes big ’re assuming parent populations equal variance (can check later).data stored stacked format file data/raw/CS1-twosample.csv.Read R:","code":"\nrivers <- read.csv(\"data/raw/CS1-twosample.csv\")"},{"path":"cs1-students-t-test.html","id":"cs1-students-sumvisual","chapter":"8 Student’s t-test","heading":"8.3 Summarise and visualise","text":"Let’s summarise data…visualise :boxplot appear suggest two samples different means, moreover guppies Guanapo may smaller guppies Aripo. isn’t immediately obvious two populations don’t equal variances though, plough .","code":"\naggregate(length ~ river, data = rivers, summary)##     river length.Min. length.1st Qu. length.Median length.Mean length.3rd Qu.\n## 1   Aripo    17.50000       19.10000      20.10000    20.33077       21.30000\n## 2 Guanapo    11.20000       17.50000      18.80000    18.29655       19.70000\n##   length.Max.\n## 1    26.40000\n## 2    23.30000\nboxplot(length ~ river, data = rivers,\n        main = \"Male guppies\",\n        ylab = \"Length (mm)\")"},{"path":"cs1-students-t-test.html","id":"implement-test","chapter":"8 Student’s t-test","heading":"8.4 Implement test","text":"Perform two-sample, two-tailed, t-test:case, data stacked format:first argument must formula format: variables ~ categoryThe second argument must name data frameThe third argument gives type alternative hypothesis must one two.sided, greater lessThe fourth argument says whether variance two samples can assumed equal (Student’s t-test) unequal (Welch’s t-test)next section shows can perform exactly test data different (unstacked) format. potentially somewhat redundant (thorough depending point view). Probably best just anyway just case gain extra insight repetition (works rowers…)Convert data unstacked format repeat t test:","code":"\nt.test(length ~ river, data = rivers,\n       alternative = \"two.sided\",\n       var.equal = TRUE)\n# create a new object that contains the unstacked data\nuns_rivers <- unstack(rivers)\n\n# have a look at the data\nuns_rivers\n# perform the t-test\nt.test(uns_rivers$Guanapo, uns_rivers$Aripo,\n       alternative = \"two.sided\",\n       var.equal = TRUE)"},{"path":"cs1-students-t-test.html","id":"interpret-output-and-report-results","chapter":"8 Student’s t-test","heading":"8.5 Interpret output and report results","text":"Let’s look results t-test performed original (stacked) data frame:1st line gives name test 2nd line reminds dataset called, variables used.3rd line contains three key outputs test:\ncalculated t-value 3.8433 (need reporting)\n66 degrees freedom (need reporting)\np-value 0.0002754.\ncalculated t-value 3.8433 (need reporting)66 degrees freedom (need reporting)p-value 0.0002754.4th line simply states alternative hypothesis terms difference two sample means (testing two sample means different equivalent testing whether difference means equal zero).5th 6th lines give 95th confidence interval (don’t need know ).7th, 8th 9th lines give sample means group (20.33077 Aripo 18.29655 Guanapo) found earlier., p-value 3rd line ’re interested . Since p-value small (much smaller standard significance level) choose say “unlikely two samples came parent distribution can reject null hypothesis” state :Student’s t-test indicated mean body length male guppies Guanapo river (18.29 mm) differs significantly mean body length male guppies Aripo river (20.33 mm) (t = 3.8433, df = 66, p = 0.0003).Now ’s conversation starter.","code":"## \n##  Two Sample t-test\n## \n## data:  length by river\n## t = 3.8433, df = 66, p-value = 0.0002754\n## alternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n## 95 percent confidence interval:\n##  0.9774482 3.0909868\n## sample estimates:\n##   mean in group Aripo mean in group Guanapo \n##              20.33077              18.29655"},{"path":"cs1-students-t-test.html","id":"assumptions-2","chapter":"8 Student’s t-test","heading":"8.6 Assumptions","text":"order use Student’s t-test (results strictly valid) make three assumptions:parent distributions samples taken normally distributed (lead sample data normally distributed ).data point samples independent others.parent distributions variance.example first assumption can ignored sample sizes large enough (maths, Aripo containing 2 Guanapo 2 samples). samples smaller use tests previous section.second point can nothing unless know data collected, ignore .third point regarding equality variance can tested using either Bartlett’s test (samples normally distributed) Levene’s test (samples normally distributed).\ngets bit trickier. Although don’t care samples normally distributed t-test valid (sample size big enough compensate), need know normally distributed order decide variance test use.perform Shapiro-Wilk test samples separately:can see whilst Guanapo data probably normally distributed (p = 0.1764 > 0.05), Aripo data unlikely normally distributed (p = 0.02802 < 0.05). Remember p-value gives probability observing sample parent population actually normally distributed.\nShapiro-Wilk test quite sensitive sample size. means large sample even small deviations normality cause sample fail test, whereas smaller samples allowed pass much larger deviations. Aripo data nearly 40 points compared Guanapo data much easier Aripo sample fail compared Guanapo data.","code":"\nshapiro.test(uns_rivers$Aripo)## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_rivers$Aripo\n## W = 0.93596, p-value = 0.02802\nshapiro.test(uns_rivers$Guanapo)## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_rivers$Guanapo\n## W = 0.94938, p-value = 0.1764"},{"path":"cs1-students-t-test.html","id":"exercise-2","chapter":"8 Student’s t-test","heading":"8.7 Exercise","text":"Exercise 8.1  Q-Q plots rivers dataCreate Q-Q plots two samples discuss neighbour see light results Shapiro-Wilk test.Q-Q plots mirror found Shapiro-Wilk tests: data Aripo pretty normally distributed, whereas assumption normality Guanapo data less certain.Remember statistical tests provide answers, merely suggest patterns. Human interpretation still crucial aspect .Nevertheless, Shapiro-Wilk test shown data normal enough order test equality variance use Levene’s test.\nLevene’s test included default R packages may require installation additional package called car (Companion Applied Regression).install car package, run following command console:Alternatively, go Tools > Install packages… > Packages, type car press InstallWe can now perform Levene’s test:Ignore warning might get coercion factors (test needs create grouped variables work R versions 4.x onwards read data factors).key bit information 3rd line text Pr(>F). p-value (0.1876) test. tells us probability observing two samples come distributions variance. probability greater arbitrary significance level 0.05 can somewhat confident necessary assumptions carrying Student’s t-test two samples valid. (woohoo!)information :wanted carry Bartlett’s test (.e. data sufficiently normally distributed) command :relevant p-value given 3rd line.","code":"\npar(mfrow=c(1,2))\nqqnorm(uns_rivers$Aripo, main = \"Aripo\")\nqqline(uns_rivers$Aripo, col = \"red\")\n\nqqnorm(uns_rivers$Guanapo, main = \"Guanapo\")\nqqline(uns_rivers$Guanapo, col = \"red\")\ninstall.packages(\"car\")\nleveneTest(length ~ river, data = rivers)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  1  1.7732 0.1876\n##       66\nbartlett.test(length ~ river, data = rivers)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  length by river\n## Bartlett's K-squared = 4.4734, df = 1, p-value = 0.03443"},{"path":"cs1-students-t-test.html","id":"exercise-3","chapter":"8 Student’s t-test","heading":"8.8 Exercise","text":"Exercise 8.2  Serum cholesterol concentrations turtlesUsing following data, test null hypothesis male female turtles mean serum cholesterol concentrations.Write null alternative hypothesesChoose representation data (stacked unstacked) create csv fileImport data RSummarise visualise dataCheck assumptions (normality variance) using appropriate tests plotsPerform two-sample t-testWrite sentence summarises results found1. Hypotheses\\(H_0\\) : male mean \\(=\\) female mean\\(H_1\\) : male mean \\(\\neq\\) female mean2-4. Import Data, Summarise visualiseI’d always recommend storing data tidy, stacked format (fact can’t think situation want store data untidy, unstacked format!) example manually input data Excel following layout, saving data CSV file reading :Let’s summarise data…visualise data:always use plot summary assess three things:look like ’ve loaded data correctly?\ntwo groups extreme values plots seem match dataset, ’m happy haven’t done anything massively wrong .\ntwo groups extreme values plots seem match dataset, ’m happy haven’t done anything massively wrong .think difference two groups?\nneed result formal test make sense given data, ’s important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn’t case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.\nneed result formal test make sense given data, ’s important develop sense think going happen . Whilst ranges two groups suggests Female serum levels might higher males look things closely realise isn’t case. boxplot shows median values two groups virtually identical backed summary statistics calculated: medians 224.1, means fairly close (225.7 vs 224.2). Based , fact 13 observations total surprised test came back showing difference groups.think assumptions?\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ’ll look carefully formal checks decided whether think data normal enough us use t-test.\nHomogeneity variance. stage spread data within group looks similar, potential skew Female group ’ll want check assumptions carefully.\nNormality looks bit worrying: whilst Male group appears nice symmetric (might normal), Female group appears quite skewed (since median much closer bottom top). ’ll look carefully formal checks decided whether think data normal enough us use t-test.Homogeneity variance. stage spread data within group looks similar, potential skew Female group ’ll want check assumptions carefully.5. Check AssumptionsNormalityLet’s look normality groups separately. several ways getting serum values Males Females separately. ’ll use unstacking method, use Shapiro-Wilk followed qqplots.p-values Shapiro-Wilk tests non-significant suggests data normal enough. bit surprising given saw boxplot two bits information can use reassure us.p-value Female group smaller Male group (suggesting Female group closer non-normal Male group) makes sense.Shapiro-Wilk test generally quite relaxed normality small sample sizes (notoriously strict large sample sizes). group 6 data points , data actually really, really skewed distribution. Given Female group 6 data points , ’s surprising Shapiro-Wilk test came back saying everything OK.results Q-Q plots echo ’ve already seen Shapiro-Wilk analyses. Male group doesn’t look bad whereas Female group looks somewhat dodgy.Overall, assumption normality data doesn’t appear well met , bear mind data points group might just seeing pattern data due random chance rather underlying populations actually normally distributed. Personally, though ’d edge towards non-normal .Homogeneity VarianceIt’s clear whether data normal , isn’t clear test use . sensible approach hope agree (fingers crossed!)Bartlett’s test gives us:Levene’s test gives us:good news Levene Bartlett agree homogeneity variance two groups (thank goodness!).Overall, means ’re sure normality, homogeneity variance pretty good.6. Carry two-sample t-testBecause result Bartlett test know can carry two-sample Student’s t-test (opposed two-sample Welch’s t-test, ’re confused, see Figure 7.2)p-value 0.544, test tells insufficient evidence suggest means two groups different. suitable summary sentence :Student’s two-sample t-test indicated mean serum cholesterol level differ significantly Male Female turtles (t = 0.627, df = 11, p = 0.544).DiscussionIn reality, ambiguous normality assumption assessment, dataset actually carry two different tests; two-sample t-test equal variance Mann-Whitney U test. agreed wouldn’t matter much one reported (’d personally report short sentence say ’m wasn’t clear whether assumption normality met), acceptable report just one.","code":"\nturtle <- read.csv(\"data/examples/cs1-turtle.csv\")\n\nturtle##    serum    sex\n## 1  220.1   Male\n## 2  218.6   Male\n## 3  229.6   Male\n## 4  228.8   Male\n## 5  222.0   Male\n## 6  224.1   Male\n## 7  226.5   Male\n## 8  223.4 Female\n## 9  221.5 Female\n## 10 230.2 Female\n## 11 224.3 Female\n## 12 223.8 Female\n## 13 230.8 Female\naggregate(serum ~ sex , data = turtle, summary)##      sex serum.Min. serum.1st Qu. serum.Median serum.Mean serum.3rd Qu.\n## 1 Female   221.5000      223.5000     224.0500   225.6667      228.7250\n## 2   Male   218.6000      221.0500     224.1000   224.2429      227.6500\n##   serum.Max.\n## 1   230.8000\n## 2   229.6000\nboxplot(serum ~ sex , data = turtle)\nuns_turtle <- unstack(turtle, serum ~ sex)\nshapiro.test(uns_turtle$Male)## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_turtle$Male\n## W = 0.94392, p-value = 0.6743\nshapiro.test(uns_turtle$Female)## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_turtle$Female\n## W = 0.84178, p-value = 0.1349\npar(mfrow=c(1,2))\nqqnorm(uns_turtle$Male, main = \"Male\")\nqqline(uns_turtle$Male, col = \"red\")\nqqnorm(uns_turtle$Female, main = \"Female\")\nqqline(uns_turtle$Female, col = \"red\")\nbartlett.test(serum ~ sex, turtle)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  serum by sex\n## Bartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n# load if needed\n# library(car)\n\nleveneTest(serum ~ sex, turtle)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  1  0.2434 0.6315\n##       11\nt.test(serum ~ sex , turtle,\n       alternative=\"two.sided\",\n       var.equal=TRUE)## \n##  Two Sample t-test\n## \n## data:  serum by sex\n## t = 0.62681, df = 11, p-value = 0.5436\n## alternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n## 95 percent confidence interval:\n##  -3.575759  6.423378\n## sample estimates:\n## mean in group Female   mean in group Male \n##             225.6667             224.2429"},{},{"path":"cs1-mannwhitney-u-test.html","id":"cs1-mannwhitney-u-test","chapter":"9 Mann-Whitney U test","heading":"9 Mann-Whitney U test","text":"test also compares two samples, however test (contrast Student’s t-test) don’t assume parent distributions normally distributed. order compare medians two groups still need parent distributions (consequently samples) shape variance. test look see medians two parent distributions differ significantly .","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"section-commands-3","chapter":"9 Mann-Whitney U test","heading":"9.1 Section commands","text":"new commands used section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"data-and-hypotheses-2","chapter":"9 Mann-Whitney U test","heading":"9.2 Data and hypotheses","text":", use rivers dataset. want test whether median body length male guppies differs samples. form following null alternative hypotheses:\\(H_0\\): difference median body length two groups 0 \\((\\mu - \\mu G = 0)\\)\\(H_1\\): difference median body length two groups 0 \\((\\mu - \\mu G \\neq 0)\\)use two-tailed Mann-Whitney U test see can reject null hypothesis.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"summarise-and-visualise-2","chapter":"9 Mann-Whitney U test","heading":"9.3 Summarise and visualise","text":"previous section.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"implement-test-1","chapter":"9 Mann-Whitney U test","heading":"9.4 Implement test","text":"Perform two-tailed, Mann-Whitney U test:case, data tidy format:first argument must formula format: variable ~ categoryThe second argument must name data frameThe third argument gives type alternative hypothesis must one two.sided, greater less","code":"\nwilcox.test(length ~ river, data = rivers,\n            alternative = \"two.sided\")## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  length by river\n## W = 841, p-value = 0.0006464\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"cs1-mannwhitney-u-test.html","id":"interpret-output-and-report-results-1","chapter":"9 Mann-Whitney U test","heading":"9.5 Interpret output and report results","text":"may get warning message console stating compute exact p-value ties. just means data points exactly value affects internal mathematics slightly. However, given p-value small, something need worry .warning message:1st line gives name test 2nd line reminds dataset called, variables usedThe 3rd line contains two key outputs test:\ncalculated W-value 841 (’ll use reporting)\np-value 0.0006464.\ncalculated W-value 841 (’ll use reporting)p-value 0.0006464.4th line simply states alternative hypothesis terms difference two sample medians difference one distribution shifted relative .Given p-value less 0.05 can reject null hypothesis confidence level.\n, p-value 3rd line ’re interested . Since p-value small (much smaller standard significance level) choose say “unlikely two samples came parent distribution can reject null hypothesis.”put completely, can state :Mann-Whitney test indicated median body length male guppies Guanapo river (18.8 mm) differs significantly median body length male guppies Aripo river (20.1 mm) (W = 841, p = 0.0006).","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"assumptions-3","chapter":"9 Mann-Whitney U test","heading":"9.6 Assumptions","text":"checked previously.","code":""},{"path":"cs1-mannwhitney-u-test.html","id":"exercise-4","chapter":"9 Mann-Whitney U test","heading":"9.7 Exercise","text":"Exercise 9.1  Analyse turtle dataset using Mann Whitney test.follow process Student’s t-test.Hypotheses\\(H_0\\) : male median \\(=\\) female median\\(H_1\\) : male median \\(\\neq\\) female medianSummarise visualiseThis .AssumptionsWe’ve already checked variances two groups similar, ’re OK . Whilst Mann-Whitney test doesn’t require normality symmetry distributions require distributions shape. example, just handful data points group, ’s quite hard make call one way another. advice case say unless ’s obvious distributions different can just allow assumption pass, ’re going see obvious differences distribution shape considerably data points .Carry Mann-Whitney testThis gives us exactly conclusion got two-sample t-test .e. isn’t significant difference two groups.Mann-Whitney test indicated wasn’t significant difference median Serum Cholesterol levels male female turtles (W = 26, p = 0.534)","code":"\nwilcox.test(serum ~ sex, data = turtle,\n            alternative = \"two.sided\")## \n##  Wilcoxon rank sum exact test\n## \n## data:  serum by sex\n## W = 26, p-value = 0.5338\n## alternative hypothesis: true location shift is not equal to 0"},{},{"path":"cs1-paired-two-sample-t-test.html","id":"cs1-paired-two-sample-t-test","chapter":"10 Paired two-sample t-test","heading":"10 Paired two-sample t-test","text":"paired t-test used two samples continuous data can paired (examples sort data weights individuals diet). test applicable number paired points within samples large (>30) , number points small, test also works parent distributions normally distributed.","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"section-commands-4","chapter":"10 Paired two-sample t-test","heading":"10.1 Section commands","text":"new commands section.","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"data-and-hypotheses-3","chapter":"10 Paired two-sample t-test","heading":"10.2 Data and hypotheses","text":"example, suppose measure cortisol levels 20 adult females (nmol/l) first thing morning evening. want test whether cortisol levels differs two measurement times. initially form following null alternative hypotheses:\\(H_0\\): difference cortisol level times (\\(\\mu M = \\mu E\\))\\(H_1\\): difference cortisol levels times (\\(\\mu M \\neq \\mu E\\))use two-sample, two-tailed paired t-test see can reject null hypothesis.use two-sample test now two samplesWe use two-tailed t-test want know data suggest true (population) means different one another rather one mean specifically bigger smaller otherWe use paired test data point first sample can linked another data point second sample connecting factorWe’re using t-test ’re assuming parent populations normal equal variance (’ll check bit)data stored unstacked format file “CS1-twopaired.csv.”\nRead R:","code":"\ncortisol <- read.csv(\"data/raw/CS1-twopaired.csv\")"},{"path":"cs1-paired-two-sample-t-test.html","id":"summarise-and-visualise-3","chapter":"10 Paired two-sample t-test","heading":"10.3 Summarise and visualise","text":"box plot capture cortisol level individual subject changed though. can explore individual changes morning evening creating boxplot differences two times measurement.differences cortisol levels appear much less zero, (meaning evening cortisol levels appear much lower morning ones). expect test give pretty significant result.","code":"\nsummary(cortisol)##     morning         evening     \n##  Min.   :146.1   Min.   : 60.1  \n##  1st Qu.:266.6   1st Qu.:137.8  \n##  Median :320.5   Median :188.9  \n##  Mean   :313.5   Mean   :197.4  \n##  3rd Qu.:359.7   3rd Qu.:260.8  \n##  Max.   :432.5   Max.   :379.3\nboxplot(cortisol, ylab = \"Level (nmol/l)\")\n# calculate the difference between evening and morning values\nchangeCor <- cortisol$evening - cortisol$morning\n\nboxplot(changeCor, ylab = \"Change in cortisol (nmol/l)\")"},{"path":"cs1-paired-two-sample-t-test.html","id":"implement-test-2","chapter":"10 Paired two-sample t-test","heading":"10.4 Implement test","text":"Perform two-sample, two-tailed, paired t-test:first two arguments must vectors containing numerical data samplesThe third argument gives type alternative hypothesis must one two.sided, greater lessThe fourth argument says data paired","code":"\nt.test(cortisol$evening, cortisol$morning,\n       alternative = \"two.sided\", paired = TRUE)"},{"path":"cs1-paired-two-sample-t-test.html","id":"interpret-output-and-report-results-2","chapter":"10 Paired two-sample t-test","heading":"10.5 Interpret output and report results","text":"perspective value interested 3rd line (p-value = 5.288e10-5). Given substantially less 0.05 can reject null hypothesis state:two-tailed, paired t-test indicated cortisol level adult females differed significantly morning (313.5 nmol/l) evening (197.4 nmol/l) (t = -5.1833, df = 19, p = 5.3x10-5).","code":"## \n##  Paired t-test\n## \n## data:  cortisol$evening and cortisol$morning\n## t = -5.1833, df = 19, p-value = 5.288e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -162.96038  -69.20962\n## sample estimates:\n## mean of the differences \n##                -116.085"},{"path":"cs1-paired-two-sample-t-test.html","id":"assumptions-4","chapter":"10 Paired two-sample t-test","heading":"10.6 Assumptions","text":"exercise!","code":""},{"path":"cs1-paired-two-sample-t-test.html","id":"exercise-5","chapter":"10 Paired two-sample t-test","heading":"10.7 Exercise","text":"Exercise 10.1  Checking assumptionsCheck assumptions necessary paired t-test.\npaired t-test appropriate test?paired test really just one-sample test disguise. actually don’t care much distributions individual groups. Instead care properties differences. paired t-test valid dataset, need differences morning evening values normally distributed.Let’s check Shapiro-Wilk Q-Q plots using changeCor variable created earlier.Shapiro-Wilk test says data normal enough whilst Q-Q plot mostly fine, suggestion snaking bottom left. ’m actually OK suggestion snaking actually due single point (last point left). cover point thumb (finger choice) remaining points Q-Q plot look pretty damn good, suggestion snaking actually driven single point (can happen chance). ’m actually happy assumption normality well met case. single point check useful thing remember assessing diagnostic plots., yep, paired t-test appropriate dataset.","code":"\nshapiro.test(changeCor)## \n##  Shapiro-Wilk normality test\n## \n## data:  changeCor\n## W = 0.92362, p-value = 0.1164\nqqnorm(changeCor)\nqqline(changeCor, col = \"red\")"},{},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"cs1-twosample-wilcoxon-signed-rank","chapter":"11 Wilcoxon signed-rank test","heading":"11 Wilcoxon signed-rank test","text":"Wilcoxon signed-rank test alternative paired t-test. require data drawn normal distributions, require distribution differences symmetric. ’re effectively testing see median differences two samples differs significantly zero.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"section-commands-5","chapter":"11 Wilcoxon signed-rank test","heading":"11.1 Section commands","text":"new commands section.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"data-and-hypotheses-4","chapter":"11 Wilcoxon signed-rank test","heading":"11.2 Data and hypotheses","text":"Using cortisol dataset form following null alternative hypotheses:\\(H_0\\): median difference cortisol levels two groups 0 \\((\\mu M = \\mu E)\\)\\(H_1\\): median difference cortisol levels two groups 0 \\((\\mu M \\neq \\mu E)\\)use two-tailed Wilcoxon signed-rank test see can reject null hypothesis.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"summarise-and-visualise-4","chapter":"11 Wilcoxon signed-rank test","heading":"11.3 Summarise and visualise","text":"Already implemented previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"implement-test-3","chapter":"11 Wilcoxon signed-rank test","heading":"11.4 Implement test","text":"Perform two-tailed, Wilcoxon signed-rank test:first two arguments must two samples numerical vector formatThe second argument gives type alternative hypothesis must one two.sided, greater lessThe third argument indicates test paired","code":"\nwilcox.test(cortisol$morning, cortisol$evening,\n            alternative = \"two.sided\", paired = TRUE)"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"interpret-output-and-report-results-3","chapter":"11 Wilcoxon signed-rank test","heading":"11.5 Interpret output and report results","text":"p value given 3rd line (p-value = 0.0001678). Given less 0.05 can still reject null hypothesis.two-tailed, Wilcoxon signed-rank test indicated median cortisol level adult females differed significantly morning (320.5 nmol/l) evening (188.9 nmol/l) (V = 197, p = 0.00017).","code":"## \n##  Wilcoxon signed rank exact test\n## \n## data:  cortisol$morning and cortisol$evening\n## V = 197, p-value = 0.0001678\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"assumptions-5","chapter":"11 Wilcoxon signed-rank test","heading":"11.6 Assumptions","text":"checked previously.","code":""},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"exercise-6","chapter":"11 Wilcoxon signed-rank test","heading":"11.7 Exercise","text":"Exercise 11.1  Deer legsUsing following data, test null hypothesis fore hind legs deer length.results provide evidence suggest fore- hind-leg length differ deer?Write null alternative hypothesesChoose representation data (stacked unstacked) create csv fileImport data RSummarise visualise dataPerform two-sample paired t-testPerform Wilcoxon signed-rank testNow check assumptions (normality variance) using appropriate testsDiscuss (virtual) neighbour test appropriate?Write sentence summarise results found1. Hypotheses\\(H_0\\) : foreleg average (mean median) \\(=\\) hindleg average (mean median)\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average2-4. Import Data, Summarise visualiseI always recommend storing data stacked format even example, even though case might seem easier store data unstacked format (pretty much time even sensible option). example manually input data excel following layout:ordering data important ; first hindleg row corresponds first foreleg row, second second . indicate use id column, observation unique ID.Let’s look data see can see.looks though might difference legs, hindlegs longer forelegs. However, representation obscures fact paired data. really need look difference leg length deer:gives us much clearer picture. looks though hindlegs 4 cm longer forelegs, average. also suggests leg differences might normally distributed (data look bit skewed).5. Perform two-sample t-testThe paired t-test assumes data stored exactly entered (.e. first hindleg row matches first foreleg row). apparently see significant difference.6. Perform paired Wilcoxon testThe paired Wilcoxon test makes assumptions order data paired t-test. significant difference.7. Check assumptionsWe need consider distribution difference leg lengths rather individual distributions.Shapiro-Wilk test Q-Q plot suggest difference data aren’t normally distributed, rules paired t-test. therefore consider paired Wilcoxon test next. Remember test requires distribution differences symmetric, whereas box-plot suggested data much skewed.8. ConclusionsSo, frustratingly, neither tests appropriate dataset. differences fore- hind leg lengths neither normal enough paired t-test symmetric enough Wilcoxon test don’t enough data just use t-test (’d need 30 points ). situation? Well answer aren’t actually traditional statistical tests valid dataset stands!two options available someone:try transforming raw data (take logs, square root, reciprocals) hope one leads modified dataset satisfies assumptions one tests ’ve covered, oruse permutation test approach (work beyond scope course).reason included example first practical purely illustrate simple dataset apparently clear message (leg lengths differ within deer) can intractable. don’t need complex datasets go beyond capabilities classical statistics.Jeremy Clarkson put :bombshell, ’s time end. Goodnight!","code":"## # A tibble: 10 × 2\n##    hindleg foreleg\n##      <dbl>   <dbl>\n##  1     142     138\n##  2     140     136\n##  3     144     147\n##  4     144     139\n##  5     142     143\n##  6     146     141\n##  7     149     143\n##  8     150     145\n##  9     142     136\n## 10     148     146\ndeer <- read.csv(\"data/examples/cs1-deer.csv\")## # A tibble: 20 × 3\n##       id leg     length\n##    <dbl> <chr>    <dbl>\n##  1     1 hindleg    142\n##  2     2 hindleg    140\n##  3     3 hindleg    144\n##  4     4 hindleg    144\n##  5     5 hindleg    142\n##  6     6 hindleg    146\n##  7     7 hindleg    149\n##  8     8 hindleg    150\n##  9     9 hindleg    142\n## 10    10 hindleg    148\n## 11     1 foreleg    138\n## 12     2 foreleg    136\n## 13     3 foreleg    147\n## 14     4 foreleg    139\n## 15     5 foreleg    143\n## 16     6 foreleg    141\n## 17     7 foreleg    143\n## 18     8 foreleg    145\n## 19     9 foreleg    136\n## 20    10 foreleg    146\naggregate(length ~ leg, data = deer, summary)##       leg length.Min. length.1st Qu. length.Median length.Mean length.3rd Qu.\n## 1 foreleg      136.00         138.25        142.00      141.40         144.50\n## 2 hindleg      140.00         142.00        144.00      144.70         147.50\n##   length.Max.\n## 1      147.00\n## 2      150.00\nboxplot(length ~ leg, data = deer)\nuns_deer <- unstack(deer, length ~ leg)\ndeerDiff <- uns_deer$hindleg - uns_deer$foreleg\nsummary(deerDiff)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    -3.0     2.5     4.5     3.3     5.0     6.0\nboxplot(deerDiff)\nt.test(length ~ leg, data = deer, paired = TRUE)## \n##  Paired t-test\n## \n## data:  length by leg\n## t = -3.4138, df = 9, p-value = 0.007703\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -5.486752 -1.113248\n## sample estimates:\n## mean of the differences \n##                    -3.3\nwilcox.test(length ~ leg, data = deer, paired = TRUE)## Warning in wilcox.test.default(x = c(138L, 136L, 147L, 139L, 143L, 141L, :\n## cannot compute exact p-value with ties## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  length by leg\n## V = 4, p-value = 0.01859\n## alternative hypothesis: true location shift is not equal to 0\nshapiro.test(deerDiff)## \n##  Shapiro-Wilk normality test\n## \n## data:  deerDiff\n## W = 0.81366, p-value = 0.02123\nqqnorm(deerDiff)\nqqline(deerDiff, col = \"red\")"},{"path":"cs1-twosample-wilcoxon-signed-rank.html","id":"key-points-1","chapter":"11 Wilcoxon signed-rank test","heading":"11.8 Key points","text":"use two-sample tests see two samples continuous data come parent distributionThis essentially boils testing mean median differs two samplesThere 5 key two-sample tests: Student’s t-test, Welch’s t-test, Mann-Whitney U test, paired t-test Wilcoxon signed-rank testWhich one use depends normality distribution, sample size, paired unpaired data variance samplesParametric tests used data normally distributed sample size largeNon-parametric tests used data normally distributed sample size smallEquality variance determines test appropriateYou can ask 3 questions determine test:\ndata paired?\nneed parametric non-parametric test\ncan assume equality variance?\ndata paired?need parametric non-parametric testcan assume equality variance?","code":""},{},{"path":"cs2-intro.html","id":"cs2-intro","chapter":"12 Introduction","heading":"12 Introduction","text":"","code":""},{"path":"cs2-intro.html","id":"objectives-3","chapter":"12 Introduction","heading":"12.1 Objectives","text":"Aim: introduce R commands analysing single categorical predictors.end practical participants able perform following statistical analyses:One-way Analysis Variance (ANOVA)Kruskal-Wallis testFor , participants able :Perform test RInterpret outputCheck assumptions testCarry post-hoc test appropriateThe tests covered practical :One-way ANOVAKruskall-Wallis test","code":""},{"path":"cs2-intro.html","id":"background-1","chapter":"12 Introduction","heading":"12.2 Background","text":"practical focuses implementation various statistical tests relating categorical predictors. boil ANOVA Kruskal-Wallis (non-parametric alternative).\n, focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-1.html","id":"introduction-1","chapter":"13 Introduction","heading":"13 Introduction","text":"practical introducing can compare data different groups.","code":""},{"path":"introduction-1.html","id":"cs2-datasets","chapter":"13 Introduction","heading":"13.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"cs2-anova.html","id":"cs2-anova","chapter":"14 ANOVA","heading":"14 ANOVA","text":"","code":""},{"path":"cs2-anova.html","id":"objectives-4","chapter":"14 ANOVA","heading":"14.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data?ANOVA?check differences groups?ObjectivesBe able perform ANOVA RUnderstand ANOVA output evaluate assumptionsUnderstand post-hoc testing R","code":""},{"path":"cs2-anova.html","id":"purpose-and-aim-2","chapter":"14 ANOVA","heading":"14.2 Purpose and aim","text":"Analysis variance ANOVA test can used multiple samples continuous data. Whilst possible use ANOVA two samples, generally used three groups. used find samples came parent distributions mean. can thought generalisation two-sample Student’s t-test.","code":""},{"path":"cs2-anova.html","id":"section-commands-6","chapter":"14 ANOVA","heading":"14.3 Section commands","text":"New commands used section.","code":""},{"path":"cs2-anova.html","id":"data-and-hypotheses-5","chapter":"14 ANOVA","heading":"14.4 Data and hypotheses","text":"example, suppose measure feeding rate oyster catchers (shellfish per hour) three sites characterised degree shelter wind, imaginatively called exposed (E), partially sheltered (P) sheltered (S). want test whether data support hypothesis feeding rates don’t differ locations. form following null alternative hypotheses:\\(H_0\\): mean feeding rates three sites \\(\\mu E = \\mu P = \\mu S\\)\\(H_1\\): mean feeding rates equal.use one-way ANOVA test check .use one-way ANOVA test one predictor variable (categorical variable location).’re using ANOVA two groups don’t know better yet respect exact assumptions.data stored file CS2-oystercatcher.csv.","code":""},{"path":"cs2-anova.html","id":"summarise-and-visualise-5","chapter":"14 ANOVA","heading":"14.5 Summarise and visualise","text":"First read data.Next summarise data visualise . quick peek first rows data head() can see data organised.data stacked format. first column contains information feeding rates called feeding. second column categorical data type site called site.Looking data, appears noticeable difference feeding rates three sites. probably expect reasonably significant statistical result .","code":"\noystercatcher <- read.csv(\"data/raw/CS2-oystercatcher.csv\")\nhead(oystercatcher)##   feeding    site\n## 1    14.2 Exposed\n## 2    16.5 Exposed\n## 3     9.3 Exposed\n## 4    15.1 Exposed\n## 5    13.4 Exposed\n## 6    18.4 Partial\naggregate(feeding ~ site, data = oystercatcher, summary)##        site feeding.Min. feeding.1st Qu. feeding.Median feeding.Mean\n## 1   Exposed         9.30           13.40          14.20        13.70\n## 2   Partial        13.00           16.50          17.40        17.14\n## 3 Sheltered        21.50           22.20          24.10        23.64\n##   feeding.3rd Qu. feeding.Max.\n## 1           15.10        16.50\n## 2           18.40        20.40\n## 3           25.10        25.30\nboxplot(feeding ~ site, data = oystercatcher)"},{"path":"cs2-anova.html","id":"implement-test-4","chapter":"14 ANOVA","heading":"14.6 Implement test","text":"Perform ANOVA test data:first line fits linear model data (.e. finds means three groups calculates load intermediary data need statistical analysis) stores information R object (’ve called lm_oystercatchers, can call like). second line actually carries ANOVA analysis.first argument must formula format: response ~ predictorIf data stored stacked format, second argument must name data frameThe anova() command takes linear model object main argument","code":"\nlm_oystercatcher <- lm(feeding ~ site, data = oystercatcher)\n\nanova(lm_oystercatcher)"},{"path":"cs2-anova.html","id":"interpret-output-and-report-results-4","chapter":"14 ANOVA","heading":"14.7 Interpret output and report results","text":"output now see console window:1st line just tells ANOVA testThe 2nd line tells response variable (case feeding)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 2 12 (’ll need reporting)\nF value column contains F statistic, 21.508 (’ll need reporting).\np-value 0.0001077 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) columns used calculate F statistic don’t need know .\nDf column contains degrees freedom values row, 2 12 (’ll need reporting)F value column contains F statistic, 21.508 (’ll need reporting).p-value 0.0001077 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) columns used calculate F statistic don’t need know .6th line symbolic codes represent big (small) p-value ; , p-value smaller 0.001 *** symbol next (). Whereas p-value 0.01 0.05 simply * character next , etc. Thankfully can cope actual numbers don’t need short-hand code determine reporting experiments (please tell ’s true…!), p-value ’re interested shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say “unlikely three samples came parent distribution” can reject null hypothesis state :one-way ANOVA showed mean feeding rate oystercatchers differed significantly locations (F = 21.51, df = 2, 12, p = 0.00011).Note included (brackets) information test statistic (F = 21.51), degrees freedom (df = 2, 12), p-value (p = 0.00011).","code":"## Analysis of Variance Table\n## \n## Response: feeding\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## site       2 254.812 127.406  21.508 0.0001077 ***\n## Residuals 12  71.084   5.924                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cs2-anova.html","id":"assumptions-6","chapter":"14 ANOVA","heading":"14.8 Assumptions","text":"use ANOVA test, make three assumptions:parent distributions samples taken normally distributedEach data point samples independent othersThe parent distributions varianceIn similar way two-sample tests consider normality equality variance assumptions using tests graphical inspection (ignore independence assumption).1. NormalityUnstack data perform Shapiro-Wilk test group separately.output now see console window:can see three groups appear normally distributed good.ANOVA however, considering group turn often considered quite excessive , cases, sufficient consider normality combined set residuals data. ’ll explain residuals properly next session effectively difference data point group mean. residuals can obtained directly linear model fitted earlier.Extract residuals data check normality:, can see combined residuals three groups appear normally distributed (expected given normally distributed individually!)2. Equality VarianceWe now test equality variance using Bartlett’s test (since ’ve just found individual groups normally distributed).Perform Bartlett’s test data:relevant p-value given 3rd line. see group appear variance.3. Graphical Interpretation Diagnostic PlotsR provides convenient set graphs allow us assess assumptions graphically. simply ask R plot lm object created, can see diagnostic plots.Create standard set diagnostic plots:second line creates three diagnostic plots (actually tries create four plots can’t dataset ’ll also see warning text output screen (something hat values). ’ll go next session ’s easier explain).example, two plots (top-left bottom-left) show effectively thing: distribution data group look like. allow informal check equality variance assumption.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).\nbottom-left graph, look red line want approximately horizontal.\ntop-left graph want data symmetric 0 horizontal line spread (please ignore red line; unhelpful addition graphs).bottom-left graph, look red line want approximately horizontal.top-right graph familiar Q-Q plot used previously assess normality, looks combined residuals groups (much way looked Shapiro-Wilk test combined residuals).can see graphs much line ’ve just looked using test, reassuring. groups appear spread data, whilst QQ-plot isn’t perfect, appears assumption normality alright.stage, point nearly always stick graphical method assessing assumptions test. Assumptions rarely either completely met met always degree personal assessment.Whilst formal statistical tests (like Shapiro) technically fine, can often create false sense things absolutely right wrong spite fact still probabilistic statistical tests. exercises using approaches whilst gain confidence experience interpreting graphical output whilst absolutely fine use future strongly recommend don’t rely solely statistical tests isolation.","code":"\nuns_oyster <- unstack(oystercatcher)\n\nshapiro.test(uns_oyster$Exposed)\nshapiro.test(uns_oyster$Partial)\nshapiro.test(uns_oyster$Sheltered)## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_oyster$Exposed\n## W = 0.9151, p-value = 0.4988## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_oyster$Partial\n## W = 0.96913, p-value = 0.8697## \n##  Shapiro-Wilk normality test\n## \n## data:  uns_oyster$Sheltered\n## W = 0.88532, p-value = 0.3341\nresid_oyster <- residuals(lm_oystercatcher)\n\nshapiro.test(resid_oyster)## \n##  Shapiro-Wilk normality test\n## \n## data:  resid_oyster\n## W = 0.93592, p-value = 0.3338\nbartlett.test(feeding ~ site, data = oystercatcher)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  feeding by site\n## Bartlett's K-squared = 0.90632, df = 2, p-value = 0.6356\n# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(lm_oystercatcher)\n# and return the window back to normal\npar(mfrow = c(1,1))## hat values (leverages) are all = 0.2\n##  and there are no factor predictors; no plot no. 5"},{"path":"cs2-anova.html","id":"post-hoc-testing","chapter":"14 ANOVA","heading":"14.9 Post-hoc testing","text":"One drawback using ANOVA test tests see means , get significant result using ANOVA can say means , rather anything pairs groups differ. example, consider following boxplot three samples.group random sample 20 points normal distribution variance 1. Groups 1 2 come parent population mean 0 whereas group 3 come parent population mean 2. data clearly satisfy assumptions ANOVA test.1. Read data plot2. Test significant difference group meansHere p-value 2.39x10-7 test conclusively rejected hypothesis means equal.However, due sample means different, rather just one groups different others. order drill investigate use new test called Tukey’s range test (Tukey’s honest significant difference test – always makes think terrible cowboy/western dialogue). compare groups pairwise fashion reports whether significant difference exists.3. Performing Tukey’s test dataThe first argument repeats ANOVA using different function ‘aov().’ store output function R object called aov_tukey.\nNote TukeyHSD() function takes output aov() function argument raw data.bottom three lines contain information want. final column (entitled p adj) p-value ’re looking . null hypothesis case difference mean two groups. can see first line shows isn’t significant difference sample1 sample2 2nd 3rd lines show significant difference sample1 sample3, well sample2 sample3. matches expected based boxplot.4. AssumptionsWhen use Tukey’s range test matter debate (strangely enough lot statistical analysis techniques currently matters opinion rather mathematical fact – explain little whole field appears bloody confusing!)people claim perform Tukey’s range test (post-hoc tests) preceding ANOVA test showed significant difference groups ANOVA test shown significant differences groups stop .people say rubbish can hell like, like long tell people .background rather involved one reasons debate prevent -called data-dredging p-hacking. scientists/analysts fixated getting “significant” result perform huge variety statistical techniques find one shows data significant (particular problem psychological studies – point fingers though, working hard sort stuff . Kudos!).Whether use post-hoc testing depend experimental design questions ’re attempting answer.Tukey’s range test, decide use , requires three assumptions ANOVA test:Normality distributionsEquality variance groupsIndependence observations","code":"\ntukey <- read.csv(\"data/raw/CS2-tukey.csv\")\nboxplot(response ~ group, data = tukey)\nlm_tukey <- lm(response ~ group, data = tukey)\n\nanova(lm_tukey)## Analysis of Variance Table\n## \n## Response: response\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## group      2 33.850 16.9250   20.16 2.392e-07 ***\n## Residuals 57 47.854  0.8395                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\naov_tukey <- aov(response ~ group, data = tukey)\n\nTukeyHSD(aov_tukey)##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = response ~ group, data = tukey)\n## \n## $group\n##                      diff        lwr      upr     p adj\n## sample2-sample1 0.3037563 -0.3934982 1.001011 0.5498005\n## sample3-sample1 1.7233591  1.0261047 2.420614 0.0000005\n## sample3-sample2 1.4196028  0.7223484 2.116857 0.0000246"},{"path":"cs2-anova.html","id":"exercise-7","chapter":"14 ANOVA","heading":"14.10 Exercise","text":"Exercise 14.1  Juvenile lobster weightJuvenile lobsters aquaculture grown three different diets (fresh mussels, semi-dry pellets dry flakes). nine weeks, wet weight :evidence diet affects growth rate lobsters?Write null alternative hypothesesImport data R\ndata stored file data/raw/CS2-lobsters.csv\ndata stored file data/raw/CS2-lobsters.csvSummarise visualise dataCheck assumptions using appropriate tests graphical analysesPerform ANOVA testWrite sentence summarise results foundPerform post-hoc test report findings\\(H_0\\) : means equal\\(H_1\\) : means equalThe data stored .csv file stacked format columns called weight diet.Let’s look data see can see.always use plot summary assess three things:load data properly?see three groups reasonable values. aren’t data points obviously wrong (negative, zero massively big) right number groups. looks didn’t anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ’s quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ’re immediately massively worried lack normality. , Flakes Mussels appear similar variances ’s bit hard decide ’s going Pellets. ’s hard say ’s going assumptions ’ll wait see tests say.NormalityWe’ll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ’ll use Q-Q plot.’ll need unstack data use Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let’s look Shapiro-Wilk test data together:hand says everything fine. Let’s look Q-Q-plot:, ’ve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots, can tell R plot specific one. (want know look plot.lm help documentation using ?plot.lm). ’ve asked R plot Q-Q plot = 2 argument.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ’d happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn’t confident.Equality VarianceWe’ll consider Bartlett test ’ll look diagnostic plots .code, ’ve used trick argument plot two diagnostic plots relate equality variance (residuals vs fitted scale-location).three methods agree isn’t issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.assumptions normality equality variance met can confident one-way ANOVA appropriate test.one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging “significant” results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey’s range test, important point result one-way ANOVA test doesn’t preclude carrying Tukey test.","code":"\nlobsters <- read.csv(\"data/raw/CS2-lobsters.csv\")##    weight    diet\n## 1   151.6 Mussels\n## 2   132.1 Mussels\n## 3   104.2 Mussels\n## 4   153.5 Mussels\n## 5   132.0 Mussels\n## 6   119.0 Mussels\n## 7   161.9 Mussels\n## 8   117.7 Pellets\n## 9   110.8 Pellets\n## 10  128.6 Pellets\n## 11  110.1 Pellets\n## 12  175.2 Pellets\n## 13  101.8  Flakes\n## 14  102.9  Flakes\n## 15   90.4  Flakes\n## 16  132.8  Flakes\n## 17  129.3  Flakes\n## 18  129.4  Flakes\naggregate(weight ~ diet, data = lobsters, summary)##      diet weight.Min. weight.1st Qu. weight.Median weight.Mean weight.3rd Qu.\n## 1  Flakes     90.4000       102.0750      116.1000    114.4333       129.3750\n## 2 Mussels    104.2000       125.5000      132.1000    136.3286       152.5500\n## 3 Pellets    110.1000       110.8000      117.7000    128.4800       128.6000\n##   weight.Max.\n## 1    132.8000\n## 2    161.9000\n## 3    175.2000\nboxplot(weight ~ diet, data = lobsters)\nlobst_uns <- unstack(lobsters, weight ~ diet)\nshapiro.test(lobst_uns$Flakes)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Flakes\n## W = 0.84368, p-value = 0.1398\nshapiro.test(lobst_uns$Mussels)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Mussels\n## W = 0.94784, p-value = 0.71\nshapiro.test(lobst_uns$Pellets)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Pellets\n## W = 0.76706, p-value = 0.0425\nresid_lobst <- residuals(lm(weight ~ diet, data = lobsters))\nshapiro.test(resid_lobst)## \n##  Shapiro-Wilk normality test\n## \n## data:  resid_lobst\n## W = 0.94779, p-value = 0.3914\nplot(lm(weight ~ diet , data = lobsters),\n     which = 2)\nbartlett.test(weight ~ diet, data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\nplot(lm(weight ~ diet, data = lobsters),\n     which = c(1,3))\nanova(lm(weight ~ diet, data = lobsters))## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87\nTukeyHSD(aov(weight ~ diet, data = lobsters))##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ diet, data = lobsters)\n## \n## $diet\n##                      diff        lwr      upr     p adj\n## Mussels-Flakes  21.895238  -9.661957 53.45243 0.2024851\n## Pellets-Flakes  14.046667 -20.300196 48.39353 0.5508657\n## Pellets-Mussels -7.848571 -41.061560 25.36442 0.8148766"},{"path":"cs2-anova.html","id":"hypotheses","chapter":"14 ANOVA","heading":"14.10.1 Hypotheses","text":"\\(H_0\\) : means equal\\(H_1\\) : means equal","code":""},{"path":"cs2-anova.html","id":"import-data-summarise-and-visualise","chapter":"14 ANOVA","heading":"14.10.2 Import Data, summarise and visualise","text":"data stored .csv file stacked format columns called weight diet.Let’s look data see can see.always use plot summary assess three things:load data properly?see three groups reasonable values. aren’t data points obviously wrong (negative, zero massively big) right number groups. looks didn’t anything obviously wrong.expect result statistical test?Whilst Mussels group look higher two groups, Pellets Flakes appear almost identical terms average values, ’s quite bit overlap Mussels group. non-significant result likely answer, surprised see significant p-value - especially given small sample size .think assumptions?groups appear mainly symmetric (although Pellets bit weird) ’re immediately massively worried lack normality. , Flakes Mussels appear similar variances ’s bit hard decide ’s going Pellets. ’s hard say ’s going assumptions ’ll wait see tests say.","code":"\nlobsters <- read.csv(\"data/raw/CS2-lobsters.csv\")##    weight    diet\n## 1   151.6 Mussels\n## 2   132.1 Mussels\n## 3   104.2 Mussels\n## 4   153.5 Mussels\n## 5   132.0 Mussels\n## 6   119.0 Mussels\n## 7   161.9 Mussels\n## 8   117.7 Pellets\n## 9   110.8 Pellets\n## 10  128.6 Pellets\n## 11  110.1 Pellets\n## 12  175.2 Pellets\n## 13  101.8  Flakes\n## 14  102.9  Flakes\n## 15   90.4  Flakes\n## 16  132.8  Flakes\n## 17  129.3  Flakes\n## 18  129.4  Flakes\naggregate(weight ~ diet, data = lobsters, summary)##      diet weight.Min. weight.1st Qu. weight.Median weight.Mean weight.3rd Qu.\n## 1  Flakes     90.4000       102.0750      116.1000    114.4333       129.3750\n## 2 Mussels    104.2000       125.5000      132.1000    136.3286       152.5500\n## 3 Pellets    110.1000       110.8000      117.7000    128.4800       128.6000\n##   weight.Max.\n## 1    132.8000\n## 2    161.9000\n## 3    175.2000\nboxplot(weight ~ diet, data = lobsters)"},{"path":"cs2-anova.html","id":"explore-assumptions","chapter":"14 ANOVA","heading":"14.10.3 Explore Assumptions","text":"NormalityWe’ll really thorough consider normality group separately jointly using Shapiro-Wilk test, well looking Q-Q plot. reality, examples , ’ll use Q-Q plot.’ll need unstack data use Shapiro-Wilk test individual groups:Flakes Mussels fine, , suspected earlier, Pellets appears marginally significant Normality test result.Let’s look Shapiro-Wilk test data together:hand says everything fine. Let’s look Q-Q-plot:, ’ve used extra argument normal diagnostic plots call. default option plot 4 diagnostic plots, can tell R plot specific one. (want know look plot.lm help documentation using ?plot.lm). ’ve asked R plot Q-Q plot = 2 argument.Q-Q plot looks OK, perfect, good enough us confidence normality data.Overall, ’d happy assumption normality adequately well met . suggested lack normality Pellets just significant take account 5 data points group. lot points group, Q-Q plot considerably worse wouldn’t confident.Equality VarianceWe’ll consider Bartlett test ’ll look diagnostic plots .code, ’ve used trick argument plot two diagnostic plots relate equality variance (residuals vs fitted scale-location).three methods agree isn’t issues equality variance:Bartlett test p-value large non-significantthe spread points three groups residuals vs fitted graph roughly samethe red line scale-location graph pretty horizontalOverall, assumption pretty well met.","code":"\nlobst_uns <- unstack(lobsters, weight ~ diet)\nshapiro.test(lobst_uns$Flakes)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Flakes\n## W = 0.84368, p-value = 0.1398\nshapiro.test(lobst_uns$Mussels)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Mussels\n## W = 0.94784, p-value = 0.71\nshapiro.test(lobst_uns$Pellets)## \n##  Shapiro-Wilk normality test\n## \n## data:  lobst_uns$Pellets\n## W = 0.76706, p-value = 0.0425\nresid_lobst <- residuals(lm(weight ~ diet, data = lobsters))\nshapiro.test(resid_lobst)## \n##  Shapiro-Wilk normality test\n## \n## data:  resid_lobst\n## W = 0.94779, p-value = 0.3914\nplot(lm(weight ~ diet , data = lobsters),\n     which = 2)\nbartlett.test(weight ~ diet, data = lobsters)## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by diet\n## Bartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\nplot(lm(weight ~ diet, data = lobsters),\n     which = c(1,3))"},{"path":"cs2-anova.html","id":"carry-out-one-way-anova","chapter":"14 ANOVA","heading":"14.10.4 Carry out one-way ANOVA","text":"assumptions normality equality variance met can confident one-way ANOVA appropriate test.","code":"\nanova(lm(weight ~ diet, data = lobsters))## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## diet       2 1567.2  783.61  1.6432 0.2263\n## Residuals 15 7153.1  476.87"},{"path":"cs2-anova.html","id":"result","chapter":"14 ANOVA","heading":"14.10.5 Result","text":"one-way ANOVA test indicated mean weight juvenile lobsters differ significantly diets (F = 1.64, df = 2,15, p = 0.23).","code":""},{"path":"cs2-anova.html","id":"post-hoc-testing-with-tukey","chapter":"14 ANOVA","heading":"14.10.6 Post-hoc testing with Tukey","text":"can see actually, significant difference pairs groups dataset.want reiterate carrying post-hoc test getting non-significant result ANOVA something think carefully depends research question .research question :diet affect lobster weight? effect diet lobster weight?got non-significant result ANOVA test just stopped answer. Going digging “significant” results running tests main factor contributes towards lack reproducibility research.hand research question :specific diets better worse lobster weight others?probably just skipped one-way ANOVA test entirely just jumped straight Tukey’s range test, important point result one-way ANOVA test doesn’t preclude carrying Tukey test.","code":"\nTukeyHSD(aov(weight ~ diet, data = lobsters))##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ diet, data = lobsters)\n## \n## $diet\n##                      diff        lwr      upr     p adj\n## Mussels-Flakes  21.895238  -9.661957 53.45243 0.2024851\n## Pellets-Flakes  14.046667 -20.300196 48.39353 0.5508657\n## Pellets-Mussels -7.848571 -41.061560 25.36442 0.8148766"},{"path":"cs2-anova.html","id":"key-points-2","chapter":"14 ANOVA","heading":"14.11 Key points","text":"use ANOVA test difference means multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform ANOVA linear model anova()check assumptions diagnostic plots check residuals normally distributedWe use post-hoc testing check significant differences group means, example using Tukey’s range test","code":""},{},{"path":"kruskal-wallis-test.html","id":"kruskal-wallis-test","chapter":"15 Kruskal-Wallis test","heading":"15 Kruskal-Wallis test","text":"","code":""},{"path":"kruskal-wallis-test.html","id":"objectives-5","chapter":"15 Kruskal-Wallis test","heading":"15.1 Objectives","text":"QuestionsHow analyse multiple samples continuous data data normally distributed?Kruskal-Wallis test?check differences groups?ObjectivesBe able perform Kruskal-Wallis test RUnderstand output test evaluate assumptionsBe able perform post-hoc testing Kruskal-Wallis test","code":""},{"path":"kruskal-wallis-test.html","id":"purpose-and-aim-3","chapter":"15 Kruskal-Wallis test","heading":"15.2 Purpose and aim","text":"Kruskal-Wallis one-way analysis variance test analogue ANOVA can used assumption normality met. way extension Mann-Whitney test two groups.","code":""},{"path":"kruskal-wallis-test.html","id":"section-commands-7","chapter":"15 Kruskal-Wallis test","heading":"15.3 Section commands","text":"New commands used section:","code":""},{"path":"kruskal-wallis-test.html","id":"data-and-hypotheses-6","chapter":"15 Kruskal-Wallis test","heading":"15.4 Data and hypotheses","text":"example, suppose behavioural ecologist records rate spider monkeys behaved aggressively towards one another function closely related two monkeys . familiarity two monkeys involved interaction classified high, low none. want test data support hypothesis aggression rates differ according strength relatedness. form following null alternative hypotheses:\\(H_0\\): median aggression rates types familiarity \\(H_1\\): median aggression rates equalWe use Kruskal-Wallis test check .data stored file data/raw/CS2-spidermonkey.csv.First read data :","code":"\nspidermonkey <- read.csv(\"data/raw/CS2-spidermonkey.csv\")"},{"path":"kruskal-wallis-test.html","id":"summarise-and-visualise-6","chapter":"15 Kruskal-Wallis test","heading":"15.5 Summarise and visualise","text":"data appear show significant difference aggression rates three types familiarity. probably expect reasonably significant result .","code":"\n# look at the data format\nhead(spidermonkey)##   aggression familiarity\n## 1        0.2        high\n## 2        0.1        high\n## 3        0.4        high\n## 4        0.8        high\n## 5        0.3        high\n## 6        0.5        high\n# summarise the data\naggregate(aggression ~ familiarity, data = spidermonkey, summary)##   familiarity aggression.Min. aggression.1st Qu. aggression.Median\n## 1        high       0.1000000          0.2000000         0.3000000\n## 2         low       0.3000000          0.4500000         0.5000000\n## 3        none       0.9000000          1.1500000         1.2000000\n##   aggression.Mean aggression.3rd Qu. aggression.Max.\n## 1       0.3571429          0.4500000       0.8000000\n## 2       0.6285714          0.7500000       1.2000000\n## 3       1.2571429          1.4000000       1.6000000\n# create boxplot\nboxplot(aggression ~ familiarity, data = spidermonkey)"},{"path":"kruskal-wallis-test.html","id":"implement-test-5","chapter":"15 Kruskal-Wallis test","heading":"15.6 Implement test","text":"Perform Kruskal-Wallis test data:first argument must formula format: variable ~ categoryIf data stored stacked format, second argument must name data frame","code":"\nkruskal.test(aggression ~ familiarity, data = spidermonkey)"},{"path":"kruskal-wallis-test.html","id":"interpret-output-and-report-results-5","chapter":"15 Kruskal-Wallis test","heading":"15.7 Interpret output and report results","text":"output now see console window:p-value given 3rd line. shows us probability getting samples null hypothesis actually true.Since p-value small (much smaller standard significance level 0.05) can say “unlikely three samples came parent distribution can reject null hypothesis” state :one-way Kruskal-Wallis rank sum test showed aggression rates spidermonkeys depends upon degree familiarity (KW = 13.597, df = 2, p = 0.0011).","code":"## \n##  Kruskal-Wallis rank sum test\n## \n## data:  aggression by familiarity\n## Kruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115"},{"path":"kruskal-wallis-test.html","id":"assumptions-7","chapter":"15 Kruskal-Wallis test","heading":"15.8 Assumptions","text":"use Kruskal-Wallis test make three assumptions:parent distributions samples drawn shape (’re normal use one-way ANOVA)data point samples independent othersThe parent distributions varianceIndependence ’ll ignore usual. Similar shape best assessed earlier visualisation data. means need check equality variance.Equality varianceWe test equality variance using Levene’s test (since can’t assume normal parent distributions rules Bartlett’s test).Levene’s test included default R packages may require installation additional package called car (Companion Applied Regression).install car package, run following command console:Alternatively, go Tools > Install packages… > Packages, type car press InstallRemember load library library(car).Perform Levene’s test data:relevant p-value given 3rd line (Pr(>F) = 0.893). quite large see group appear variance.also warning group coerced factor. need worry - Levene’s test needs compare different groups aggression encoded numeric value, converts categorical one running test.","code":"\ninstall.packages(\"car\")\nleveneTest(aggression ~ familiarity, data = spidermonkey)## Warning in leveneTest.default(y = y, group = group, ...): group coerced to\n## factor.## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  2  0.1139  0.893\n##       18"},{"path":"kruskal-wallis-test.html","id":"post-hoc-testing-1","chapter":"15 Kruskal-Wallis test","heading":"15.9 Post-hoc testing","text":"equivalent Tukey’s range test non-normal data Dunn’s test.\nDunn’s test also included default R packages may require installation additional package called dunn.test.install dunn.test package, run following command console:Alternatively, go Tools > Install packages… > Packages, type dunn.test press InstallRemember load library library(dunn.test).Test significant difference group medians:Note Dunn’s test requires us enter two arguments, first vector values second vector containing category labels (.e. factor).give following output:can see dunn.test() function also performs Kruskal-Wallis test data, results reported initially.comparison pairs groups reported table bottom. cell table two rows. bottom row contains p-values want. table shows isn’t significant difference high low groups, p-value (0.0799) high. two comparisons high familiarity familiarity groups low groups significant though.","code":"\ninstall.packages(\"dunn.test\")\ndunn.test(spidermonkey$aggression, spidermonkey$familiarity)##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 13.5972, df = 2, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                 (No adjustment)                                \n## Col Mean-|\n## Row Mean |       high        low\n## ---------+----------------------\n##      low |  -1.405820\n##          |     0.0799\n##          |\n##     none |  -3.655132  -2.249312\n##          |    0.0001*    0.0122*\n## \n## alpha = 0.05\n## Reject Ho if p <= alpha/2"},{"path":"kruskal-wallis-test.html","id":"exercise-8","chapter":"15 Kruskal-Wallis test","heading":"15.10 Exercise","text":"Exercise 15.1  Kruskal-Wallis Dunn’s test lobster dataPerform Kruskal-Wallis test post-hoc test lobster data set.1. Hypotheses\\(H_0\\) : medians equal\\(H_1\\) : medians equal2. Import data, summarise visualiseAll done previously.3. AssumptionsFrom , since data normal enough definitely similar enough Kruskal-Wallis test equality variance assessment diagnostic plots. completeness though look Levene’s testGiven p-value high, agrees previous assessment equality variance assumption well met. Rock .4. Kruskal-Wallis testA Kruskal-Wallis test indicated median weight juvenile lobsters differ significantly diets (KW = 3.26, df = 2, p = 0.20).5. Dunn’s testAlthough rather unneccessary, since detect significant differences diets, can perform non-parametric equivalent Tukey’s range test: Dunn’s test., ’ve used optional argument called altp dunn.test() call. default option reports p-values divided 2. assessment significance requires compare p-value 0.025 rather 0.05. Using argument altp = TRUE means Dunn’s test reports actual p-values.Either way, can see none comparisons significant ().","code":"\nleveneTest(weight ~ diet, data = lobsters)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  2  0.0028 0.9972\n##       15\nkruskal.test(weight ~ diet, data = lobsters)## \n##  Kruskal-Wallis rank sum test\n## \n## data:  weight by diet\n## Kruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\ndunn.test(lobsters$weight, lobsters$diet, altp = TRUE)##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.2\n## \n## \n##                            Comparison of x by group                            \n##                                 (No adjustment)                                \n## Col Mean-|\n## Row Mean |     Flakes    Mussels\n## ---------+----------------------\n##  Mussels |  -1.787664\n##          |     0.0738\n##          |\n##  Pellets |  -0.670245   1.005415\n##          |     0.5027     0.3147\n## \n## alpha = 0.05\n## Reject Ho if p <= alpha"},{"path":"kruskal-wallis-test.html","id":"key-points-3","chapter":"15 Kruskal-Wallis test","heading":"15.11 Key points","text":"use Kruskal-Wallis test see difference medians multiple continuous variablesIn R first define linear model lm(), using format response ~ predictorNext, perform Kruskal-Wallis test linear model kruskal.test()assume parent distributions shape; data point independent parent distributions varianceWe test equality variance using LeveneTest() car packagePost-hoc testing check significant differences group medians done dunn.test() dunn.test package","code":""},{},{"path":"cs3-intro.html","id":"cs3-intro","chapter":"16 Introduction","heading":"16 Introduction","text":"","code":""},{"path":"cs3-intro.html","id":"objectives-6","chapter":"16 Introduction","heading":"16.1 Objectives","text":"Aim: introduce R commands analysing simple linear modelsBy end practical participants able perform following statistical analyses:Simple Linear RegressionCorrelationFor , participants able :Perform test RInterpret outputCheck assumptions test","code":""},{"path":"cs3-intro.html","id":"background-2","chapter":"16 Introduction","heading":"16.2 Background","text":"practical focuses implementation various statistical tests relating simple linear regression correlation., focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction-2.html","id":"introduction-2","chapter":"17 Introduction","heading":"17 Introduction","text":"practical introducing can compare data different continuous variables.","code":""},{"path":"introduction-2.html","id":"cs3-datasets","chapter":"17 Introduction","heading":"17.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"correlation-coefficients.html","id":"correlation-coefficients","chapter":"18 Correlation coefficients","heading":"18 Correlation coefficients","text":"","code":""},{"path":"correlation-coefficients.html","id":"objectives-7","chapter":"18 Correlation coefficients","heading":"18.1 Objectives","text":"QuestionsWhat correlation coefficients?kind correlation coefficients use ?ObjectivesBe able calculate correlation coefficients RUse visual tools explore correlations variablesKnow limitations correlation coefficients","code":""},{"path":"correlation-coefficients.html","id":"purpose-and-aim-4","chapter":"18 Correlation coefficients","heading":"18.2 Purpose and aim","text":"Correlation refers relationship two variables (datasets) one another. Two datasets said correlated independent one another. Correlations can useful can indicate predictive relationship may exist. However just two datasets correlated mean causally related.","code":""},{"path":"correlation-coefficients.html","id":"section-commands-8","chapter":"18 Correlation coefficients","heading":"18.3 Section commands","text":"New commands used section:","code":""},{"path":"correlation-coefficients.html","id":"data-and-hypotheses-7","chapter":"18 Correlation coefficients","heading":"18.4 Data and hypotheses","text":"use USArrests dataset example. rather bleak dataset contains statistics arrests per 100,000 residents assault, murder robbery 50 US states 1973, alongside proportion population lived urban areas time. USArrests unstacked data frame 50 observations four variables: Murder, Assault, UrbanPop Robbery.data stored file data/raw/CS3-usarrests.csv.First read data:syntax reading data frame little different. want use first column .csv file specify names rows dataset rather include information inside dataset . using row.names = 1 argument tells R use 1st column file row names. need functions using require matrix input (basically data frame containing numbers).","code":"\nUSArrests <- read.csv(\"data/raw/CS3-usarrests.csv\", row.names = 1)\n\n# have a look at the data\nhead(USArrests)##            Murder Assault UrbanPop Robbery\n## Alabama      13.2     236       58    21.2\n## Alaska       10.0     263       48    44.5\n## Arizona       8.1     294       80    31.0\n## Arkansas      8.8     190       50    19.5\n## California    9.0     276       91    40.6\n## Colorado      7.9     204       78    38.7"},{"path":"correlation-coefficients.html","id":"pearsons-product-moment-correlation-coefficient","chapter":"18 Correlation coefficients","heading":"18.5 Pearson’s product moment correlation coefficient","text":"Pearson’s r (quantity also known) measure linear correlation two variables. value -1 +1, +1 means perfect positive correlation, -1 means perfect negative correlation 0 means correlation .","code":""},{"path":"correlation-coefficients.html","id":"summarise-and-visualise-7","chapter":"18 Correlation coefficients","heading":"18.6 Summarise and visualise","text":"Run command:first argument matrix data frameThe argument lower.panel tells R add redundant reflected lower set plots, diagonalFrom visual inspection scatter plots can see appears slight positive correlation pairs variables, although may weak case (Murder UrbanPop example).","code":"\npairs(USArrests, lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"implement-test-6","chapter":"18 Correlation coefficients","heading":"18.7 Implement test","text":"Let’s test possible correlations variables:first argument matrix data frameThe argument method tells R correlation coefficient use (pearson (default), kendall, spearman)","code":"\ncor(USArrests, method = \"pearson\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-6","chapter":"18 Correlation coefficients","heading":"18.8 Interpret output and report results","text":"give following output:matrix gives correlation coefficient pair variables data frame. matrix symmetric (?) diagonal values 1 (?). correlated variables Murder Assault r value 0.801. appears agree well set scatter plots produced earlier.","code":"##              Murder   Assault   UrbanPop   Robbery\n## Murder   1.00000000 0.8018733 0.06957262 0.5635788\n## Assault  0.80187331 1.0000000 0.25887170 0.6652412\n## UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n## Robbery  0.56357883 0.6652412 0.41134124 1.0000000"},{"path":"correlation-coefficients.html","id":"exercise-9","chapter":"18 Correlation coefficients","heading":"18.9 Exercise","text":"Exercise 18.1  State data correlationWe use data file data/raw/CS3-statedata.csv dataset exercise. rather benign dataset contains information general properties US state, population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (’s getting away ), percentage population high-school graduates, average number days minimum temperature freezing 1931 1960, state area square miles. dataset contains 50 rows 8 columns, column names: Population, Income, Illiteracy, Life.Exp, Murder, HS.Grad, Frost Area.Load data (remembering tell R first column CSV file used specify row names dataset) use pairs command visually identify 3 different pairs variables appear bethe positively correlatedthe negatively correlatednot correlated allCalculate Pearson’s r variable pairs see well able identify correlation visually.1. Read data2. Look pair-wise comparisons3. Create correlation matrixThe positively correlated variables Murder IlliteracyThe negatively correlated variables Murder LifeExpThe uncorrelated variables Area Population","code":"\nUSAstate <- read.csv(\"data/raw/CS3-statedata.csv\",\n                     row.names = 1)\n\n# have a look at the data\nhead(USAstate)##            Population Income Illiteracy LifeExp Murder HSGrad Frost   Area\n## Alabama          3615   3624        2.1   69.05   15.1   41.3    20  50708\n## Alaska            365   6315        1.5   69.31   11.3   66.7   152 566432\n## Arizona          2212   4530        1.8   70.55    7.8   58.1    15 113417\n## Arkansas         2110   3378        1.9   70.66   10.1   39.9    65  51945\n## California      21198   5114        1.1   71.71   10.3   62.6    20 156361\n## Colorado         2541   4884        0.7   72.06    6.8   63.9   166 103766\npairs(USAstate, lower.panel = NULL)\ncor(USAstate, method = \"pearson\")##             Population     Income  Illiteracy     LifeExp     Murder\n## Population  1.00000000  0.2082276  0.10762237 -0.06805195  0.3436428\n## Income      0.20822756  1.0000000 -0.43707519  0.34025534 -0.2300776\n## Illiteracy  0.10762237 -0.4370752  1.00000000 -0.58847793  0.7029752\n## LifeExp    -0.06805195  0.3402553 -0.58847793  1.00000000 -0.7808458\n## Murder      0.34364275 -0.2300776  0.70297520 -0.78084575  1.0000000\n## HSGrad     -0.09848975  0.6199323 -0.65718861  0.58221620 -0.4879710\n## Frost      -0.33215245  0.2262822 -0.67194697  0.26206801 -0.5388834\n## Area        0.02254384  0.3633154  0.07726113 -0.10733194  0.2283902\n##                 HSGrad      Frost        Area\n## Population -0.09848975 -0.3321525  0.02254384\n## Income      0.61993232  0.2262822  0.36331544\n## Illiteracy -0.65718861 -0.6719470  0.07726113\n## LifeExp     0.58221620  0.2620680 -0.10733194\n## Murder     -0.48797102 -0.5388834  0.22839021\n## HSGrad      1.00000000  0.3667797  0.33354187\n## Frost       0.36677970  1.0000000  0.05922910\n## Area        0.33354187  0.0592291  1.00000000"},{"path":"correlation-coefficients.html","id":"spearmans-rank-correlation-coefficient","chapter":"18 Correlation coefficients","heading":"18.10 Spearman’s rank correlation coefficient","text":"test first calculates rank numerical data (.e. position smallest (negative) largest (positive)) two variables calculates Pearson’s product moment correlation coefficient using ranks. consequence, test less sensitive outliers distribution.","code":""},{"path":"correlation-coefficients.html","id":"implement-test-7","chapter":"18 Correlation coefficients","heading":"18.11 Implement test","text":"using USArrests data set , run command:first argument matrix data frameThe argument method tells R correlation coefficient use","code":"\ncor(USArrests, method = \"spearman\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-7","chapter":"18 Correlation coefficients","heading":"18.12 Interpret output and report results","text":"gives following output:matrix gives correlation coefficient pair variables data frame. , matrix symmetric, diagonal values 1 expected. values obtained similar correlation coefficients obtained using Pearson test.","code":"##             Murder   Assault  UrbanPop   Robbery\n## Murder   1.0000000 0.8172735 0.1067163 0.6794265\n## Assault  0.8172735 1.0000000 0.2752133 0.7143681\n## UrbanPop 0.1067163 0.2752133 1.0000000 0.4381068\n## Robbery  0.6794265 0.7143681 0.4381068 1.0000000"},{"path":"correlation-coefficients.html","id":"exercise-10","chapter":"18 Correlation coefficients","heading":"18.13 Exercise","text":"Exercise 18.2  Spearman’s correlation USA state dataCalculate Spearman’s correlation coefficient data/raw/CS3-statedata.csv dataset.variable’s correlations affected use Spearman’s rank compared Pearson’s r?reference scatter plot produced earlier, can explain might ?Remember use row.names = 1 argument load data matrixInstead eye-balling differences, think can determine difference two correlation matricesThe heatmap() function can useful visualise matricesIn order determine variables affected choice Spearman vs Pearson just plot matrices side side try spot going , one reasons ’re using R can bit programmatic things.Let’s calculate difference two correlation matrices:, now just look grid 64 numbers see can spot biggest differences, eyes aren’t good processing parsing sort information display. better way somehow visualise data. can using R plotting functions, heatmap() exact. heatmap() function lot features don’t need ’m going go detail . main reason ’m using displays matrices right way round (plotting functions display matrices rotated 90 degrees) automatically labels rows columns.abs() function calculates absolute value (.e. just magnitude) matrix values. just care situations two correlation coefficients different don’t care larger. symm argument tells function symmetric matrix conjunction Rowv = NA argument stops plot reordering rows columns. Rowv = NA argument also stops function adding dendrograms margins plot.plot coloured yellow, indicating smallest values (case correspond difference correlation coefficients), orange dark red, indicating biggest values (case correspond variables biggest difference correlation coefficients).plot symmetric along leading diagonal (hopefully obvious reasons) can see majority squares light yellow colour, means isn’t much difference Spearman Pearson vast majority variables. squares appear darkest look along Area row/column suggesting ’s big difference correlation coefficients .can now revisit pairwise scatter plot see makes sense:can see clearly correspond plots noticeable outliers. example, Alaska twice big next biggest state, Texas. Big outliers data can large impact Pearson coefficient, whereas Spearman coefficient robust effects outliers. can see detail look Area vs Income graph coefficients. Pearson gives value 0.36, slight positive correlation, whereas Spearman gives value 0.057, basically uncorrelated. single outlier (Alaska) top-right scatter plot big effect Pearson practically ignored Spearman.Well done, Mr. Spearman.","code":"\ncor(USAstate, method = \"spearman\")##            Population      Income Illiteracy    LifeExp     Murder     HSGrad\n## Population  1.0000000  0.12460984  0.3130496 -0.1040171  0.3457401 -0.3833649\n## Income      0.1246098  1.00000000 -0.3145948  0.3241050 -0.2174623  0.5104809\n## Illiteracy  0.3130496 -0.31459482  1.0000000 -0.5553735  0.6723592 -0.6545396\n## LifeExp    -0.1040171  0.32410498 -0.5553735  1.0000000 -0.7802406  0.5239410\n## Murder      0.3457401 -0.21746230  0.6723592 -0.7802406  1.0000000 -0.4367330\n## HSGrad     -0.3833649  0.51048095 -0.6545396  0.5239410 -0.4367330  1.0000000\n## Frost      -0.4588526  0.19686382 -0.6831936  0.2983910 -0.5438432  0.3985351\n## Area       -0.1206723  0.05709484 -0.2503721  0.1275002  0.1064259  0.4389752\n##                 Frost        Area\n## Population -0.4588526 -0.12067227\n## Income      0.1968638  0.05709484\n## Illiteracy -0.6831936 -0.25037208\n## LifeExp     0.2983910  0.12750018\n## Murder     -0.5438432  0.10642590\n## HSGrad      0.3985351  0.43897520\n## Frost       1.0000000  0.11228778\n## Area        0.1122878  1.00000000\ncorPear <- cor(USAstate, method = \"pearson\")\ncorSpea <- cor(USAstate, method = \"spearman\")\ncorDiff <- corPear - corSpea\nheatmap(abs(corDiff), symm = TRUE, Rowv = NA)\npairs(USAstate)"},{"path":"correlation-coefficients.html","id":"key-points-4","chapter":"18 Correlation coefficients","heading":"18.14 Key points","text":"Correlation degree two variables linearly relatedCorrelation imply causationWe can visualise correlations using pairs() functionUsing cor() function can calculate correlation matricesTwo main correlation coefficients Pearson’s r Spearman’s rank, Spearman’s rank less sensitive outliers","code":""},{},{"path":"linear-regression.html","id":"linear-regression","chapter":"19 Linear regression","heading":"19 Linear regression","text":"","code":""},{"path":"linear-regression.html","id":"objectives-8","chapter":"19 Linear regression","heading":"19.1 Objectives","text":"QuestionsWhen use linear regression?interpret results?ObjectivesBe able perform linear regression RUse ANOVA check slope regression differs zeroUnderstand underlying assumptions linear regression analysisUse diagnostic plots check assumptions","code":""},{"path":"linear-regression.html","id":"purpose-and-aim-5","chapter":"19 Linear regression","heading":"19.2 Purpose and aim","text":"Regression analysis tests association two variables, also allows one investigate quantitatively nature relationship present, thus determine whether one variable may used predict values another.\nSimple linear regression essentially models dependence scalar dependent variable (y) independent (explanatory) variable (x) according relationship:\\[\\begin{equation*} \ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\\(\\beta_0\\) value intercept \\(\\beta_1\\) slope fitted line. aim simple linear regression analysis assess whether coefficient slope, \\(\\beta_1\\), actually different zero. different zero can say \\(x\\) significant effect \\(y\\) (since changing \\(x\\) leads predicted change \\(y\\)), whereas isn’t significantly different zero, say isn’t sufficient evidence relationship. course, order assess whether slope significantly different zero first need calculate values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"section-commands-9","chapter":"19 Linear regression","heading":"19.3 Section commands","text":"new commands used section.","code":""},{"path":"linear-regression.html","id":"data-and-hypotheses-8","chapter":"19 Linear regression","heading":"19.4 Data and hypotheses","text":"perform simple linear regression analysis two variables Murder Assault USArrests dataset. wish determine whether Assault variable significant predictor Murder variable. means need find coefficients \\(\\beta_0\\) \\(\\beta_1\\) best fit following macabre equation:\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 Assault\n\\end{equation*}\\]testing following null alternative hypotheses:\\(H_0\\): Assault significant predictor Murder, \\(\\beta_1 = 0\\)\\(H_1\\): Assault significant predictor Murder, \\(\\beta_1 \\neq 0\\)","code":""},{"path":"linear-regression.html","id":"summarise-and-visualise-8","chapter":"19 Linear regression","heading":"19.5 Summarise and visualise","text":"can visualise data :appears relatively strong positive relationship two variables whilst reasonable scatter points around trend line, probably expect significant result case.","code":"\nplot(Murder ~ Assault, data = USArrests)"},{"path":"linear-regression.html","id":"implement-test-8","chapter":"19 Linear regression","heading":"19.6 Implement test","text":"Fit straight line data:first argument lm formula saying Murder depends Assaults. seen , syntax generally dependent variable ~ independent variable.second argument specifies dataset useThe function lm returns linear model (lm) object essentially list containing everything necessary understand analyse linear model. However, just type (2nd line) just prints screen actual coefficients model .e. intercept slope line.found line best fit given :\\[\\begin{equation*}\nMurder = 0.63 + 0.042 Assault\n\\end{equation*}\\]Assess whether slope significantly different zero:, use anova() command assess significance. shouldn’t surprising stage introductory lectures made sense. mathematical perspective, one-way ANOVA simple linear regression exactly makes sense use command analyse R.","code":"\nlm_1 <- lm(Murder ~ Assault, data = USArrests)\n\n# show the linear model\nlm_1## \n## Call:\n## lm(formula = Murder ~ Assault, data = USArrests)\n## \n## Coefficients:\n## (Intercept)      Assault  \n##     0.63168      0.04191\nanova(lm_1)"},{"path":"linear-regression.html","id":"interpret-output-and-report-results-8","chapter":"19 Linear regression","heading":"19.7 Interpret output and report results","text":"exactly format table saw one-way ANOVA:1st line just tells ANOVA testThe 2nd line tells response variable (case Murder)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 1 48 (’ll need reporting)\nF value column contains F statistic, 86.454 (’ll need reporting).\np-value 2.596e-12 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) column used calculate F statistic don’t need know .\nDf column contains degrees freedom values row, 1 48 (’ll need reporting)F value column contains F statistic, 86.454 (’ll need reporting).p-value 2.596e-12 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) column used calculate F statistic don’t need know ., p-value ’re interested shows us probability getting data null hypothesis actually true slope line actually zero.\nSince p-value excruciatingly tiny can reject null hypothesis state :simple linear regression showed assault rate US states significant predictor number murders (F = 86.45, df = 1,48, p = 2.59x10-12).Plotting regression lineIt can helpful plot regression line original data see far data predicted linear values. can :first command creates scatter plot dataThe second command uses results linear model fitting (object lm_1) add line best fit plot (colour red).","code":"## Analysis of Variance Table\n## \n## Response: Murder\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Assault    1 597.70  597.70  86.454 2.596e-12 ***\n## Residuals 48 331.85    6.91                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nplot(Murder ~ Assault, data = USArrests)\n\n# add the regression line\nabline(lm_1, col = \"red\")"},{"path":"linear-regression.html","id":"assumptions-8","chapter":"19 Linear regression","heading":"19.8 Assumptions","text":"order linear regression analysis valid 4 key assumptions need met:data must linear (entirely possible calculate straight line data straight - doesn’t mean though!)residuals must normally distributedThe residuals must correlated fitted valuesThe fit depend overly much single point (point high leverage).Whether assumptions met can easily checked visually producing four key diagnostic plots.top left graph plots residuals fitted values. data best explained straight line uniform distribution points horizontal grey dotted line (sufficient points red line, moving average, top grey dotted line). plot pretty good.top right graph shows Q-Q plot allows visual inspection normality. residuals normally distributed, points lie diagonal dotted line. isn’t bad slight snaking towards upper end Georgia appears outlier .bottom left scale-location graph allows us investigate whether correlation residuals fitted values whether variance residuals changes significantly. , red line horizontal. correlation change variance red line horizontal. plot fine.last graph shows Cook’s distance tests one point unnecessarily large effect fit. important aspect see points lie beyond red dashed contour line top right corner plot. , point undue influence. plot good.Formally, concern looking diagnostic plots, linear regression valid. However, disappointingly, people ever check whether linear regression assumptions met quoting results.Let’s change leading example!","code":"\n# create a 2 x 2 output window\npar(mfrow = c(2,2))\n\n# and create the diagnostic plots for our model\nplot(lm_1)"},{"path":"linear-regression.html","id":"exercise-11","chapter":"19 Linear regression","heading":"19.9 Exercise","text":"Exercise 19.1  Linear regressionCalculate two simple linear regressions using data/raw/CS3-statedata.csv dataset, first variable LifeExp variable Murder variable HSGrad Frost.following cases:Find value slope intercept coefficients regressionsDetermine slope significantly different zero (.e. relationship two variables)Produce scatter plot data line best fit superimposed top.Produce diagnostic plots discuss (virtual) neighbour carried simple linear regression caseMurder Life ExpectancyLet’s see Murder variable can used predict Life.Exp variable. Let’s plot first ., ’ve fit linear model (second line) time plotting raw data (first line) just can add line best fit (third line). visualise reasons:check data aren’t obviously wrong. sensible values life expectancy (nothing massively large small), plausible values murder rates (’m au fait US murder rates 1976 small positive numbers seem plausible).check see expect statistical analysis. appear reasonable downward trend data. surprised didn’t get significant result given amount data spread data lineWe check assumptions (roughly though ’ll properly minute). Nothing immediately gives cause concern; data appear linear, spread data around line appears homogeneous symmetrical. outliers either.Now, let’s check assumptions diagnostic plots.residuals vs fitted plot appears symmetric enough (similar distribution points horizontal grey dotted line) happy linearity. Similarly red line scale-location plot looks horizontal enough happy homogeneity variance. aren’t influential points residuals vs leverage. plot give bit concern Normal Q-Q graph. see clear evidence snaking, although degree snaking isn’t actually bad. just means can pretty certain distribution residuals isn’t normal, also isn’t non-normal. situation? Well, three possible options:Appeal Central Limit Theorem. states large enough sample size don’t worry whether distribution residuals normally distributed. Large enough bit moving target honest depends non-normal underlying data . data little bit non-normal can get away using smaller sample data massively skewed (example). exact science, anything 30 data points considered lot mild moderate non-normality (case). data skewed looking data points (50-100). , example can legitimately just carry analysis without worrying.Try transforming data. try applying mathematical functions response variable (LifeExp) hope repeating analysis transformed variable make things better. honest might work won’t know try. Dealing transformed variables legitimate approach can make interpreting model bit challenging. particular example none traditional transformations (log, square-root, reciprocal) anything fix slight lack normality (can take word try ; plot(lm(log(LifeExp ~ Murder, data = USAstate))) example.Go permutation methods / bootstrapping. approach definitely work. don’t time explain (’s subject entire practical). approach also requires us reasonably large sample size work well assume distribution sample good approximation distribution entire dataset.case, large enough sample size deviation normality isn’t bad, can just crack standard analysis., let’s actually analysis:find Murder rate statistically significant predictor life expectancy US states. Woohoo!High School Graduation Frosty DaysNow let’s investigate relationship proportion High School Graduates state (HSGrad) mean number days freezing (Frost) within state., look data.doesn’t appear ridiculous errors data; High School graduation proportions 0-100% range mean number sub-zero days state 0 365, numbers plausible.Whilst trend upwards, wouldn’t surprise came back significant, ’m bit concerned …assumptions. ’m mainly concerned data aren’t linear. appears noticeable pattern data sort minimum around 50-60 Frost days. means ’s hard assess assumptions.Let’s check properlyNow, let’s check assumptions diagnostic plots.can see suspected backed residual vs fitted graph. data aren’t linear appears sort odd -pattern . Given lack linearity just isn’t worth worrying plots model misspecified: straight line just doesn’t represent data .Just reference, practice looking diagnostic plots, ignore lack linearity can say thatNormality pretty good Normal Q-Q plotHomogeneity variance isn’t good appears noticeable drop variance go left right (consideration Scale-Location plot)don’t appear influential points (looking residuals vs leverage graph)However, none relevant particular case since data aren’t linear straight line wrong model fit.situation?Well actually, bit tricky aren’t easy fixes . two broad solutions dealing misspecified model.common solution need predictor variables model. ’re trying explain/predict high school graduation using number frost days. Obviously many things affect proportion high school graduates just cold State (weird potential predictor think ) need statistical approach allows us look multiple predictor variables. ’ll cover approach next two sessions.potential solution say high school graduation can fact predicted number frost days relationship isn’t linear. need specify relationship (curve basically) try fit data new, non-linear, curve. process called, unsurprisingly, non-linear regression don’t cover course. process best used already strong theoretical reason non-linear relationship two variables (sigmoidal dose-response curves pharmacology exponential relationships cell growth). case don’t preconceived notions wouldn’t really appropriate case.Neither solutions can tackled knowledge far course can definitely say based upon data set, isn’t linear relationship (significant otherwise) frosty days high school graduation rates.","code":"\n# plot the data\nplot(LifeExp ~ Murder, data = USAstate)\n\n# create a linear model\nlm1 <- lm(LifeExp ~ Murder, data = USAstate)\n\n# and add a regression line\nabline(lm1, col = \"red\")\npar(mfrow = c(2,2))\nplot(lm1)\nanova(lm1)## Analysis of Variance Table\n## \n## Response: LifeExp\n##           Df Sum Sq Mean Sq F value   Pr(>F)    \n## Murder     1 53.838  53.838  74.989 2.26e-11 ***\n## Residuals 48 34.461   0.718                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nplot(HSGrad ~ Frost, data = USAstate)\n\n# create a linear model\nlm2<-lm(HSGrad ~ Frost, data=USAstate)\n\n# and add a regression line\nabline(lm2, col = \"red\")\npar(mfrow = c(2,2))\nplot(lm2)"},{"path":"linear-regression.html","id":"key-points-5","chapter":"19 Linear regression","heading":"19.10 Key points","text":"Linear regression tests linear relationship exists two variablesIf , can use one variable predict anotherA linear model intercept slope test slope differs zeroWe create linear models R lm() function use anova() assess slope coefficientWe can use linear regression four assumptions met:\ndata linear\nResiduals normally distributed\nResiduals correlated fitted values\nsingle point large influence linear model\ndata linearResiduals normally distributedResiduals correlated fitted valuesNo single point large influence linear modelWe use plot(model_name) get four diagnostic plots R, help evaluate assumptions","code":""},{},{"path":"cs4-intro.html","id":"cs4-intro","chapter":"20 Introduction","heading":"20 Introduction","text":"","code":""},{"path":"cs4-intro.html","id":"objectives-9","chapter":"20 Introduction","heading":"20.1 Objectives","text":"Aim: introduce R commands carrying two-way ANOVA linear regression grouped data/ANCOVABy end practical participants able achieve following:Carry two-way ANOVA using R interpret outputAnalyse linear regression grouped data (ANCOVA)","code":""},{"path":"cs4-intro.html","id":"background-3","chapter":"20 Introduction","heading":"20.2 Background","text":"practical focuses implementation various statistical tests relating multiple predictor variables R. focus underlying theory tests (although demonstrators happy answer questions may ).test section explaining perform test, section explaining results output screen, exercise complete relating test .","code":""},{},{"path":"introduction-3.html","id":"introduction-3","chapter":"21 Introduction","heading":"21 Introduction","text":"","code":""},{"path":"introduction-3.html","id":"cs4-datasets","chapter":"21 Introduction","heading":"21.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"two-way-anova.html","id":"two-way-anova","chapter":"22 Two-way ANOVA","heading":"22 Two-way ANOVA","text":"","code":""},{"path":"two-way-anova.html","id":"objectives-10","chapter":"22 Two-way ANOVA","heading":"22.1 Objectives","text":"QuestionsWhen use two-way ANOVA appropriate?perform R?ObjectivesBe able perform two-way ANOVA RUnderstand concept interaction two predictor variablesBe able plot interactions R","code":""},{"path":"two-way-anova.html","id":"purpose-and-aim-6","chapter":"22 Two-way ANOVA","heading":"22.2 Purpose and aim","text":"two-way analysis variance used two categorical predictor variables (factors) single continuous response variable. example, looking body Weight (continuous response variable kilograms) affected gender (categorical variable, Male Female) exercise type (categorical variable, Control Runner).analysing type data two things want know:either predictor variables effect response variable .e. gender affect body weight? runner affect body weight?interaction two predictor variables? interaction mean effect exercise weight depends whether male female rather independent gender. example male means runners weigh non-runners, female means runners weight less non-runners say interaction.first consider visualise data carrying appropriate statistical test.","code":""},{"path":"two-way-anova.html","id":"section-commands-10","chapter":"22 Two-way ANOVA","heading":"22.3 Section commands","text":"New commands used section:","code":""},{"path":"two-way-anova.html","id":"data-and-hypotheses-9","chapter":"22 Two-way ANOVA","heading":"22.4 Data and hypotheses","text":"recreate example analysis used lecture. data stored .csv file called CS4-exercise.csv.","code":""},{"path":"two-way-anova.html","id":"summarise-and-visualise-9","chapter":"22 Two-way ANOVA","heading":"22.5 Summarise and visualise","text":"Experiment dataframe three variables; Weight, Gender Exercise. Weight continuous response variable, whereas Gender Exercise categorical predictor variables.First,read data:visualise:produce basic box plots showing response variable (Weight) terms one predictor variables. values predictor variable case aren’t taken account. argument Weight ~ Gender (Weight ~ Exercise) key . tells R treat Weight function Gender (function Exercise .)\nalso basic plots just showing raw data using default arguments.(Optional) Add titles, axis labels information see fit plots make presentable.Visualise predictor variables together:produces box plots (four) combinations predictor variables. key argument Weight ~ Gender + Exercise. tells R treat Weight function Gender Exercise. + symbol mean add numbers together, Weight treated function Gender plus Exercise.\n, basic plot just shows raw data uses default arguments.(Optional) Add titles, axis labels information see fit plot make presentable.example four box plots relatively easy compare look interactions variables, two levels (groups) per categorical variable, become harder spot going . compare categorical variables easily just plot group means aids ability look interactions main effects predictor variable.Create interaction plot:first argument defines categorical variable used horizontal axis. must factor vector (comes data.frame automatically factor). function called x.factor.second argument defines categorical variable used different lines plotted. must factor vector. function called trace.factor.third argument defines response variable used vertical axis. must numerical vector. function argument called response.’s common get order arguments muddled . Remember ’s third argument defines variable goes vertical axis!default settings aren’t great displaying interaction plots. Try following (opinion) user-friendly display.choice categorical factor plotted horizontal axis plotted different lines completely arbitrary. Looking data ways shouldn’t add anything often ’ll find prefer one plot another.Plot interaction plot way round:now good feeling data already provide guesses following three questions:appear interaction two categorical variables?:\nExercise effect Weight?\nGender effect Weight?\nExercise effect Weight?Gender effect Weight?can now attempt answer three questions formally using ANOVA test. ask R explicitly test three things: interaction, effect Exercise effect Gender. use following code:Gender:Exercise term R represents concept interaction two variables.produces following output:row table different effects ’ve asked R consider. last column important one contains p-values (although also need F-values degrees freedom reporting purposes). need look interaction row first.Gender:Exercise p-value 0.028 (smaller 0.05) can conclude interaction Gender Exercise significant.must stop.top two lines (corresponding effects Gender Exercise) meaningless now p-values reported utterly redundant (particular way care p-values small).model significant interaction logically impossible meaningfully interpret main effects.report follows:two-way ANOVA test showed significant interaction effects Gender Exercise Weight (F = 5.8521, df = 1,16, p = 0.028). Exercise associated small loss weight males larger loss weight females.","code":"\nExperiment <- read.csv(\"data/raw/CS4-exercise.csv\")\nboxplot(Weight ~ Gender, data = Experiment)\nboxplot(Weight ~ Exercise, data = Experiment)\nboxplot(Weight ~ Gender + Exercise, data = Experiment)\ninteraction.plot(Experiment$Gender,\n                 Experiment$Exercise,\n                 Experiment$Weight)\ninteraction.plot(Experiment$Gender,\n                 Experiment$Exercise,\n                 Experiment$Weight,\n                 xlab = \"Gender\", ylab = \"Weight\",\n                 trace.label = \"Exercise\",\n                 type = \"b\", pch = 4,\n                 col = c(\"blue\", \"red\"))\ninteraction.plot(Experiment$Exercise,\n                 Experiment$Gender,\n                 Experiment$Weight,\n                 xlab = \"Gender\", ylab = \"Weight\",\n                 trace.label = \"Exercise\",\n                 type = \"b\", pch = 4,\n                 col = c(\"blue\", \"red\"))\n# define the linear model\nlm.exercise <- lm(Weight ~ Gender + Exercise + Gender:Exercise,\n                  data = Experiment)\n\n# perform the ANOVA\nanova(lm.exercise)## Analysis of Variance Table\n## \n## Response: Weight\n##                 Df Sum Sq Mean Sq F value    Pr(>F)    \n## Gender           1 607.20  607.20 43.1144 6.493e-06 ***\n## Exercise         1 184.83  184.83 13.1240  0.002287 ** \n## Gender:Exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals       16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"assumptions-9","chapter":"22 Two-way ANOVA","heading":"22.6 Assumptions","text":"two-way ANOVA type linear model need satisfy pretty much assumptions simple linear regression one-way ANOVA:data must systematic pattern itThe residuals must normally distributedThe residuals must homogeneity varianceThe fit depend overly much single point (point high leverage)., check assumptions visually producing four key diagnostic plots.first command changes plotting parameters splits graphics window 2 rows 2 columns (won’t notice anything whilst run ).first command changes plotting parameters splits graphics window 2 rows 2 columns (won’t notice anything whilst run ).second command produces 3 plots graphics window one warning stating Residuals vs Factor Levels plot left . groups exactly number data points.second command produces 3 plots graphics window one warning stating Residuals vs Factor Levels plot left . groups exactly number data points.top left graph plots residuals fitted values. systematic pattern plot pretty good.top left graph plots residuals fitted values. systematic pattern plot pretty good.top right graph allows visual inspection normality. , looks OK (perfect OK).top right graph allows visual inspection normality. , looks OK (perfect OK).bottom left graph allows us investigate whether homogeneity variance. plot fine (perfect fine).bottom left graph allows us investigate whether homogeneity variance. plot fine (perfect fine).shorthand way writing:Weight ~ Gender + Exercise + Gender:ExerciseIf use following syntax:Weight ~ Gender * ExerciseThen R interprets exactly way writing three terms.\ncan see compare output following two commands:","code":"\npar(mfrow = c(2, 2))\n\nplot(lm.exercise)## hat values (leverages) are all = 0.2\n##  and there are no factor predictors; no plot no. 5\nanova(lm(Weight ~ Gender + Exercise + Gender:Exercise,\n         data = Experiment))## Analysis of Variance Table\n## \n## Response: Weight\n##                 Df Sum Sq Mean Sq F value    Pr(>F)    \n## Gender           1 607.20  607.20 43.1144 6.493e-06 ***\n## Exercise         1 184.83  184.83 13.1240  0.002287 ** \n## Gender:Exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals       16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(lm(Weight ~ Gender * Exercise,\n         data = Experiment))## Analysis of Variance Table\n## \n## Response: Weight\n##                 Df Sum Sq Mean Sq F value    Pr(>F)    \n## Gender           1 607.20  607.20 43.1144 6.493e-06 ***\n## Exercise         1 184.83  184.83 13.1240  0.002287 ** \n## Gender:Exercise  1  82.42   82.42  5.8521  0.027839 *  \n## Residuals       16 225.34   14.08                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"two-way-anova.html","id":"exercise-cells","chapter":"22 Two-way ANOVA","heading":"22.7 Exercise: Cells","text":"Exercise 22.1  Cell growthThese data/examples/cs4-cells.csv data fictional experiment involves looking effect different concentrations substance growth rate two different cell types (annoyingly vague know – suggestions context welcome !). two cell types three concentrations.cell type control experiment substance added (.e. concentration none); low concentration substance high concentration substance. cells called B.\ncombination cell type substance concentration add substance individual cell petri dish 8 hours, count number cells dish (may well biologically weird/impossible – suggestions welcome). experiment repeated three times.cell type control experiment substance added (.e. concentration none); low concentration substance high concentration substance. cells called B.\ncombination cell type substance concentration add substance individual cell petri dish 8 hours, count number cells dish (may well biologically weird/impossible – suggestions welcome). experiment repeated three times.Let’s first visualise data:Let’s look interaction plots:’re constructed box plots ’ve also constructed two interaction plots. needed one interaction plot find can quite useful look data looks different angles. interaction plots suggest interaction lines plots aren’t parallel. Looking interaction plot concentration x-axis, appears non difference cell types concentration none, difference cell types concentration low high.Let’s carry two-way ANOVA:shows definitely significant interaction concentration cell_type.Let’s check assumptions:, actually look pretty good, although first glance might bit worried apparent heterogeneity variance. last group Residuals vs fitted graph appear spread 5 groups. echoed Scale-Location graph, red line kicks end. Whilst technically signify heterogeneity variance aren’t worried three data points per group. low number data points per group get one data point little bit extreme others (purely chance) large impact perception homogeneity variance. data points group certain observed heterogeneity variance true feature underlying parent population (therefore problem) rather just caused single random point (therefore problem).","code":"\n# read in the data\ncells <- read.csv(\"data/examples/cs4-cells.csv\")\n\n# let's have a peek at the data\nhead(cells)##   id cell_type concentration cell_number\n## 1  1         A          none           7\n## 2  2         A          none           9\n## 3  3         A          none           4\n## 4  4         B          none           5\n## 5  5         B          none           8\n## 6  6         B          none           9\nboxplot(cell_number ~ concentration,\n        data = cells)\nboxplot(cell_number ~ cell_type,\n        data = cells)\n# by cell type\ninteraction.plot(cells$concentration,\n                 cells$cell_type,\n                 cells$cell_number)\n# by concentration\ninteraction.plot(cells$cell_type,\n                 cells$concentration,\n                 cells$cell_number)\n# define the linear model, with interaction term\nlm1 <- lm(cell_number ~ concentration * cell_type,\n          data = cells)\n\n# perform the ANOVA\nanova(lm1)## Analysis of Variance Table\n## \n## Response: cell_number\n##                         Df  Sum Sq Mean Sq F value    Pr(>F)    \n## concentration            2 10932.1  5466.1 537.645 1.807e-12 ***\n## cell_type                1  1152.0  1152.0 113.311 1.816e-07 ***\n## concentration:cell_type  2  1158.3   579.2  56.967 7.485e-07 ***\n## Residuals               12   122.0    10.2                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npar(mfrow = c(2, 2))\n\nplot(lm1)## hat values (leverages) are all = 0.3333333\n##  and there are no factor predictors; no plot no. 5"},{"path":"two-way-anova.html","id":"exercise-tulips","chapter":"22 Two-way ANOVA","heading":"22.8 Exercise: Tulips","text":"Exercise 22.2  Blooms growing conditionsThe data/raw/CS4-tulip.csv dataset contains information experiment determine best conditions growing tulips (well someone care sorts things!). average number flower heads (blooms) recorded 27 different plots. plot experienced one three different watering regimes one three different shade regimes.Investigate number blooms affected different growing conditions.dataset three variables; Blooms (response variable) Water Shade (two potential predictor variables). always ’ll visualise data first:, interaction plots suggest might interaction . Digging little deeper descriptive perspective, looks though Water regime 1 behaving differently Water regimes 2 3 different shade conditions.Let’s carry two-way ANOVA check assumptions. ’s worth pointing order carry doesn’t really matter ’ll making decision everything place. Technically, check assumptions first statistical test, long check ’m fairly relaxed order steps.appear significant interaction Water Shade expected.Let’s check assumptions:actually OK. Point number 8 messing homogeneity variance assumption little bit, since ’s one point won’t worry . 2-way ANOVA analysis stands.","code":"\n# read in the data\ntulip <- read.csv(\"data/raw/CS4-tulip.csv\")\nboxplot(Blooms ~ Water, data = tulip)\nboxplot(Blooms ~ Shade, data = tulip)\ninteraction.plot(tulip$Water,\n                 tulip$Shade,\n                 tulip$Blooms)\ninteraction.plot(tulip$Shade,\n                 tulip$Water,\n                 tulip$Blooms)\n# define the linear model\nlm.tulip <- lm(Blooms ~ Water * Shade,\n               data = tulip)\n\n# perform the ANOVA\nanova(lm.tulip)## Analysis of Variance Table\n## \n## Response: Blooms\n##             Df Sum Sq Mean Sq F value    Pr(>F)    \n## Water        1 103426  103426  43.057 1.075e-06 ***\n## Shade        1  31154   31154  12.970  0.001505 ** \n## Water:Shade  1  33520   33520  13.954  0.001082 ** \n## Residuals   23  55248    2402                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npar(mfrow = c(2, 2))\nplot(lm.tulip)"},{"path":"two-way-anova.html","id":"key-points-6","chapter":"22 Two-way ANOVA","heading":"22.9 Key points","text":"two-way ANOVA used two categorical variables single continuous variableWe can visually check interactions categorical variables using interaction.plot()two-way ANOVA type linear model assumes following:\ndata systematic pattern\nresiduals normally distributed\nresiduals homogeneity variance\nfit depend single point (single point high leverage)\ndata systematic patternthe residuals normally distributedthe residuals homogeneity variancethe fit depend single point (single point high leverage)","code":""},{},{"path":"linear-regression-with-grouped-data.html","id":"linear-regression-with-grouped-data","chapter":"23 Linear regression with grouped data","heading":"23 Linear regression with grouped data","text":"","code":""},{"path":"linear-regression-with-grouped-data.html","id":"objectives-11","chapter":"23 Linear regression with grouped data","heading":"23.1 Objectives","text":"QuestionsHow perform linear regression grouped data?ObjectivesBe able perform linear regression grouped data RCalculate linear regression individual groups visualise dataUnderstand able create equations line best fitBe able deal interactions context","code":""},{"path":"linear-regression-with-grouped-data.html","id":"purpose-and-aim-7","chapter":"23 Linear regression with grouped data","heading":"23.2 Purpose and aim","text":"linear regression analysis grouped data used one categorical predictor variable (factor), one continuous predictor variable. response variable must still continuous however.example experiment looks light intensity woodland, light intensity (continuous: lux) affected height measurement taken, recorded depth measured top canopy (continuous: metres) type woodland (categorical: Conifer Broad leaf).analysing type data want know:difference groups?continuous predictor variable affect continuous response variable (canopy depth affect measured light intensity?)interaction two predictor variables? interaction display difference slopes lines best fit group, example perhaps conifer dataset significantly steeper line broad leaf woodland dataset.case, interaction means lines best fit slope.\nEssentially analysis identical two-way ANOVA (R doesn’t really notice difference).plot data visually inspect .test interaction doesn’t exist :\ncan test see either predictor variable effect (.e. lines best fit different intercepts? common gradient significantly different zero?)\ncan test see either predictor variable effect (.e. lines best fit different intercepts? common gradient significantly different zero?)first consider visualise data carrying appropriate statistical test.","code":""},{"path":"linear-regression-with-grouped-data.html","id":"section-commands-11","chapter":"23 Linear regression with grouped data","heading":"23.3 Section commands","text":"New commands used section:","code":""},{"path":"linear-regression-with-grouped-data.html","id":"data-and-hypotheses-10","chapter":"23 Linear regression with grouped data","heading":"23.4 Data and hypotheses","text":"data stored data/raw/CS4-treelight.csv.Read data inspect :treelight data frame three variables; Light, Depth Species. Light continuous response variable, Depth continuous predictor variable Species categorical predictor variables.","code":"\n# read in the data\ntreelight <- read.csv(\"data/raw/CS4-treelight.csv\")\n\n# inspect the data\nhead(treelight)##      Light Depth Species\n## 1 4105.646  1.00 Conifer\n## 2 4933.925  1.75 Conifer\n## 3 4416.527  2.50 Conifer\n## 4 4528.618  3.25 Conifer\n## 5 3442.610  4.00 Conifer\n## 6 4640.297  4.75 Conifer"},{"path":"linear-regression-with-grouped-data.html","id":"summarise-and-visualise-10","chapter":"23 Linear regression with grouped data","heading":"23.5 Summarise and visualise","text":"plots points data set window, unfortunately isn’t way easily distinguishing , least using base R. need extract correct subset data treelight dataframe.Create subsets data frame look raw data:subset function creates subsets data frames. first argument original data frame, subset argument logical expression defines observations (rows) extracted. logical expression must enclosed parentheses. first case says (Species == \"Conifer\"). tells R extract rows original data frame Conifer species variable column. Ditto Broadleaf.\ncan use smaller data frames distinguish points plot., need calculate linear regression group, add plot:Type :Now type create plot data linear regressions:now basic plot looks similar figure (different labels, legend different characters).(Optional) Modify graph make look figure.Looking plot, doesn’t appear significant interaction woodland type (Broadleaf Conifer) depth light measurements taken (Depth) amount light intensity getting canopy gradients two lines appear similar. appear noticeable slope lines lines look though different intercepts. suggests isn’t interaction Depth Species significant effect Light independently.","code":"\nplot(Light ~ Depth,\n     data = treelight)\n# subset the conifers\nconLight <- subset(treelight,\n                   subset = (Species == \"Conifer\"))\n\n# subset the broad leaf\nbroLight <- subset(treelight,\n                   subset = (Species == \"Broadleaf\"))\n\n# look at the first few rows of the raw data\nhead(conLight)##      Light Depth Species\n## 1 4105.646  1.00 Conifer\n## 2 4933.925  1.75 Conifer\n## 3 4416.527  2.50 Conifer\n## 4 4528.618  3.25 Conifer\n## 5 3442.610  4.00 Conifer\n## 6 4640.297  4.75 Conifer\nhead(broLight)##    Light Depth   Species\n## 14  7652 2.438 Broadleaf\n## 15  6866 3.488 Broadleaf\n## 16  5437 8.316 Broadleaf\n## 17  7327 1.597 Broadleaf\n## 18  5991 6.265 Broadleaf\n## 19  7574 2.651 Broadleaf\n# linear regression for Broadleaf\nlm.Broadleaf <- lm(Light ~ Depth,\n                   data = broLight)\n\n# linear regression for Conifer\nlm.Conifer <- lm(Light ~ Depth,\n                 data = conLight)\n# create a new plot based on Light ~ Depth\nplot(Light ~ Depth, \n     data = treelight, type = \"n\")\n\n# add the Broadleaf data\npoints(Light ~ Depth,\n     data = broLight, col = \"blue\")\n\n# add the Conifer data\npoints(Light ~ Depth,\n     data = conLight, col = \"red\")\n\n# add the Broadleaf linear regression\nabline(lm.Broadleaf, col = \"blue\")\n\n# add the Conifer linear regression\nabline(lm.Conifer, col = \"red\")"},{"path":"linear-regression-with-grouped-data.html","id":"implemention","chapter":"23 Linear regression with grouped data","heading":"23.6 Implemention","text":"can test formally:Remember Depth * Species shorthand way writing full set Depth + Species + Depth:Species terms R .e. main effects interaction effect.","code":"\nanova(lm(Light ~ Depth * Species,\n         data = treelight))"},{"path":"linear-regression-with-grouped-data.html","id":"interpret-output-and-present-results","chapter":"23 Linear regression with grouped data","heading":"23.7 Interpret output and present results","text":"gives following output:two-way ANOVA row table different effects ’ve asked R consider. last column important one contains p-values. need look interaction row first.\nDepth:Species p-value 0.393 (bigger 0.05) can conclude interaction Depth Species isn’t significant. can now consider whether predictor variables independently effect. Depth Species small p-values (2.86x10-9 4.13x10 -11) can conclude significant effect Light.means two lines best fit non-zero slope, different intercepts. now like know values .","code":"## Analysis of Variance Table\n## \n## Response: Light\n##               Df   Sum Sq  Mean Sq  F value    Pr(>F)    \n## Depth          1 30812910 30812910 107.8154 2.861e-09 ***\n## Species        1 51029543 51029543 178.5541 4.128e-11 ***\n## Depth:Species  1   218138   218138   0.7633    0.3932    \n## Residuals     19  5430069   285793                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-regression-with-grouped-data.html","id":"finding-intercept-values","chapter":"23 Linear regression with grouped data","heading":"23.7.1 Finding intercept values","text":"Unfortunately, R doesn’t make obvious easy us deciphering required getting right.\nsimple straight line linear regression conifer dataset , output relatively straightforward.can interpret meaning intercept line best fit 5014 coefficient depth variable (number front equation) -292.2., equation line best fit given :\\[\\begin{equation}\nLight = 5014 + -292.2 * Depth\n\\end{equation}\\]came fitting simple linear model using conifer dataset, meaning every extra 1 m depth forest canopy lose 292.2 lux light.looked full dataset, found interaction wasn’t important. means model two distinct intercepts single slope (’s get linear regression without interaction), need ask R calculate specific combination. command simply:Notice + symbol argument, opposed * symbol used earlier. means explicitly including interaction term fit, consequently forcing R calculate equation lines gradient.Ideally like R give us two equations, one forest type, four parameters total.\nUnfortunately, R parsimonious doesn’t . Instead R gives three coefficients, require bit interpretation.first two numbers R returns (underneath Intercept Depth) exact intercept slope coefficients one lines (case correspond data Broadleaf woodlands).coefficients belonging line, R uses first two coefficients baseline values expresses coefficients relative ones. R also doesn’t tell explicitly group using baseline reference group! (mention R can helpful times 😉?), decipher output?First, need work group used baseline.\n* group comes first alphabetically, Broadleaf\n* way check look see group mentioned table. Conifer mentioned (SpeciesConifer heading) baseline group Broadleaf.\nmeans intercept value Depth coefficient correspond Broadleaf group result know equation one lines :Broadleaf:\\[\\begin{equation}\nLight = 7962 + -262.2 * Depth\n\\end{equation}\\]example know gradient lines (explicitly asked R include interaction), need find intercept value Conifer group. Unfortunately, final value given underneath SpeciesConifer give intercept Conifer, instead tells difference Conifer group intercept baseline intercept .e. equation line best fit conifer woodland given :\\[\\begin{equation}\nLight = (7962 + -3113) + -262.2 * Depth\n\\end{equation}\\]\\[\\begin{equation}\nLight = 4829 + -262.2 * Depth\n\\end{equation}\\]","code":"\nlm(Light ~ Depth,\n   data = conLight)## \n## Call:\n## lm(formula = Light ~ Depth, data = conLight)\n## \n## Coefficients:\n## (Intercept)        Depth  \n##      5014.0       -292.2\nlm(Light ~ Depth + Species,\n   data = treelight)## \n## Call:\n## lm(formula = Light ~ Depth + Species, data = treelight)\n## \n## Coefficients:\n##    (Intercept)           Depth  SpeciesConifer  \n##         7962.0          -262.2         -3113.0"},{"path":"linear-regression-with-grouped-data.html","id":"adding-multiple-regression-lines","chapter":"23 Linear regression with grouped data","heading":"23.7.2 Adding multiple regression lines","text":"Unfortunately, base R doesn’t sensible way automatically adding multiple regression lines plot want , manually (easier ggplot added materials later).First, create underlying plot containing raw data values. previously, can just copy/paste code raw data.can add linear regression without interaction term.first need extract relative coefficient values lm object combine manually create separate vectors containing intercept slope coefficients line. next set command bit annoying stick ; ’ll pay dividends (, really – always secretly wanted computer programmer didn’t ? medic/biologist/life scientist thing just passing phase ’ll grow …)first line extracts three (case) coefficients lm object vector called cf, second line prints screen.third line take 1st 2nd components cf store coefficients Broadleaf line vector called cf.BroadleafThe fourth line actual calculations. realise intercept conifer line actually sum 1st 3rd values cf, whereas slope just 2nd value, create vector conifer line reflects .5th 6th lines just print two vectors screen.can now use two vectors add appropriate regression lines existing plot. final code looks like :","code":"\n# create a new plot based on Light ~ Depth\nplot(Light ~ Depth, \n     data = treelight, type = \"n\")\n\n# add the Broadleaf data\npoints(Light ~ Depth,\n     data = broLight, col = \"blue\")\n\n# add the Conifer data\npoints(Light ~ Depth,\n     data = conLight, col = \"red\")\nlm.add <- lm(Light ~ Depth + Species,\n             data = treelight)\ncf <- coef(lm.add)\ncf##    (Intercept)          Depth SpeciesConifer \n##      7962.0316      -262.1656     -3113.0265\ncf.Broadleaf <- c(cf[1], cf[2])\ncf.Conifer <- c(cf[1] + cf[3], cf[2])\n\ncf.Broadleaf## (Intercept)       Depth \n##   7962.0316   -262.1656\ncf.Conifer## (Intercept)       Depth \n##   4849.0051   -262.1656\n# create a new plot based on Light ~ Depth\nplot(Light ~ Depth, \n     data = treelight, type = \"n\")\n\n# add the Broadleaf data\npoints(Light ~ Depth,\n     data = broLight, col = \"blue\")\n\n# add the Conifer data\npoints(Light ~ Depth,\n     data = conLight, col = \"red\")\n\n# add the Broadleaf regression line\nabline(cf.Broadleaf, col = \"blue\")\n\n# add the Conifer regression line\nabline(cf.Conifer, col = \"red\")"},{"path":"linear-regression-with-grouped-data.html","id":"assumptions-10","chapter":"23 Linear regression with grouped data","heading":"23.8 Assumptions","text":"Hopefully, ’ve got gist checking assumptions linear models now: diagnostic plots!top left graph looks OK, systematic pattern.top right graph isn’t perfect, ’m happy normality assumption.bottom left graph OK, slight suggestion heterogeneity variance, nothing worried .bottom right graph shows points OKWoohoo!","code":"\npar(mfrow = c(2, 2))\nplot(lm(Light ~ Depth + Species,\n        data = treelight))"},{"path":"linear-regression-with-grouped-data.html","id":"dealing-with-interaction","chapter":"23 Linear regression with grouped data","heading":"23.9 Dealing with interaction","text":"significant interaction two predictor variables (example, light intensity dropped significantly faster conifer woods broad leaf woods, addition lower overall, looking two equations linear regression, time gradients vary well.\ncase interaction important need output linear regression explicitly includes interaction term:written using short-hand:really absolutely difference end result.\nEither way gives us following output:broadleaf line used baseline regression can read values intercept slope directly:Broadleaf:\n\\[\\begin{equation}\nLight = 7798.57 + -221.13 * Depth\n\\end{equation}\\]Note different previous section, allowing interaction fitted values change.conifer line different intercept value different gradient value. value underneath SpeciesConifer gives us difference intercept conifer line broad leaf line. new, additional term Depth:SpeciesConifer tells us coefficient Depth varies conifer line .e. gradient different. Putting two together gives us following equation line best fit conifer woodland:Conifer:\n\\[\\begin{equation}\nLight = (7798.57 + -2784.58) + (-221.13 + -71.04) * Depth\n\\end{equation}\\]\\[\\begin{equation}\nLight = 5014 + -292.2 * Depth\n\\end{equation}\\]also happen exactly lines best fit get calculating linear regression group’s data separately.","code":"\nlm(Light ~ Depth + Species + Depth:Species,\n   data = treelight)\nlm(Light ~ Depth * Species,\n   data = treelight)## \n## Call:\n## lm(formula = Light ~ Depth * Species, data = treelight)\n## \n## Coefficients:\n##          (Intercept)                 Depth        SpeciesConifer  \n##              7798.57               -221.13              -2784.58  \n## Depth:SpeciesConifer  \n##               -71.04"},{"path":"linear-regression-with-grouped-data.html","id":"exercise-clover-and-yarrow","chapter":"23 Linear regression with grouped data","heading":"23.10 Exercise: Clover and yarrow","text":"Exercise 23.1  Clover yarrow field trialsThe data/raw/CS4-clover.csv dataset contains information field trials three different farms (, B C). farm recorded yield clover ten fields along density yarrow stalks field.Investigate clover yield affected yarrow stalk density. evidence competition two species?difference farms?dataset three variables; Yield (response variable), Yarrow (continuous predictor variable) Farm (categorical predictor variables). always ’ll visualise data first:, ’ve used trick (admittedly probably told practical handout) can use categorical variable (Farm) specify colours used plot. trick works categorical variables 8 levels 8 colours hard-coded feature. colours : black, red, green, blue, cyan, magenta, yellow grey, order.plot means farm black, farm B red farm C green.method still rather clunky ggplot much better job respect. ’ll see works updated version.importantly, looking plot stands, ’s pretty clear yarrow density significant effect yield, ’s pretty hard see plot whether effect Farm, whether interaction. order work ’ll want add lines best fit Farm separately.lines best fit close together, looks much isn’t interaction, also isn’t effect Farm. Let’s carry analysis:confirms suspicions looking plot. isn’t interaction Yarrow Farm. Yarrow density statistically significant effect Yield isn’t difference different farms yields clover.Let’s check assumptions:borderline case.Normality fine (Normal QQ)aren’t highly influential points (Residuals vs Leverage)strong suggestion heterogeneity variance. points relatively close lines best fit, much great spread points low Yarrow density (corresponds high Yield values, fitted values correspond ).Finally, slight suggestion data might linear, might curve slightly.two options; arguably OK real life.can claim assumptions well enough met just report analysis ’ve just done.can decide analysis appropriate look options.\ncan try transform data taking logs Yield. might fix problems: taking logs response variable effect improving heterogeneity variance residual vs fitted plot spread right vs. left (like ). also appropriate think true relationship response predictor variables exponential rather linear (might ). capabilities try option.\ntry permutation based approach (beyond remit course, actually bit tricky situation). wouldn’t address non-linearity deal variance assumption.\ncome specific functional, mechanistic relationship yarrow density clover yield based upon aspects biology. example might threshold effect yarrow densities particular value, clover yields unaffected, soon yarrow values get threshold clover yield decreases (maybe even linearly). require much better understanding clover-yarrow dynamics (personally know little).\ncan try transform data taking logs Yield. might fix problems: taking logs response variable effect improving heterogeneity variance residual vs fitted plot spread right vs. left (like ). also appropriate think true relationship response predictor variables exponential rather linear (might ). capabilities try option.try permutation based approach (beyond remit course, actually bit tricky situation). wouldn’t address non-linearity deal variance assumption.come specific functional, mechanistic relationship yarrow density clover yield based upon aspects biology. example might threshold effect yarrow densities particular value, clover yields unaffected, soon yarrow values get threshold clover yield decreases (maybe even linearly). require much better understanding clover-yarrow dynamics (personally know little).Let’s quick little transformation data, repeat analysis see assumptions better met time (just hell ):, looks plausible. ’s noticeable outlier Farm B (red point bottom plot) otherwise see : probably isn’t interaction; likely effect Yarrow log(Yield); probably isn’t difference farms.Let’s analysis:Woop. good far. conclusions terms significant isn’t. Now just need check assumptions:Well, actually better set diagnostic plots. Whilst point number 12 clear outlier, ignore point rest plots look better.now know Yarrow significant predictor Yield ’re happy assumptions met.","code":"\nclover <- read.csv(\"data/raw/CS4-clover.csv\",\n                   stringsAsFactors = TRUE)\nplot(Yield ~ Yarrow,\n     data = clover, col = Farm)\nplot(Yield ~ Yarrow,\n     data = clover, col = Farm)\n\nfarmA <- subset(clover, subset = (Farm == \"A\"))\nfarmB <- subset(clover, subset = (Farm == \"B\"))\nfarmC <- subset(clover, subset = (Farm == \"C\"))\n\nlmA <- lm(Yield ~ Yarrow, data = farmA)\nlmB <- lm(Yield ~ Yarrow, data = farmB)\nlmC <- lm(Yield ~ Yarrow, data = farmC)\n\nabline(lmA, col = \"black\")\nabline(lmB, col = \"red\")\nabline(lmC, col = \"green\")\nlm.clover <- lm(Yield ~ Yarrow * Farm,\n                data = clover)\n\nanova(lm.clover)## Analysis of Variance Table\n## \n## Response: Yield\n##             Df Sum Sq Mean Sq F value    Pr(>F)    \n## Yarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\n## Farm         2    3.8     1.9  0.0063    0.9937    \n## Yarrow:Farm  2  374.7   187.4  0.6213    0.5457    \n## Residuals   24 7237.3   301.6                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npar(mfrow = c(2, 2))\nplot(lm.clover)\nplot(log(Yield) ~ Yarrow,\n     data = clover, col = Farm)\n\nlmlogA <- lm(log(Yield) ~ Yarrow, data = farmA)\nlmlogB <- lm(log(Yield) ~ Yarrow, data = farmB)\nlmlogC <- lm(log(Yield) ~ Yarrow, data = farmC)\n\nabline(lmlogA, col = \"black\")\nabline(lmlogB, col = \"red\")\nabline(lmlogC, col = \"green\")\nlm.log <- lm(log(Yield) ~ Yarrow * Farm,\n             data = clover)\nanova(lm.log)## Analysis of Variance Table\n## \n## Response: log(Yield)\n##             Df  Sum Sq Mean Sq F value   Pr(>F)    \n## Yarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\n## Farm         2  0.0862  0.0431  0.1103   0.8960    \n## Yarrow:Farm  2  0.8397  0.4199  1.0740   0.3575    \n## Residuals   24  9.3823  0.3909                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\npar(mfrow = c(2, 2))\nplot(lm.log)"},{"path":"linear-regression-with-grouped-data.html","id":"key-points-7","chapter":"23 Linear regression with grouped data","heading":"23.11 Key points","text":"linear regression analysis grouped data used one categorical one continuous predictor variable, together one continuous response variableWe can visualise data plotting line best fit together raw dataWhen performing ANOVA, need check interaction termsAgain, check underlying assumptions using diagnostic plotsWe can create equation line best fit group data using lm() output","code":""},{},{"path":"cs5-intro.html","id":"cs5-intro","chapter":"24 Introduction","heading":"24 Introduction","text":"","code":""},{"path":"cs5-intro.html","id":"objectives-12","chapter":"24 Introduction","heading":"24.1 Objectives","text":"Aim: introduce R commands constructing linear models multiple continuous categorical variables performing backwards stepwise eliminationBy end practical participants able achieve following:Construct linear model three continuous categorical variables\nUnderstand include exclude interaction terms\nUnderstand interpret output\nUnderstand include exclude interaction termsUnderstand interpret outputPerform backwards stepwise elimination produce minimal model","code":""},{"path":"cs5-intro.html","id":"background-4","chapter":"24 Introduction","heading":"24.2 Background","text":"practical divided two main sections. first section explores concept linear model framework revisits work previous practicals. linear model concept expanded systems three predictor variables.second section focuses model selection technique called backwards stepwise elimination. technique allows comparisons made nested models uninformative predictor variables can dropped minimal model remains.Within section worked example exercise.","code":""},{},{"path":"introduction-4.html","id":"introduction-4","chapter":"25 Introduction","heading":"25 Introduction","text":"","code":""},{"path":"introduction-4.html","id":"cs5-datasets","chapter":"25 Introduction","heading":"25.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"linear-models.html","id":"linear-models","chapter":"26 Linear models","heading":"26 Linear models","text":"","code":""},{"path":"linear-models.html","id":"objectives-13","chapter":"26 Linear models","heading":"26.1 Objectives","text":"QuestionsHow use linear model framework three predictor variables?ObjectivesBe able expand linear model framework R three predictor variablesDefine equation line best fit categorical variableBe able construct analyse possible combination predictor variables data","code":""},{"path":"linear-models.html","id":"purpose-and-aim-8","chapter":"26 Linear models","heading":"26.2 Purpose and aim","text":"Revisiting linear model framework expanding systems three predictor variables.","code":""},{"path":"linear-models.html","id":"section-commands-12","chapter":"26 Linear models","heading":"26.3 Section commands","text":"Commands used section","code":""},{"path":"linear-models.html","id":"data-and-hypotheses-11","chapter":"26 Linear models","heading":"26.4 Data and hypotheses","text":"first section uses following dataset:\ndata/raw/CS5-H2S.csv. dataset comprising 16 observations three variables (one dependent two predictor). records air pollution caused H2S produced two types waste treatment plants. types plant, obtain eight measurements H2S production (ppm). also obtain information daily temperature (C). data stacked.","code":""},{"path":"linear-models.html","id":"summarise-and-visualise-11","chapter":"26 Linear models","heading":"26.5 Summarise and visualise","text":"Let’s first load data:Next, visualise data:first line plots H2S values temperature values airpoll dataset regardless plant . However neglects actually put points screen (argument type=n prevents actual plotting data). command used simply create plotting region correct size extent later.second third lines create subsets data frame airpoll. second line extracts observations subset airpoll Plant variable equal (via subset = (Plant == \"\") argument).fourth fifth lines add subsetted points graph window using different colours characters data points plant can distinguished.looks though variable Plant effect H2S emissions (one cloud points higher ). also suggestion temperature might affect emissions (data sets look like gradient line best fit respective cloud might zero) also appears might interaction Plant Temperature (gradient two lines best fit don’t look like ’ll similar).","code":"\nairpoll <- read.csv(\"data/raw/CS5-H2S.csv\")\n# create a plot canvas without data points\nplot(H2S ~ Temp,\n     data = airpoll, type = \"n\")\n\n# subset plant A\nPlantA <- subset(airpoll, subset = (Plant == \"A\"))\nPlantB <- subset(airpoll, subset = (Plant == \"B\"))\npoints(H2S ~ Temp, PlantA, col = \"red\")\npoints(H2S ~ Temp, PlantB, col = \"blue\", pch=2)"},{"path":"linear-models.html","id":"implemention-1","chapter":"26 Linear models","heading":"26.6 Implemention","text":"Construct analyse full linear model.first line creates linear model seeks explain H2S values terms categorical Plant variable, continuous Temp variable interaction two variables. linear model object given name lm.fullThe first argument H2S ~ Plant + Temp + Plant:Temp formula summarises model fit. H2S dependent variable asking R use Plant, Temp Plant:Temp interaction terms predictor variables.second line produces following output:gives us coefficients model:best interpreted using linear model notation:\\[\\begin{equation}\nH_2S = 6.20495 - 0.05448 \\cdot Temp + \\binom{0}{-2.73075}\\binom{PlantA}{PlantB} + \\binom{0}{0.18141 \\cdot Temp}\\binom{PlantA}{PlantB}\n\\end{equation}\\]effectively shorthand writing equation two straight lines (one categorical variable):\\[\\begin{equation}\nPlantA = 6.20495 - 0.05448 \\cdot Temp\n\\end{equation}\\]\\[\\begin{equation}\nPlantB = 3.4742 + 0.12693 \\cdot Temp\n\\end{equation}\\]anova(lm.full) produces following output:can see interaction term appears marginally significant, implying effect temperature H2S different two different plants.Finally, plotting lm.full model shows us diagnostic plots:","code":"\n# define the linear model with all terms and interactions\nlm.full <- lm(H2S ~ Plant + Temp + Plant:Temp,\n              data=airpoll)\n\n# view the model\nlm.full\n\n# perform an ANOVA\nanova(lm.full)\n\n# plot the model\npar(mfrow=c(2,2))\nplot(lm.full)## \n## Call:\n## lm(formula = H2S ~ Plant + Temp + Plant:Temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)       PlantB         Temp  PlantB:Temp  \n##     6.20495     -2.73075     -0.05448      0.18141## # A tibble: 4 × 2\n##   term        estimate\n##   <chr>          <dbl>\n## 1 (Intercept)   6.20  \n## 2 PlantB       -2.73  \n## 3 Temp         -0.0545\n## 4 PlantB:Temp   0.181## Analysis of Variance Table\n## \n## Response: H2S\n##            Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Plant       1 13.3225 13.3225 54.1557 8.746e-06 ***\n## Temp        1  0.2316  0.2316  0.9415   0.35104    \n## Plant:Temp  1  1.4470  1.4470  5.8822   0.03201 *  \n## Residuals  12  2.9520  0.2460                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-models.html","id":"exploring-models","chapter":"26 Linear models","heading":"26.7 Exploring models","text":"Rather stop however, use concept linear model full potential show can construct analyse possible combination predictor variables dataset. Namely consider following four extra models:","code":""},{"path":"linear-models.html","id":"additive-model","chapter":"26 Linear models","heading":"26.7.1 Additive model","text":"Construct analyse additive linear model.first line creates linear model seeks explain H2S values purely terms categorical Plant variable continuous Temp variable.second line produces following output:gives us coefficients additive model:best interpreted using linear model notation:\\[\\begin{equation}\nH_S = 3.9 + 0.036 \\cdot Temp + \\binom{0}{1.8} \\binom{PlantA}{PlantB}\n\\end{equation}\\]effectively shorthand writing equation two straight lines (one categorical variable):\\[\\begin{equation}\nH_2S_{PlantA} = 3.9 + 0.036 \\cdot Temp\n\\end{equation}\\]\\[\\begin{equation}\nH_2S_{PlantB} = 5.7 + 0.036 \\cdot Temp\n\\end{equation}\\]important note much coefficients changed (natural assume change model given ’ve altered predictor variables included). striking signs coefficients changed! example, full model saw coefficient PlantB negative (implying general PlantB produced lower H2S values PlantA default) whereas now positive indicating exactly opposite effect. Given difference two models inclusion interaction term saw significant analysis full model, perhaps, surprising dropping term lead different results.just imagine never included first place! looked additive model come completely different conclusions baseline pollution levels plant.3rd line produces following output:can see temperature term significant, whereas Plant term significant indeed.Exercise 26.1  Check assumptions additive model. differ significantly full model?","code":"\n# define the linear model\nlm.add <- lm(H2S ~ Plant + Temp,\n             data = airpoll)\n\n# view the linear model\nlm.add\n\n# perform an ANOVA on the model\nanova(lm.add)## \n## Call:\n## lm(formula = H2S ~ Plant + Temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)       PlantB         Temp  \n##     3.90164      1.83861      0.03629## # A tibble: 3 × 2\n##   term        estimate\n##   <chr>          <dbl>\n## 1 (Intercept)   3.90  \n## 2 PlantB        1.84  \n## 3 Temp          0.0363## Analysis of Variance Table\n## \n## Response: H2S\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Plant      1 13.3225 13.3225 39.3702 2.858e-05 ***\n## Temp       1  0.2316  0.2316  0.6845     0.423    \n## Residuals 13  4.3991  0.3384                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-models.html","id":"revisiting-anova","chapter":"26 Linear models","heading":"26.7.2 Revisiting ANOVA","text":"Construct analyse effect Plant:first line creates box plot:second line fits linear model dataThe third line gives us model coefficients:case tells us means groups. (Intercept) mean PlantA H2S data (4.8225) whilst PlantB tells us mean Plant B H2S data 1.8250 intercept value .e. mean PlantB 4.8225 + 1.8250 = 6.6475.fourth line gives us normal ANOVA table testing whether means two groups differ significantly .Exercise 26.2  Check assumptions plant model. differ significantly previous models?","code":"\n# visualise the data\nboxplot(H2S ~ Plant,\n        data = airpoll)\n# define the linear model\nlm.Plant <- lm(H2S ~ Plant,\n               data = airpoll)\n\n# view the linear model\nlm.Plant\n\n# perform an ANOVA on the model\nanova(lm.Plant)## \n## Call:\n## lm(formula = H2S ~ Plant, data = airpoll)\n## \n## Coefficients:\n## (Intercept)       PlantB  \n##       4.823        1.825## Analysis of Variance Table\n## \n## Response: H2S\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Plant      1 13.3225 13.3225  40.278 1.809e-05 ***\n## Residuals 14  4.6307  0.3308                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-models.html","id":"revisiting-regression","chapter":"26 Linear models","heading":"26.7.3 Revisiting regression","text":"Construct simple linear regression model:first line fits linear model dataThe second line creates scatter plotThe third line uses results linear model fitting (lm.Temp) add line best fit scatter plot.fourth line gives us coefficients equation line best fitIn case tells us intercept (Intercept) gradient (Temp) line best fit.fifth line gives us ANOVA analysis:Temperature clearly significant effect.Exercise 26.3  , check assumptions temperature model. differ significantly previous models?","code":"\n# define the linear model\nlm.Temp <- lm(H2S ~ Temp,\n              data=airpoll)\n\n# create a scatter plot of the data\nplot(H2S ~ Temp, data=airpoll)\n# and add a line of best fit\nabline(lm.Temp)\n# view the model\nlm.Temp\n\n# perform an ANOVA on the model\nanova(lm.Temp)## \n## Call:\n## lm(formula = H2S ~ Temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)         Temp  \n##     5.21465      0.02066## Analysis of Variance Table\n## \n## Response: H2S\n##           Df  Sum Sq Mean Sq F value Pr(>F)\n## Temp       1  0.0753  0.0753   0.059 0.8117\n## Residuals 14 17.8779  1.2770"},{"path":"linear-models.html","id":"the-null-model","chapter":"26 Linear models","heading":"26.7.4 The null model","text":"Construct analyse null model:first line fits null model data (effectively just finding mean H2S values dataset)second line creates box plotThe third line gives us mean H2S values (.e. coefficient null model)null model rarely analysed sake instead used reference point sophisticated model selection techniques.","code":"\n# define the null model\nlm.null <- lm(H2S ~ 1, data = airpoll)\n\n# visualise the data\nboxplot(airpoll$H2S)\n# view the model\nlm.null## \n## Call:\n## lm(formula = H2S ~ 1, data = airpoll)\n## \n## Coefficients:\n## (Intercept)  \n##       5.735"},{"path":"linear-models.html","id":"exercise-trees","chapter":"26 Linear models","heading":"26.8 Exercise: trees","text":"Exercise 26.4  Trees: example continuous variablesUse internal dataset trees. data frame 31 observations 3 continuous variables. variables height Height, diameter Girth timber volume Volume 31 felled black cherry trees.Investigate relationship Volume (dependent variable) Height Girth (predictor variables).variables continuous isn’t way producing 2D plot three variables visualisation purposes using R’s standard plotting functions.construct four linear models\nAssume volume depends Height, Girth interaction Girth Height\nAssume Volume depends Height Girth isn’t interaction .\nAssume Volume depends Girth (plot result, regression line).\nAssume Volume depends Height (plot result, regression line).\nAssume volume depends Height, Girth interaction Girth HeightAssume Volume depends Height Girth isn’t interaction .Assume Volume depends Girth (plot result, regression line).Assume Volume depends Height (plot result, regression line).linear model write algebraic equation linear model produces relates volume two continuous predictor variables.Check assumptions model. concerns?NB: two continuous predictors, interaction term simply two values multiplied together (Girth:Height means Girth x Height)Use equations calculate predicted volume tree diameter 20 inches height 67 feet case.Let’s construct four linear models turn.r commands :gives us following output:can use output get following equation:Volume = 69.40 + -1.30\\(\\cdot\\)Height + -5.86\\(\\cdot\\)Girth + 0.13\\(\\cdot\\)Height\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) 67 + -5.86 \\(\\cdot\\) 20 + 0.13 \\(\\cdot\\) 67 \\(\\cdot\\) 20Volume = 45.81Here note interaction term just requires us multiple three numbers together (haven’t looked continuous predictors examples exercise included check see whole process making sense).look diagnostic plots model using following commands get:assumptions OK.suggestion heterogeneity variance (variance lower small large fitted (.e. predicted Volume) values), can attributed small number data points edges, ’m overly concerned.Similarly, suggestion snaking Normal Q-Q plot (suggesting lack normality) mainly due inclusion point 18 overall plot looks acceptable.highly influential pointsThe r commands :gives us following output:can use output get following equation:Volume = -57.99 + 0.34\\(\\cdot\\)Height + 4.71\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) 67 + 4.71 \\(\\cdot\\) 20Volume = 58.91If look diagnostic plots model using following commands get following:model isn’t great.worrying lack linearity exhibited Residuals vs Fitted plot suggesting linear model isn’t appropriate.Assumptions Normality seem OKEquality variance harder interpret. Given lack linearity data isn’t really sensible interpret Scale-Location plot stands (since plot generated assuming ’ve fitted straight line data), sake practising interpretation ’ll go. definitely suggestions homogeneity variance cluster points fitted values around 20 noticeably lower variance rest dataset.Point 31 influential weren’t issues linearity model remove point repeat analysis. stands isn’t much point.r commands :gives us following output:can use output get following equation:Volume = -87.12 + 1.54\\(\\cdot\\)HeightIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -87.12 + 1.54 \\(\\cdot\\) 67Volume = 16.28If look diagnostic plots model using following commands get following:model also isn’t great.main issue clear heterogeneity variance. trees bigger volumes data much spread trees smaller volumes (can seen clearly Scale-Location plot).Apart , assumption Normality seems OKAnd aren’t hugely influential points modelThe r commands :gives us following output:can use output get following equation:Volume = -36.94 + 5.07\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) 20Volume = 64.37If look diagnostic plots model using following commands get following:diagnostic plots look rather similar ones generated additive model issue lack linearity, heterogeneity variance point 31 influential.","code":"\n# define the model\nlm.full <- lm(Volume ~ Height * Girth,\n              data = trees)\n\n# view the model\nlm.full## \n## Call:\n## lm(formula = Volume ~ Height * Girth, data = trees)\n## \n## Coefficients:\n##  (Intercept)        Height         Girth  Height:Girth  \n##      69.3963       -1.2971       -5.8558        0.1347\npar(mfrow = c(2, 2))\n\nplot(lm.full)\n# define the model\nlm.add <- lm(Volume ~ Height + Girth,\n             data = trees)\n\n# view the model\nlm.add## \n## Call:\n## lm(formula = Volume ~ Height + Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height        Girth  \n##    -57.9877       0.3393       4.7082\npar(mfrow = c(2, 2))\n\nplot(lm.add)\n# define the model\nlm.height <- lm(Volume ~ Height,\n              data = trees)\n\n# view the model\nlm.height## \n## Call:\n## lm(formula = Volume ~ Height, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height  \n##     -87.124        1.543\npar(mfrow = c(2, 2))\n\nplot(lm.height)\n# define the model\nlm.girth <- lm(Volume ~ Girth,\n               data = trees)\n\n# view the model\nlm.girth## \n## Call:\n## lm(formula = Volume ~ Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)        Girth  \n##     -36.943        5.066\npar(mfrow = c(2, 2))\n\nplot(lm.girth)"},{"path":"linear-models.html","id":"full-model","chapter":"26 Linear models","heading":"26.8.1 Full model","text":"r commands :gives us following output:can use output get following equation:Volume = 69.40 + -1.30\\(\\cdot\\)Height + -5.86\\(\\cdot\\)Girth + 0.13\\(\\cdot\\)Height\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = 69.40 + -1.30 \\(\\cdot\\) 67 + -5.86 \\(\\cdot\\) 20 + 0.13 \\(\\cdot\\) 67 \\(\\cdot\\) 20Volume = 45.81Here note interaction term just requires us multiple three numbers together (haven’t looked continuous predictors examples exercise included check see whole process making sense).look diagnostic plots model using following commands get:assumptions OK.suggestion heterogeneity variance (variance lower small large fitted (.e. predicted Volume) values), can attributed small number data points edges, ’m overly concerned.Similarly, suggestion snaking Normal Q-Q plot (suggesting lack normality) mainly due inclusion point 18 overall plot looks acceptable.highly influential points","code":"\n# define the model\nlm.full <- lm(Volume ~ Height * Girth,\n              data = trees)\n\n# view the model\nlm.full## \n## Call:\n## lm(formula = Volume ~ Height * Girth, data = trees)\n## \n## Coefficients:\n##  (Intercept)        Height         Girth  Height:Girth  \n##      69.3963       -1.2971       -5.8558        0.1347\npar(mfrow = c(2, 2))\n\nplot(lm.full)"},{"path":"linear-models.html","id":"additive-model-1","chapter":"26 Linear models","heading":"26.8.2 Additive model","text":"r commands :gives us following output:can use output get following equation:Volume = -57.99 + 0.34\\(\\cdot\\)Height + 4.71\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -57.99 + 0.34 \\(\\cdot\\) 67 + 4.71 \\(\\cdot\\) 20Volume = 58.91If look diagnostic plots model using following commands get following:model isn’t great.worrying lack linearity exhibited Residuals vs Fitted plot suggesting linear model isn’t appropriate.Assumptions Normality seem OKEquality variance harder interpret. Given lack linearity data isn’t really sensible interpret Scale-Location plot stands (since plot generated assuming ’ve fitted straight line data), sake practising interpretation ’ll go. definitely suggestions homogeneity variance cluster points fitted values around 20 noticeably lower variance rest dataset.Point 31 influential weren’t issues linearity model remove point repeat analysis. stands isn’t much point.","code":"\n# define the model\nlm.add <- lm(Volume ~ Height + Girth,\n             data = trees)\n\n# view the model\nlm.add## \n## Call:\n## lm(formula = Volume ~ Height + Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height        Girth  \n##    -57.9877       0.3393       4.7082\npar(mfrow = c(2, 2))\n\nplot(lm.add)"},{"path":"linear-models.html","id":"height-only-model","chapter":"26 Linear models","heading":"26.8.3 Height-only model","text":"r commands :gives us following output:can use output get following equation:Volume = -87.12 + 1.54\\(\\cdot\\)HeightIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -87.12 + 1.54 \\(\\cdot\\) 67Volume = 16.28If look diagnostic plots model using following commands get following:model also isn’t great.main issue clear heterogeneity variance. trees bigger volumes data much spread trees smaller volumes (can seen clearly Scale-Location plot).Apart , assumption Normality seems OKAnd aren’t hugely influential points model","code":"\n# define the model\nlm.height <- lm(Volume ~ Height,\n              data = trees)\n\n# view the model\nlm.height## \n## Call:\n## lm(formula = Volume ~ Height, data = trees)\n## \n## Coefficients:\n## (Intercept)       Height  \n##     -87.124        1.543\npar(mfrow = c(2, 2))\n\nplot(lm.height)"},{"path":"linear-models.html","id":"girth-only-model","chapter":"26 Linear models","heading":"26.8.4 Girth-only model","text":"r commands :gives us following output:can use output get following equation:Volume = -36.94 + 5.07\\(\\cdot\\)GirthIf stick numbers (Girth = 20 Height = 67) get following equation:Volume = -36.94 + 5.07 \\(\\cdot\\) 20Volume = 64.37If look diagnostic plots model using following commands get following:diagnostic plots look rather similar ones generated additive model issue lack linearity, heterogeneity variance point 31 influential.","code":"\n# define the model\nlm.girth <- lm(Volume ~ Girth,\n               data = trees)\n\n# view the model\nlm.girth## \n## Call:\n## lm(formula = Volume ~ Girth, data = trees)\n## \n## Coefficients:\n## (Intercept)        Girth  \n##     -36.943        5.066\npar(mfrow = c(2, 2))\n\nplot(lm.girth)"},{"path":"linear-models.html","id":"key-points-8","chapter":"26 Linear models","heading":"26.9 Key points","text":"Point 1Point 2Point 3","code":""},{},{"path":"model-comparisons.html","id":"model-comparisons","chapter":"27 Model comparisons","heading":"27 Model comparisons","text":"","code":""},{"path":"model-comparisons.html","id":"objectives-14","chapter":"27 Model comparisons","heading":"27.1 Objectives","text":"QuestionsHow compare linear models?decide one “best” model?ObjectivesBe able compare models using Akaike Information Criterion (AIC)Use AIC context Backwards Stepwise Elimination R","code":""},{"path":"model-comparisons.html","id":"purpose-and-aim-9","chapter":"27 Model comparisons","heading":"27.2 Purpose and aim","text":"previous example used single dataset fitted five linear models depending predictor variables used. Whilst fun (seriously, else right now?) seems “better way.” Well, thankfully ! fact several methods can used compare different models order help identify “best” model. specifically, can determine full model (uses available predictor variables interactions) necessary appropriately describe dependent variable, whether can throw away terms (e.g. interaction term) don’t offer useful predictive power.use Akaike Information Criterion order compare different models.","code":""},{"path":"model-comparisons.html","id":"section-commands-13","chapter":"27 Model comparisons","heading":"27.3 Section commands","text":"New commands section:","code":""},{"path":"model-comparisons.html","id":"data-and-hypotheses-12","chapter":"27 Model comparisons","heading":"27.4 Data and hypotheses","text":"section uses data/raw/CS5-Ladybird.csv data set. data set comprises 20 observations three variables (one dependent two predictor). records clutch size (Eggs) species ladybird alongside two potential predictor variables; mass female (Weight), colour male (Male) categorical variable.","code":""},{"path":"model-comparisons.html","id":"backwards-stepwise-elimination","chapter":"27 Model comparisons","heading":"27.5 Backwards Stepwise Elimination","text":"First, load data store object called ladybird. visualise data.","code":"\nladybird <- read.csv(\"data/raw/CS5-Ladybird.csv\")\n# create a plot canvas\nplot(Eggs ~ Weight,\n     data = ladybird, type = \"n\")\n\n# subset the wild-type ladybirds\nWild <- subset(ladybird,\n               subset = (Male == \"Wild\"))\n\n# subset the melanic ladybirds\nMelanic <- subset(ladybird,\n                  subset = (Male == \"Melanic\"))\n\n# add the data for the wild-type ladybirds\npoints(Eggs ~ Weight,\n       data = Wild, col = \"red\")\n\n# and add the data for the melanic ladybirds\npoints(Eggs ~ Weight,\n       data = Melanic, col = \"blue\", pch = 2)"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-1","chapter":"27 Model comparisons","heading":"27.5.1 Comparing models with AIC (step 1)","text":"First, construct full linear model:Now construct reduced model (.e. next simplest model) doesn’t interactions:compare two models simply use command extractAIC() model.line first number tells many parameters model second number tells AIC score model. can see full model 4 parameters (intercept, coefficient continuous variable Weight, coefficient categorical variable Male coefficient interaction term Weight:Male) AIC score 41.3 (1dp). reduced model lower AIC score 40.4 (1dp) 3 parameters (since ’ve dropped interaction term). different ways interpreting AIC scores widely used interpretation says :difference two AIC scores greater 2 model smallest AIC score supported model higher AIC scoreif difference two models’ AIC scores less 2 models equally well supportedThis choice language (supported vs significant) deliberate areas statistics AIC scores used differently way going use (ask want bit philosophical ramble ). However, situation use AIC scores decide whether reduced model least good full model. since difference AIC scores less 2, can say dropping interaction term left us model simpler (fewer terms) least good (AIC score) full model. reduced model Eggs ~ Weight + Male designated current working minimal model.","code":"\n# define the full model\nlm.full <- lm(Eggs ~ Weight + Male + Weight:Male,\n              data = ladybird)\n\n# view the model summary\nsummary(lm.full)\n# define the model\nlm.red <- lm(Eggs ~ Weight + Male,\n             data = ladybird)\n\n# view the model summary\nsummary(lm.red)\nextractAIC(lm.full)## [1]  4.00000 41.28452\nextractAIC(lm.red)## [1]  3.00000 40.43819"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-2","chapter":"27 Model comparisons","heading":"27.5.2 Comparing models with AIC (step 2)","text":"Next, see remaining terms can dropped. look models dropped Male Weight (.e. Eggs ~ Weight Eggs ~ Male) compare AIC values AIC current minimal model (Eggs ~ Weight + Male). AIC values least one new reduced models lower (least 2 greater) AIC current minimal model, can drop relevant term get new minimal model. find situation can drop one term drop term gives us model lowest AIC.Drop variable Weight examine AIC:Drop variable Male examine AIC:Considering outputs together comparing AIC current minimal model (40.4) can see dropping Male decreased AIC 38.8, whereas dropping Weight actually increased AIC 60.0 thus worsened model quality.Hence can drop Male new minimal model Eggs ~ Weight.","code":"\n# define the model\nlm.male <- lm(Eggs ~ Male,\n              data = ladybird)\n\n# extract the AIC\nextractAIC(lm.male)## [1]  2.00000 59.95172\n# define the model\nlm.weight <- lm(Eggs ~ Weight,\n                data=ladybird)\n\n# extract the AIC\nextractAIC(lm.weight)## [1]  2.00000 38.76847"},{"path":"model-comparisons.html","id":"comparing-models-with-aic-step-3","chapter":"27 Model comparisons","heading":"27.5.3 Comparing models with AIC (step 3)","text":"final comparison drop variable Weight compare simple model null model (Eggs ~ 1), assumes brood size constant across parameters.Drop variable Weight see effect:AIC null model quite bit larger current minimal model Eggs ~ Weight conclude Weight important. minimal model Eggs ~ Weight., summary, conclude :Female size useful predictor clutch size, male type important.stage can analyse minimal linear (lm.weight) model using anova() function, consider diagnostic plots using plot(lm.weight) command.","code":"\n# define the model\nlm.null <- lm(Eggs ~ 1,\n              data = ladybird)\n\n# extract the AIC\nextractAIC(lm.null)## [1]  1.00000 58.46029"},{"path":"model-comparisons.html","id":"notes-on-backwards-stepwise-elimination","chapter":"27 Model comparisons","heading":"27.6 Notes on Backwards Stepwise Elimination","text":"method finding minimal model starting full model removing variables called backward stepwise elimination. Although regularly practised data analysis, increasing criticism approach, calls avoided entirely.made work procedure ? Given prevalence academic papers, useful aware procedures know issues . situations, using AIC model comparisons justified come across regularly. Additionally, may situations feel good reasons drop parameter model – using technique can justify doesn’t affect model fit. Taken together, using backwards stepwise elimination model comparison still useful technique.Performing backwards stepwise elimination manually can quite tedious. Thankfully R acknowledges single inbuilt function called step() can perform necessary steps using AIC.perform full backwards stepwise elimination process find minimal model . output familiar ask demonstrator questions.Yes, told earlier, ’s fun ? (also useful understand steps behind technique suppose…)","code":"## Start:  AIC=41.28\n## Eggs ~ Weight + Male + Weight:Male\n## \n##               Df Sum of Sq    RSS    AIC\n## - Weight:Male  1    6.2724 111.90 40.438\n## <none>                     105.63 41.285\n## \n## Step:  AIC=40.44\n## Eggs ~ Weight + Male\n## \n##          Df Sum of Sq    RSS    AIC\n## - Male    1     1.863 113.77 38.768\n## <none>                111.90 40.438\n## - Weight  1   216.196 328.10 59.952\n## \n## Step:  AIC=38.77\n## Eggs ~ Weight\n## \n##          Df Sum of Sq    RSS    AIC\n## <none>                113.77 38.768\n## - Weight  1    222.78 336.55 58.460## \n## Call:\n## lm(formula = Eggs ~ Weight, data = ladybird)\n## \n## Coefficients:\n## (Intercept)       Weight  \n##       4.320        1.873"},{"path":"model-comparisons.html","id":"exercise-bse","chapter":"27 Model comparisons","heading":"27.7 Exercise: BSE","text":"Exercise 27.1  BSE trees airpollUse internal dataset trees airpoll dataset earlier.Perform backwards stepwise elimination datasets discover minimal model using AIC.NB: interaction term significant main factor part interaction term dropped model.’re feeling attempt backwards stepwise elimination process internal CO2 dataset. data frame 1 dependent variable (uptake) 4 predictor variables (Plant, Type, Treatment, conc). Unfortunately, dataset contain enough data construct full linear model using 4 predictor variables (interactions), ignore Plant variable take uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc +  Treatment:conc + Type:Treatment:conc full model.relatively straightforward using step() function.need first construct full linear model simply pass linear model object step function R rest.construct full linear model Height, Girth interaction run step() function:BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.construct full linear model Plant, Temp interaction run step function:, BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.time manage three steps. first successful manage drop three-way interaction Type:Treatment:conc. next step end dropping Treatment:conc interaction. final step realise can’t drop terms ’re done. minimal model 5 terms coefficients model given bottom output.","code":"\n# define the full model\nlm.trees <- lm(Volume ~ Girth * Height,\n               data = trees)\n\n# perform BSE\nstep(lm.trees)## Start:  AIC=65.49\n## Volume ~ Girth * Height\n## \n##                Df Sum of Sq    RSS    AIC\n## <none>                      198.08 65.495\n## - Girth:Height  1    223.84 421.92 86.936## \n## Call:\n## lm(formula = Volume ~ Girth * Height, data = trees)\n## \n## Coefficients:\n##  (Intercept)         Girth        Height  Girth:Height  \n##      69.3963       -5.8558       -1.2971        0.1347\n# define the full model\nlm.airpoll <- lm(H2S ~ Plant * Temp,\n                 data = airpoll)\n\n# perform BSE\nstep(lm.airpoll)## Start:  AIC=-19.04\n## H2S ~ Plant * Temp\n## \n##              Df Sum of Sq    RSS     AIC\n## <none>                    2.9520 -19.041\n## - Plant:Temp  1     1.447 4.3991 -14.659## \n## Call:\n## lm(formula = H2S ~ Plant * Temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)       PlantB         Temp  PlantB:Temp  \n##     6.20495     -2.73075     -0.05448      0.18141\n# define the model, ignore the Plant variable\nlm.co2 <- lm(uptake ~ Type + Treatment + conc\n             + Type:Treatment + Type:conc + Treatment:conc\n             + Type:Treatment:conc,\n             data = CO2)\nstep(lm.co2)## Start:  AIC=302.6\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc + Type:Treatment:conc\n## \n##                       Df Sum of Sq    RSS    AIC\n## - Type:Treatment:conc  1    55.535 2602.7 302.41\n## <none>                             2547.2 302.60\n## \n## Step:  AIC=302.41\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## - Treatment:conc  1    31.871 2634.6 301.44\n## <none>                        2602.7 302.41\n## - Type:conc       1   207.998 2810.7 306.87\n## - Type:Treatment  1   225.730 2828.5 307.40\n## \n## Step:  AIC=301.44\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## <none>                        2634.6 301.44\n## - Type:conc       1    208.00 2842.6 305.82\n## - Type:Treatment  1    225.73 2860.3 306.34## \n## Call:\n## lm(formula = uptake ~ Type + Treatment + conc + Type:Treatment + \n##     Type:conc, data = CO2)\n## \n## Coefficients:\n##                      (Intercept)                   TypeMississippi  \n##                         25.29351                          -4.72692  \n##                 Treatmentchilled                              conc  \n##                         -3.58095                           0.02308  \n## TypeMississippi:Treatmentchilled              TypeMississippi:conc  \n##                         -6.55714                          -0.01070"},{"path":"model-comparisons.html","id":"trees-dataset","chapter":"27 Model comparisons","heading":"27.7.1 trees dataset","text":"construct full linear model Height, Girth interaction run step() function:BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.","code":"\n# define the full model\nlm.trees <- lm(Volume ~ Girth * Height,\n               data = trees)\n\n# perform BSE\nstep(lm.trees)## Start:  AIC=65.49\n## Volume ~ Girth * Height\n## \n##                Df Sum of Sq    RSS    AIC\n## <none>                      198.08 65.495\n## - Girth:Height  1    223.84 421.92 86.936## \n## Call:\n## lm(formula = Volume ~ Girth * Height, data = trees)\n## \n## Coefficients:\n##  (Intercept)         Girth        Height  Girth:Height  \n##      69.3963       -5.8558       -1.2971        0.1347"},{"path":"model-comparisons.html","id":"airpoll-dataset","chapter":"27 Model comparisons","heading":"27.7.2 airpoll dataset","text":"construct full linear model Plant, Temp interaction run step function:, BSE approach gets far first step (trying drop interaction term). see immediately dropping interaction term makes model worse process stops. next line (underneath Call:) see best model still full model get see coefficients term.","code":"\n# define the full model\nlm.airpoll <- lm(H2S ~ Plant * Temp,\n                 data = airpoll)\n\n# perform BSE\nstep(lm.airpoll)## Start:  AIC=-19.04\n## H2S ~ Plant * Temp\n## \n##              Df Sum of Sq    RSS     AIC\n## <none>                    2.9520 -19.041\n## - Plant:Temp  1     1.447 4.3991 -14.659## \n## Call:\n## lm(formula = H2S ~ Plant * Temp, data = airpoll)\n## \n## Coefficients:\n## (Intercept)       PlantB         Temp  PlantB:Temp  \n##     6.20495     -2.73075     -0.05448      0.18141"},{"path":"model-comparisons.html","id":"co2-dataset","chapter":"27 Model comparisons","heading":"27.7.3 CO2 dataset","text":"time manage three steps. first successful manage drop three-way interaction Type:Treatment:conc. next step end dropping Treatment:conc interaction. final step realise can’t drop terms ’re done. minimal model 5 terms coefficients model given bottom output.","code":"\n# define the model, ignore the Plant variable\nlm.co2 <- lm(uptake ~ Type + Treatment + conc\n             + Type:Treatment + Type:conc + Treatment:conc\n             + Type:Treatment:conc,\n             data = CO2)\nstep(lm.co2)## Start:  AIC=302.6\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc + Type:Treatment:conc\n## \n##                       Df Sum of Sq    RSS    AIC\n## - Type:Treatment:conc  1    55.535 2602.7 302.41\n## <none>                             2547.2 302.60\n## \n## Step:  AIC=302.41\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc + \n##     Treatment:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## - Treatment:conc  1    31.871 2634.6 301.44\n## <none>                        2602.7 302.41\n## - Type:conc       1   207.998 2810.7 306.87\n## - Type:Treatment  1   225.730 2828.5 307.40\n## \n## Step:  AIC=301.44\n## uptake ~ Type + Treatment + conc + Type:Treatment + Type:conc\n## \n##                  Df Sum of Sq    RSS    AIC\n## <none>                        2634.6 301.44\n## - Type:conc       1    208.00 2842.6 305.82\n## - Type:Treatment  1    225.73 2860.3 306.34## \n## Call:\n## lm(formula = uptake ~ Type + Treatment + conc + Type:Treatment + \n##     Type:conc, data = CO2)\n## \n## Coefficients:\n##                      (Intercept)                   TypeMississippi  \n##                         25.29351                          -4.72692  \n##                 Treatmentchilled                              conc  \n##                         -3.58095                           0.02308  \n## TypeMississippi:Treatmentchilled              TypeMississippi:conc  \n##                         -6.55714                          -0.01070"},{"path":"model-comparisons.html","id":"key-points-9","chapter":"27 Model comparisons","heading":"27.8 Key points","text":"can use Backwards Stepwise Elimination (BSE) full model see certain terms add predictive power model notThe AIC allows us compare different models - difference AIC 2 two models, smallest AIC score supportedWe can use step() function let R perform automatic BSE","code":""},{},{"path":"cs6-intro.html","id":"cs6-intro","chapter":"28 Introduction","heading":"28 Introduction","text":"","code":""},{"path":"cs6-intro.html","id":"objectives-15","chapter":"28 Introduction","heading":"28.1 Objectives","text":"introduce R commands conducting power analyses practising systematic statistical analysisBy end practical participants able :apply priori power analysis techniques t-tests linear models order determine appropriate sample sizes given effect size, power significance levelssystematically analyse produce readable statistical reports previously unseen simple datasets containing single continuous, binary proportion response variable multiple categorical continuous predictor variables","code":""},{"path":"cs6-intro.html","id":"background-5","chapter":"28 Introduction","heading":"28.2 Background","text":"practical consists two separate sections:first looks power analysis t-tests linear models.second section consists single worked example plus series example datasets, can analysed written using R techniques explored previous practicals. can considered opportunity review techniques ’ve looked course. Details R commands given instead expected use previous materials need look various commands. intentional aims reflect use course resources future!","code":""},{},{"path":"introduction-5.html","id":"introduction-5","chapter":"29 Introduction","heading":"29 Introduction","text":"","code":""},{"path":"introduction-5.html","id":"cs6-datasets","chapter":"29 Introduction","heading":"29.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"power-analysis.html","id":"power-analysis","chapter":"30 Power analysis","heading":"30 Power analysis","text":"","code":""},{"path":"power-analysis.html","id":"objectives-16","chapter":"30 Power analysis","heading":"30.1 Objectives","text":"QuestionsWhat power analysis?can use power analysis design better experiments?ObjectivesBe able perform power analysis RUnderstand importance effect sizeUse power, significance level effect size optimise experimental design","code":""},{"path":"power-analysis.html","id":"background-6","chapter":"30 Power analysis","heading":"30.2 Background","text":"hypothesis tests can wrong two ways:can appear found significant result really isn’t anything : false positive (Type error), orwe can fail spot significant result really something interesting going : false negative (Type II error).probability getting false positive analysis precisely significance level use analysis. , order reduce likelihood getting false positive simply reduce significance level test (0.05 0.01 say). Easy .Unfortunately, unintended consequences (doesn’t everything?). turns reducing significance level means increase chance getting false negatives. make sense; ’re increasing barrier entry terms acceptance ’ll also accidentally miss good stuff.Power capacity test detect significant different results. affected three things:effect size: .e. big difference want able detect, alternatively consider meaningful effect/difference ?sample sizethe significance levelIn ideal world want carrying highly powerful tests using low significance levels, reduce chance getting false positive maximise chances finding true effect.Power analysis allows us design experiments just . Given:desired power (0.8 80% considered pretty good)significance level (0.05 5% trusty yet arbitrary steed )effect size like detectwe can calculate amount data need collect experiments. (Woohoo! looks statistics actually give us answer last rather perpetual shades--grey “maybes”).reality easily usable power analysis functions operate assumption data collect meet assumptions chosen statistical test perfectly. , example, want design experiment investigating effectiveness single drug compared placebo (simple t-test) want know many patients group order test work, standard power analysis techniques still assume data end collecting meet assumptions t-test carry (Sorry raised hopes ever slightly 😉).","code":""},{"path":"power-analysis.html","id":"effect-size","chapter":"30 Power analysis","heading":"30.2.1 Effect size","text":"shall see commands carrying power analyses simple implement apart concept effect size. tricky issue people get grips two reasons:Effect size related biological significance rather statistical significanceThe way specify effect sizesWith respect first point common conversation goes bit like :: “’ve told carry power analysis, eh? Lucky . sort effect size looking ?”: “idea ’re talking . want know drug better placebo. many patients need?”: “depends big difference think drug compared placebo.”: “haven’t carried experiment yet, absolutely idea big effect !”: (honest relatively well-informed conversation: much closer things actually go)key point effect sizes power analyses need specify effect size interested observing, one biologically relevant see. may well actually 0.1% difference effectiveness drug placebo designing experiment detect require markedly individuals experiment trying detect 50% difference effectiveness. reality three places can get sense effect sizes :pilot studyPrevious literature theoryJacob CohenJacob Cohen American statistician developed large set measures effect sizes (use today). came rough set numerical measures “small,” “medium” “large” effect sizes still use today. come caveats though; Jacob psychologist assessment large effect may somewhat different . form useful starting point however.lot different ways specifying effects sizes, can split three distinct families estimates:Correlation estimates: use R2 measure variance explained model (linear models, anova etc. large R2 value indicate lot variance explained model expect see lot difference groups, tight cluster points around line best fit. argument goes need fewer data points observe relationship confidence. Trying find relationship low R2 value trickier therefore require data points equivalent power.Difference means: look far apart means two groups , measured units standard deviations (t-tests). effect size 2 case interpreted two groups means two standard deviations away (quite big difference), whereas effect size 0.2 harder detect require data pick .Difference count data: freely admit idea intuitively explain (shock, horror). Mathematically based chi-squared statistic ’s good can tell ’m afraid. , however, pretty easy calculate.reference Cohen’s suggested values effect sizes different tests. ’ll probably surprised small .look carry power analyses estimate effect sizes section.","code":""},{"path":"power-analysis.html","id":"packages","chapter":"30 Power analysis","heading":"30.3 Packages","text":"using pwr powerAnalysis packages section. Please install now.can running following code console:Next, load running:packages lot overlap unfortunately neither one quite functionality ’d like. powerAnalysis functions explicitly calculating effect sizes previous studies smaller range power calculation functions. pwr power functions fewer effect size functions. However, together job.","code":"\ninstall.packages(c(\"pwr\", \"powerAnalysis\"))\nlibrary(pwr)\nlibrary(powerAnalysis)"},{"path":"power-analysis.html","id":"section-commands-14","chapter":"30 Power analysis","heading":"30.4 Section commands","text":"New commands used section:","code":""},{"path":"power-analysis.html","id":"t-tests","chapter":"30 Power analysis","heading":"30.5 t-tests","text":"Let’s assume want design experiment determine whether difference mean price male female students pay cafe. many male female students need observe order detect “medium” effect size 80% power significance level 0.05?first need think test use analyse data. two groups continuous response. Clearly t-test.","code":""},{"path":"power-analysis.html","id":"get-the-effect-size","chapter":"30 Power analysis","heading":"30.5.1 Get the effect size","text":"Now need work “medium” effect size . absence information appeal Cohen’s conventional values:function just returns default conventional values effect sizes determined Jacob Cohen back day. just saves us scrolling back page look table provided. takes two arguments:test one \n“t,” t-tests,\n“anova” anova,\n“f2” linear models\n“chisq” chi-squared test\n“t,” t-tests,“anova” anova,“f2” linear models“chisq” chi-squared testsize, just one “small,” “medium” “large.”bit want bottom line; apparently want effect size 0.5.\nsort study effect size measured terms Cohen’s d statistic. simply measure different means two groups expressed terms number standard deviations apart . , case ’re looking detect two means 0.5 standard deviations away . minute ’ll look means real data.","code":"\ncohen.ES(test = \"t\", size = \"medium\")## \n##      Conventional effect size from Cohen (1982) \n## \n##            test = t\n##            size = medium\n##     effect.size = 0.5"},{"path":"power-analysis.html","id":"carry-out-a-power-analysis","chapter":"30 Power analysis","heading":"30.5.2 Carry out a power analysis","text":"follows:first line ’re looking n = 63.76 tells need 64 (rounding ) students group (128 total) order carry study sufficient power. lines self-explanatory (well stage; need tell function just returning values ’ve just typed bigger problems worry ).pwr.t.test() function six arguments. Two specify sort t-test ’ll carrying \n* type; describes type t-test eventually carrying (one two.sample, one.sample paired), \n* alternative; describes type alternative hypothesis want test (one two.sided, less greater)four arguments used power analysis:d; effect size, single number calculated using Cohen’s d statistic.sig.level; significance levelpower; powern; number observations per sample.function works allowing specify three four arguments function works fourth. example used test standard fashion specifying power, significance desired effect size getting function tell us necessary sample size.can use function answer different question:know advance can observe 30 students per group, effect size able observe 80% power 5% significance level?Let’s see :time want see effect size look second line can see experiment many people expected detect difference means d = 0.74 standard deviations. good bad? Well, depends natural variation data; data really noisy large variation large standard deviation mean 0.74 standard deviations might actually quite big difference groups. hand data doesn’t vary much, 0.74 standard deviations might actually really small number test pick even quite small differences mean.previous two examples little bit context-free terms effect size. Let’s look can use pilot study real data calculate effect sizes perform power analysis inform future study.Let’s look fishlength data saw first practical relating lengths fish two separate rivers. saved data/raw/CS6-fishlength.csv.summary() command can see 39 observations Aripo river 29 observations Guanapo river. box plot see groups appear different means t-test analysis can see difference significant.Can use information design efficient experiment? One confident powerful enough pick difference means big observed study fewer observations?Let’s first work exactly effect size previous study really estimating Cohen’s d using data.function calculates effect size using t-statistic (t) values sample sizes two groups (n1, n2). function can perform calculation given information (means two groups, standard deviations two groups etc.) look five examples bottom help file (?ES.t.two) see ways can used.second line d value want.rest output just padding reminds us conventional values effect sizes reference .can know actually answer question see many fish really need catch future:can see future experiments really need use 19 fish group (18.77 first line rounded , fish harmed experiment…) wanted confident detecting difference observed previous study.approach can also used pilot study showed smaller effect size wasn’t observed significant (indeed arguably, pilot study shouldn’t really concern significance really used way assessing potential effect sizes can used follow-study).","code":"\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative=\"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 63.76561\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\npwr.t.test(n = 30, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 30\n##               d = 0.7356292\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n# read in the data\nfishlength <- read.csv(\"data/raw/CS6-fishlength.csv\")\n\n# summarise the data\nsummary(fishlength)##      length         river          \n##  Min.   :11.20   Length:68         \n##  1st Qu.:18.40   Class :character  \n##  Median :19.30   Mode  :character  \n##  Mean   :19.46                     \n##  3rd Qu.:20.93                     \n##  Max.   :26.40\n# visualise the data\nboxplot(length ~ river,\n        data = fishlength)\n# perform a t-test, assuming equal variance\nt.test(length ~ river,\n       data = fishlength, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  length by river\n## t = 3.8433, df = 66, p-value = 0.0002754\n## alternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n## 95 percent confidence interval:\n##  0.9774482 3.0909868\n## sample estimates:\n##   mean in group Aripo mean in group Guanapo \n##              20.33077              18.29655\nES.t.two(t = 3.8433, n1 = 39, n2 = 29)## \n##      effect size (Cohen's d) of independent two-sample t test \n## \n##               d = 0.942383\n##     alternative = two.sided\n## \n## NOTE: The alternative hypothesis is m1 != m2\n## small effect size:  d = 0.2\n## medium effect size: d = 0.5\n## large effect size:  d = 0.8\npwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")## \n##      Two-sample t test power calculation \n## \n##               n = 18.77618\n##               d = 0.94\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group"},{"path":"power-analysis.html","id":"exercise-one-sample","chapter":"30 Power analysis","heading":"30.6 Exercise: one-sample","text":"Exercise 30.1  Performing power analysis one-sample data setLoad data/raw/CS6-onesample.csv (data looked earlier practical containing information fish lengths single river).Assume pilot study analyse data using one-sample t-test see evidence mean length fish differs 19 cm.Use results analysis estimate effect size.Work big sample size required detect effect big power 0.8 significance 0.05.sample size change wanted 0.9 power significance 0.01?First, read data:Let’s run one-sample t-test :OK, doesn’t appear statistically significant result ; mean length fish doesn’t appear different 19cm. output though see mean length sample fish 18.30 (2dp), sample little bit smaller 19 cm.Let’s calculate effect size using t-statistic degrees freedom t-test output . gives us following value effect size terms Cohen’s d metric.effect size 0.277 somewhere small medium effect size. means hard detect small sample size ’s likely need just 29 observations detect effect big.Now, let’s power analysis actually calculate sample size required:need 105 (round n value) observations experimental protocol order able detect effect size big (small?) 5% significance level 80% power. Let’s see happen wanted even stringent:198 observations! need lot work wanted work level significance power. small differences fish length biologically meaningful?","code":"\nexOne <- read.csv(\"data/raw/CS6-onesample.csv\")\nt.test(exOne$Guanapo, mu = 19)## \n##  One Sample t-test\n## \n## data:  exOne$Guanapo\n## t = -1.4657, df = 28, p-value = 0.1539\n## alternative hypothesis: true mean is not equal to 19\n## 95 percent confidence interval:\n##  17.31341 19.27969\n## sample estimates:\n## mean of x \n##  18.29655\nES.t.one(t = -1.4657, df = 28)## \n##      effect size (Cohen's d) of one-sample t test \n## \n##               d = 0.2769913\n##     alternative = two.sided\n## \n## NOTE: The alternative hypothesis is m != mu\n## small effect size:  d = 0.2\n## medium effect size: d = 0.5\n## large effect size:  d = 0.8\npwr.t.test(d = 0.2769913, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 104.2368\n##               d = 0.2769913\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\npwr.t.test(d = 0.2769913, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 197.2625\n##               d = 0.2769913\n##       sig.level = 0.01\n##           power = 0.9\n##     alternative = two.sided"},{"path":"power-analysis.html","id":"exercise-two-sample-paired","chapter":"30 Power analysis","heading":"30.7 Exercise: two-sample paired","text":"Exercise 30.2  Power analysis paired two-sample data setLoad data/raw/CS6-twopaired.csv (data used earlier practical relates cortisol levels measured 20 participants morning evening).first carry power analysis work big effect size experiment able detect power 0.8 significance level 0.05. Don’t look data just yet!Now calculate actual observed effect size study.repeat study future, many observations necessary detect observed effect 80% power significance level 0.01?First, read data:paired dataset 20 pairs observations, sort effect size detect significance level 0.05 power 0.8?Remember get effect size measured Cohen’s d metric. experimental design able detect d value 0.66 (2dp) medium large effect size.Now let’s look actual data work effect size actually :Use t-statistic calculate effect size:(1.19) massive effect size. ’s quite likely actually participants study actually need given large effect. Let calculate many individuals actually need:needed 12 pairs participants study given size effect trying detect.","code":"\nexTwo <- read.csv(\"data/raw/CS6-twopaired.csv\")\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")## \n##      Paired t test power calculation \n## \n##               n = 20\n##               d = 0.6604413\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number of *pairs*\nt.test(exTwo$morning, exTwo$evening, paired=TRUE)## \n##  Paired t-test\n## \n## data:  exTwo$morning and exTwo$evening\n## t = 5.1833, df = 19, p-value = 5.288e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   69.20962 162.96038\n## sample estimates:\n## mean of the differences \n##                 116.085\nES.t.paired(t = 5.1833, df = 19)## \n##      effect size (Cohen's d) of paired two-sample t test \n## \n##               d = 1.189131\n##     alternative = two.sided\n## \n## NOTE: The alternative hypothesis is md != 0\n## small effect size:  d = 0.2\n## medium effect size: d = 0.5\n## large effect size:  d = 0.8\npwr.t.test(d = 1.189131, sig.level = 0.01, power = 0.8,\n           type = \"paired\")## \n##      Paired t test power calculation \n## \n##               n = 11.67291\n##               d = 1.189131\n##       sig.level = 0.01\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number of *pairs*"},{"path":"power-analysis.html","id":"linear-model-power-calculations","chapter":"30 Power analysis","heading":"30.8 Linear model power calculations","text":"Thankfully ideas ’re already covered t-test section see us good stead going forward can stop writing everything excruciating detail (things know).linear models ’ll just use pwr.f2.test() power calculation won’t need function effect sizes (’s just based R2 just able read screen).Read data/raw/CS6-lobsters.csv. dataset used earlier practical describes effect lobster weight three different food sources.Type :Running commands:summary() shows us 6, 7 5 observations groupthe box plot shows us might well differences groupsthe ANOVA analysis though shows isn’t sufficient evidence support claim given insignificant p-value observe.question can ask :really difference different food sources big appears , big sample need order able detect statistically?First let’s calculate observed effect size study. linear models effect size called Cohen’s f2. can calculate easily using R2 value model fit shoving following formula:\\[\\begin{equation}\nf^2 = \\frac{R^2}{1-R^2}\n\\end{equation}\\]find R2 looking lm.lob object:give following:’ve seen output ; summarises coefficients linear model. though ’re interested Multiple R-squared value penultimate line. pilot lobster study R2 value appears 0.1797.\nUsing R calculator can work Cohen’s f2 value find effect size approximately 0.219.’s one thing need power calculation linear model (gets bit arbitrary); degrees freedom. can read bottom line output:first number bottom line F-statistic model (don’t need right now)next number called numerator degrees freedom, case 2. number want. simply number parameters model minus 1. model three parameters three groups (can see noticing table output three rows; intercept, dietMussels dietPellets). 3 - 1 = 2 (see maths isn’t bad)third number called denominator degrees freedom, case 15. actually number want power analysis calculate ’s proxy number observations used model, ’ll see minute., now want run power analysis linear model, using following information:power = 0.8significance = 0.05effect size = 0.219numerator DF = 2We can feed pwr.f2.test() function, useu represent numerator DF valuef2 represent Cohen’s f2 effect size valueAs numbers just ’ve put function . new number v. denominator degrees freedom required analysis sufficient power. Thankfully number related number observations use straightforward manner:\\(number\\:\\:observations = u + v + 1\\)case ideally 48 observations (45 + 2 + 1, remembering round ) experiment.\ntwo questions might now ask (’re still following – ’re quite possibly definitely need coffee now):many observations go group?\nideally equally distributed (case 16 per group).\nideally equally distributed (case 16 per group).complicated, isn’t just single function just , just tells many observations need?\ngood question – answer sorry – sometimes life just hard.\ngood question – answer sorry – sometimes life just hard.challenging part using power analyses linear models working numerator degrees freedom . easiest way thinking say ’s number parameters model, excluding intercept. look back wrote linear model equations, able see many non-zero parameters expected. simple cases table help , complex linear models need write linear model equation count parameters (sorry!).","code":"\n# read in the data\nlobsters <- read.csv(\"data/raw/CS6-lobsters.csv\")\n\n# summarise the data\nsummary(lobsters)\n\n# visualise the data\nboxplot(weight ~ diet, data = lobsters)\n# define the linear model\nlm.lob <- lm(weight ~ diet, data = lobsters)\n\n# perform an ANOVA on the model\nanova(lm.lob)\nsummary(lm.lob)## \n## Call:\n## lm(formula = weight ~ diet, data = lobsters)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -32.129 -16.155  -4.279  15.195  46.720 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  114.433      8.915  12.836 1.71e-09 ***\n## dietMussels   21.895     12.149   1.802   0.0916 .  \n## dietPellets   14.047     13.223   1.062   0.3049    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.84 on 15 degrees of freedom\n## Multiple R-squared:  0.1797, Adjusted R-squared:  0.07035 \n## F-statistic: 1.643 on 2 and 15 DF,  p-value: 0.2263\npwr.f2.test(u = 2, f2 = 0.219,\n            sig.level = 0.05, power = 0.8)## \n##      Multiple regression power calculation \n## \n##               u = 2\n##               v = 44.12292\n##              f2 = 0.219\n##       sig.level = 0.05\n##           power = 0.8"},{"path":"power-analysis.html","id":"exercise-mussel-muscles","chapter":"30 Power analysis","heading":"30.9 Exercise: Mussel muscles","text":"Exercise 30.3  Calculating effect sizeThe file data/raw/CS6-shelllength.csv contains information pilot study looking whether standardised length anterior adductor muscle scar mussel Mytilus trossulus differs across five locations around world (well might interest someone…).Find effect size study perform power calculation (0.8 0.05 significance level) determine many mussel muscles need recorded order confident effect really exists.Let’s load data:Let’s just quick look data see ’re dealing :effectively looking one-way ANOVA five groups. useful know later.Now fit linear model:get \\(R^2\\) value 0.4559 can use calculate Cohen’s \\(f^2\\) value using formula notes:Now, model 5 parameters (5 groups) numerator degrees freedom 4 \\((5 - 1 = 4)\\). means can now carry power analysis:tells us denominator degrees freedom 15 (14.62 rounded ), means need 20 observations total across five groups detect effect size (Remember: number observations = numerator d.f. + denominator d.f. + 1)","code":"\nmusseldat <- read.csv(\"data/raw/CS6-shelllength.csv\")\nboxplot(length ~ location,\n        data = musseldat)\n# define the model\nlm.mussel <- lm(length ~ location,\n                data = musseldat)\n\n# summarise the model\nsummary(lm.mussel)## \n## Call:\n## lm(formula = length ~ location, data = musseldat)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.025400 -0.007956  0.000100  0.007000  0.031757 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         0.078012   0.004454  17.517  < 2e-16 ***\n## locationNewport    -0.003213   0.006298  -0.510  0.61331    \n## locationPetersburg  0.025430   0.006519   3.901  0.00043 ***\n## locationTillamook   0.002187   0.005975   0.366  0.71656    \n## locationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.0126 on 34 degrees of freedom\n## Multiple R-squared:  0.4559, Adjusted R-squared:  0.3918 \n## F-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\nf2 <- 0.4559 / (1 - 0.4559)\nf2## [1] 0.8378974\npwr.f2.test(u = 4, f2 = 0.8378974,\n            sig.level = 0.05 , power = 0.8)## \n##      Multiple regression power calculation \n## \n##               u = 4\n##               v = 14.62182\n##              f2 = 0.8378974\n##       sig.level = 0.05\n##           power = 0.8"},{"path":"power-analysis.html","id":"exercise-epilepsy","chapter":"30 Power analysis","heading":"30.10 Exercise: epilepsy","text":"Exercise 30.4  Power effectThe file /data/raw/CS6-epilepsy1.csv contains information ages rates seizures 236 patients undertaking clinical trial.Analyse data using linear model calculate effect size.relationship large age seizure rate big study needed observe effect 90% power.Let’s load data:Let’s just quick look data see ’re dealing :effectively looking simple linear regression .Now fit linear model:get \\(R^2\\) value 0.0009134 (tiny!) can use calculate Cohen’s \\(f^2\\) value using formula notes:effect size absolutely tiny. really wanted design experiment pick effect size small expect ’ll need 1000s participants.Now, model 2 parameters (intercept slope) numerator degrees freedom (u) 1 (2 - 1 = 1!). means can now carry power analysis:tells us denominator degrees freedom (v) 11494 (11493.05 rounded ), means need 11496 participants detect effect size (Remember: number observations = numerator d.f. (u) + denominator d.f. (v) + 1).","code":"\nepilepsydat <- read.csv(\"data/raw/CS6-epilepsy1.csv\")\nplot(seizure ~ age,\n     data = epilepsydat)\n# define the model\nlm.epilepsy <- lm(seizure ~ age,\n                  data = epilepsydat)\n\n# summarise the model\nsummary(lm.epilepsy)## \n## Call:\n## lm(formula = seizure ~ age, data = epilepsydat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.77513 -0.19585 -0.04333  0.22288  1.24168 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\n## age         -0.001990   0.004303  -0.463    0.644    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.413 on 234 degrees of freedom\n## Multiple R-squared:  0.0009134,  Adjusted R-squared:  -0.003356 \n## F-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\nf2 <- 0.0009134 / (1 - 0.0009134)\nf2## [1] 0.0009142351\npwr.f2.test(u = 1, f2 = 0.0009142351,\n            sig.level = 0.05 , power = 0.9)## \n##      Multiple regression power calculation \n## \n##               u = 1\n##               v = 11493.05\n##              f2 = 0.0009142351\n##       sig.level = 0.05\n##           power = 0.9"},{"path":"power-analysis.html","id":"exercise-drug-versus-placebo","chapter":"30 Power analysis","heading":"30.11 Exercise: drug versus placebo","text":"Exercise 30.5  Study sizeWe wish test effectiveness new drug placebo. thought gender age patients may effect response.Write linear model equation might describe relationship variables including possible two-way interactions.big study need detect medium effect size (according Cohen) power 90%, significance level 0.05?system single response variable, three categorical predictor variables. One gender, two possible levels (M, F). One treatment, two possible levels (Drug, placebo) one continuous (age). linear model possible two-way interactions look something like :response ~ treatment + gender + age + treatment:gender + treatment:age + age:genderIn order power calculation set , ’ll need four things:effect size. ’re told ’s medium effect size according Cohen can use default values.desired power. ’re told ’s 90%significance level work . ’re told going 0.05.numerator degrees freedom. tricky bit. can adding degrees freedom term separately.effect size easy get:Alternatively, looked online (may give us different values, values relevant specific discipline).numerator degrees freedom best calculated working degrees freedom six terms separately adding .three simple ideas need:degrees freedom categorical variable just number groups - 1The degrees freedom continuous variable always 1the degrees freedom interaction simple product degrees main effects involved interaction.means:df gender 1 (2 groups - 1)df treatment 1 (2 groups -1)df age 1 (continuous predictor)df gender:treatment 1 (1 x 1)df gender:age 1 (1 x 1)df age:treatment 1 (1 x 1)Rather boring 1 honest. Anyway, given denominator degrees freedom just sum , can see \\(u = 6\\).now information carry power analysis:get denominator df 116, means need least 123 participants study (Remember: number observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given four unique combinations gender treatment, practically sensible round 124 participants equal number (31) combination gender treatment. also sensible aim similar distribution age ranges group well.","code":"\ncohen.ES(test = \"f2\", size = \"medium\")## \n##      Conventional effect size from Cohen (1982) \n## \n##            test = f2\n##            size = medium\n##     effect.size = 0.15\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)## \n##      Multiple regression power calculation \n## \n##               u = 6\n##               v = 115.5826\n##              f2 = 0.15\n##       sig.level = 0.05\n##           power = 0.9"},{"path":"power-analysis.html","id":"key-points-10","chapter":"30 Power analysis","heading":"30.12 Key points","text":"Power capacity test detect significant results affected \neffect size\nsample size\nsignificance level\neffect sizesample sizethe significance levelPower analysis optimises trade-power, significance level desired effect size like detectPoint 3","code":""},{},{"path":"statistical-reporting.html","id":"statistical-reporting","chapter":"31 Statistical reporting","heading":"31 Statistical reporting","text":"","code":""},{"path":"statistical-reporting.html","id":"objectives-17","chapter":"31 Statistical reporting","heading":"31.1 Objectives","text":"QuestionsHow consistently report statistical results?ObjectivesBe able follow consistent analysis methodology RReport statistical results clear structured way","code":""},{"path":"statistical-reporting.html","id":"purpose-and-aim-10","chapter":"31 Statistical reporting","heading":"31.2 Purpose and aim","text":"practise statistical methods covered previous sessions produce consistent statistical reports.","code":""},{"path":"statistical-reporting.html","id":"data-and-hypotheses-13","chapter":"31 Statistical reporting","heading":"31.3 Data and hypotheses","text":"first section uses data file data/raw/CS6-NPYield.csv. dataset comprising 24 observations three variables (one dependent two predictor). records yield peas (pounds/plot) different plots. plot record whether nitrogen /phosphate fertiliser added.","code":""},{"path":"statistical-reporting.html","id":"analysis-methodology","chapter":"31 Statistical reporting","heading":"31.4 Analysis methodology","text":"Carry analysis suggested following steps confirm analysis matches text section.","code":""},{"path":"statistical-reporting.html","id":"step-1---identify-the-variables-and-the-question-to-be-explored","chapter":"31 Statistical reporting","heading":"31.4.1 Step 1 - Identify the variables and the question to be explored","text":"Load dataset look raw data\nConfirm NPYield dataset contains 24 observations 3 variables\nConfirm NPYield dataset contains 24 observations 3 variablesCheck names types variables\nyield continuous variable\nNit Pho categorical (factor) predictor variables\nyield continuous variableNit Pho categorical (factor) predictor variablesThe obvious question ask variables Nit Pho affect variable yield?","code":""},{"path":"statistical-reporting.html","id":"step-2-describe-the-data","chapter":"31 Statistical reporting","heading":"31.4.2 Step 2 – Describe the data","text":"Plot data\nPlot effects individual predictor variables first. box plots yield Nit yield Pho appropriate.\nPlot effects interactions pairs predictor variables. interaction plot appropriate since two predictor variables categorical\nPlot effects individual predictor variables first. box plots yield Nit yield Pho appropriate.Plot effects interactions pairs predictor variables. interaction plot appropriate since two predictor variables categoricalCarry descriptive statistics may find useful\nCalculate means, variances, ranges etc. ’re going bother given simplicity dataset.\nCalculate means, variances, ranges etc. ’re going bother given simplicity dataset.Consider whether appear significant effects variables\nappears effect nitrogen\nappear effect phosphate\nmay interaction effect clear\nappears effect nitrogenThere appear effect phosphateThere may interaction effect clear","code":""},{"path":"statistical-reporting.html","id":"step-3---perform-tests-and-or-fit-models","chapter":"31 Statistical reporting","heading":"31.4.3 Step 3 - Perform tests and or fit models","text":"Select test appropriate data \n, single continuous response variable two categorical variables two options: two-way ANOVA test linear model (’d always go linear model framework). ’ll first fit model interactions lm(yield ~ Nit * Pho) look model reduction see terms valid.\n, single continuous response variable two categorical variables two options: two-way ANOVA test linear model (’d always go linear model framework). ’ll first fit model interactions lm(yield ~ Nit * Pho) look model reduction see terms valid.Assess results model fit\nIdentify significant effects. full model significant performing backwards stepwise elimination find minimal model yield ~ Nit.\nDetermine coefficients best fitting model. find formula given :\n\\[\\begin{equation}\n yield = 52.07 + \\binom{0}{5.62} \\binom{Nit:N}{Nit:Y}\n \\end{equation}\\]Identify significant effects. full model significant performing backwards stepwise elimination find minimal model yield ~ Nit.Determine coefficients best fitting model. find formula given :","code":""},{"path":"statistical-reporting.html","id":"step-4---check-assumptions-of-testsmodels","chapter":"31 Statistical reporting","heading":"31.4.4 Step 4 - Check assumptions of tests/models","text":"Plot Diagnostics plots /carry appropriate tests\njust plot diagnostic plots. look diagnostics full model minimal model.\njust plot diagnostic plots. look diagnostics full model minimal model.","code":""},{"path":"statistical-reporting.html","id":"writing-up-the-analysis","chapter":"31 Statistical reporting","heading":"31.5 Writing up the analysis","text":"four broad sections statistical report:IntroductionMethods & ResultsDiscussionAppendix","code":""},{"path":"statistical-reporting.html","id":"introduction-6","chapter":"31 Statistical reporting","heading":"31.5.1 Introduction","text":"aim :Describe data represent information collectedState question investigated.Always try keep language non-technicalFor example:IntroductionThe aim analysis investigate relationship yield peas addition fertilisers. yield peas pounds/plot recorded 24 plots used response variable. Two predictor variables thought potentially effect: addition fixed amount nitrogen based fertiliser addition fixed amount phosphate based fertiliser.","code":""},{"path":"statistical-reporting.html","id":"methods-and-results","chapter":"31 Statistical reporting","heading":"31.5.2 Methods and results","text":"section aim :Describe data using descriptive statistics plotsDescribe/state procedures undertaken\ne.g. “Fit linear model interactions variables”\ne.g. “Fit linear model interactions variables”Present figures state key results\ne.g. Show lines best fit scatter graph state whether F-statistics significant alongside relevant p-values.\ne.g. Show lines best fit scatter graph state whether F-statistics significant alongside relevant p-values.Produce diagnostic plots /results assumption testsThis section contain enough detail enable someone else replicate results. R output needs included .example using ggplot2 patchwork, makes lot easier produce nice graphs. possible similar things using base R syntax, ’ll probably find composing plot panels best done external programme.example:Methods resultsThe response variable yield plotted two categorical variables Pho Nit independently, means different categorical combinations calculated plotted. shown Figure 1:Figure 1a appears suggest isn’t effect phosphate yield. Figure 1b indicates might effect nitrogen yield. Figure 1c suggests might interaction effect nitrogen phosphate yield, although might due presence outlier (Phosphate & Nitrogen) group.full linear model containing variables interaction fitted data (yield ~ Nit + Pho + Nit:Pho) model assumptions checked using full residual analysis (see Figure 2). assumptions equal variance normality appear met, suggesting linear model analysis may adequate data.ANOVA analysis full model compared null model results gives non-significant result (F3,20 = 2.21, p = 0.12) suggesting insufficient evidence yield affected variables.Backwards stepwise elimination used find minimal model. interaction Nitrogen Phosphate Phosphate terms yield found significant minimal model found :\\[\\begin{equation}\nyield ~ Nit\n\\end{equation}\\]variable Nit significant predictor yield (F1,22 = 6.06, p = 0.02).\\[\\begin{equation}\nyield = 52.07 + \\binom{0}{5.62} \\binom{Nit:N}{Nit:Y}\n\\end{equation}\\]box plot final model result () alongside diagnostic plots minimal model (b, c) shown Figure 3. assumptions equal variance normality still appear met, suggesting linear model analysis appropriate data.","code":""},{"path":"statistical-reporting.html","id":"discussion","chapter":"31 Statistical reporting","heading":"31.5.3 Discussion","text":"section aim :Summarise results context question, .e. find?Discuss results model assumption tests. met? anything slightly dodgy?Discuss data limitations, e.g. number data points, presence outliers etc.example:DiscussionThe analysis shows linear model may adequate data addition Nitrogen based fertiliser significant predictor pea yield, whereas Phosphate based fertiliser statistically significant effect. can seen addition Nitrogen appears increase yield.\ninteresting note whilst reduced model shown statistically significant, full model significant compared null model. number data points relatively small full interaction analysis (6 points per 2-way classification), repeating analysis larger number observations might beneficial.","code":""},{"path":"statistical-reporting.html","id":"appendix","chapter":"31 Statistical reporting","heading":"31.5.4 Appendix","text":"section always necessary / included option :Add R output , .e. Full ANOVA tables printed outputCould include copy R script ’ve done something clever terms data manipulation. can aid reproducibility results ’ve obtained","code":""},{"path":"statistical-reporting.html","id":"exercise-cars-2-variables","chapter":"31 Statistical reporting","heading":"31.6 Exercise: cars 2 variables","text":"Exercise 31.1  Investigate relationship continuous response variable mpg categorical predictor variables cyl gear.data can found data/raw/CS6-cars2var.csv. dataset:mpg miles per galloncyl number cylinders (categorical variable)gear number gears (categorical variable)wt weight car (continuous variable)load datasets R, check variable interpreted correctly. Specifically check categorical variables loaded factors numbers. Specifically, may force variable interpreted factor using line code : cars3$gear <- factor(cars3$gear)","code":""},{"path":"statistical-reporting.html","id":"exercise-cars-3-variables","chapter":"31 Statistical reporting","heading":"31.7 Exercise: cars 3 variables","text":"Exercise 31.2  dataset now also includes continuous predictor variable wt. Investigate relationships four variables.data can found data/raw/CS6-cars3var.csv. dataset:mpg miles per galloncyl number cylinders (categorical variable)gear number gears (categorical variable)wt weight car (continuous variable)dataset three predictor variables (, B, C) one response variables (Y) six initial plots:3 individual plots: Y vs. , Y vs. B, Y vs. C3 two-variable plots: Y vs. &B, Y vs. &C, Y vs. B&CWe know look two types two-variable plots:categorical x categorical (interaction.plot)categorical x continuous (see linear model practical)won’t usually plot continuous x continuous two-variable plots (since require 3D plotting), ’re interested look plot3d() command rgl library.","code":""},{"path":"statistical-reporting.html","id":"key-points-11","chapter":"31 Statistical reporting","heading":"31.8 Key points","text":"useful analysis methodology :\nidentify variables question explored\ndescribe data (plots, summaries)\nperform tests fit models\ncheck assumptions tests/models\nidentify variables question exploreddescribe data (plots, summaries)perform tests fit modelscheck assumptions tests/modelsWriting statistical analysis often follows structure:\nIntroduction (description data non-technical way; collection methods; question answer)\nMethods & Results (description data; procedures; present figures key results; results assumptions checks)\nDiscussion (summarise results; discuss results assumptions; limitations data)\nAppendix (optional, can include R output)\nIntroduction (description data non-technical way; collection methods; question answer)Methods & Results (description data; procedures; present figures key results; results assumptions checks)Discussion (summarise results; discuss results assumptions; limitations data)Appendix (optional, can include R output)","code":""}]
