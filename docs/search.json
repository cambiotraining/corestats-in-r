[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform core data analysis techniques appropriately confidently using R.6 lecture-practicals6 lecture-practicalsOngoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented arbitrary dataset e.g.Know data analysis techniques availableKnow ones allowableBe able carry understand results","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Simple hypothesis testingCategorical predictor variablesContinuous predictorsTwo predictor variablesMultiple predictor variablesPower analysis","code":""},{"path":"index.html","id":"practicals","chapter":"1 Overview","heading":"1.3 Practicals","text":"practical document divided various sections. section explanatory text help understand going ’re trying achieve.\nmay list commands relevant section displayed boxes like :Conditional operatorsTo set filtering conditions, use following relational operators:> greater >= greater equal < less <= less equal == equal != different %% contained combine conditions, use following logical operators:& | ","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.4 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data/tidy.","code":""},{},{"path":"cs3-intro.html","id":"cs3-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"cs3-intro.html","id":"objectives","chapter":"2 Introduction","heading":"2.1 Objectives","text":"Aim: introduce R commands analysing simple linear modelsBy end practical participants able perform following statistical analyses:Simple Linear RegressionCorrelationFor , participants able :Perform test RInterpret outputCheck assumptions test","code":""},{"path":"cs3-intro.html","id":"background","chapter":"2 Introduction","heading":"2.2 Background","text":"practical focuses implementation various statistical tests relating simple linear regression correlation., focus underlying theory tests (although demonstrators happy answer questions may ).test section :explains purpose test,explains visualise data,explains perform test R,explains interpret output report results, andexplains assess assumptions required perform test.","code":""},{},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction","heading":"3 Introduction","text":"practical introducing can compare data different continuous variables.","code":""},{"path":"introduction.html","id":"cs3-datasets","chapter":"3 Introduction","heading":"3.1 Datasets","text":"section uses various data sets. located data/raw/ folder working directory. Please see Datasets information.","code":""},{},{"path":"correlation-coefficients.html","id":"correlation-coefficients","chapter":"4 Correlation coefficients","heading":"4 Correlation coefficients","text":"","code":""},{"path":"correlation-coefficients.html","id":"objectives-1","chapter":"4 Correlation coefficients","heading":"4.1 Objectives","text":"QuestionsWhat correlation coefficients?kind correlation coefficients use ?ObjectivesBe able calculate correlation coefficients RUse visual tools explore correlations variablesKnow limitations correlation coefficients","code":""},{"path":"correlation-coefficients.html","id":"purpose-and-aim","chapter":"4 Correlation coefficients","heading":"4.2 Purpose and aim","text":"Correlation refers relationship two variables (datasets) one another. Two datasets said correlated independent one another. Correlations can useful can indicate predictive relationship may exist. However just two datasets correlated mean causally related.","code":""},{"path":"correlation-coefficients.html","id":"section-commands","chapter":"4 Correlation coefficients","heading":"4.3 Section commands","text":"New commands used section:","code":""},{"path":"correlation-coefficients.html","id":"data-and-hypotheses","chapter":"4 Correlation coefficients","heading":"4.4 Data and hypotheses","text":"use USArrests dataset example. rather bleak dataset contains statistics arrests per 100,000 residents assault, murder robbery 50 US states 1973, alongside proportion population lived urban areas time. USArrests data frame 50 observations five variables: state, murder, assault, urban_pop robbery.data stored file data/tidy/CS3-usarrests.csv.First read data:","code":"\n# load the data\nUSArrests <- read_csv(\"data/tidy/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests## # A tibble: 50 × 5\n##    state       murder assault urban_pop robbery\n##    <chr>        <dbl>   <dbl>     <dbl>   <dbl>\n##  1 Alabama       13.2     236        58    21.2\n##  2 Alaska        10       263        48    44.5\n##  3 Arizona        8.1     294        80    31  \n##  4 Arkansas       8.8     190        50    19.5\n##  5 California     9       276        91    40.6\n##  6 Colorado       7.9     204        78    38.7\n##  7 Connecticut    3.3     110        77    11.1\n##  8 Delaware       5.9     238        72    15.8\n##  9 Florida       15.4     335        80    31.9\n## 10 Georgia       17.4     211        60    25.8\n## # … with 40 more rows"},{"path":"correlation-coefficients.html","id":"pearsons-product-moment-correlation-coefficient","chapter":"4 Correlation coefficients","heading":"4.5 Pearson’s product moment correlation coefficient","text":"Pearson’s r (quantity also known) measure linear correlation two variables. value -1 +1, +1 means perfect positive correlation, -1 means perfect negative correlation 0 means correlation .can look correlations need reformat data little bit. functions ’re going use require data frames contain numbers input. want keep state information linked data, need define state column name rows.need update original USArrests data frame, ’re just piping displaying output can see ’s going .","code":"\n# convert the state column to row names\nUSArrests %>% \n  column_to_rownames(var = \"state\")"},{"path":"correlation-coefficients.html","id":"summarise-and-visualise","chapter":"4 Correlation coefficients","heading":"4.6 Summarise and visualise","text":"Knowing reformatting works, can first visualise data:argument lower.panel tells R add redundant reflected lower set plots, diagonalFrom visual inspection scatter plots can see appears slight positive correlation pairs variables, although may weak cases (murder urban_pop example).","code":"\n# create correlation plot\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"implement-test","chapter":"4 Correlation coefficients","heading":"4.7 Implement test","text":"can calculate Pearson’s correlation coefficients pair variables (e.g. coefficient murder assault). several functions allow . cor() function base R cor_mat() rstatix package, spit results matrix (grid) format. ’ll use cor_mat() can keep using tibble data sets.First create matrix, keeping state data linked row namesThe method argument tells R correlation coefficient use (pearson (default), kendall, spearman)","code":"\n# calculate Pearson's correlation coefficients\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results","chapter":"4 Correlation coefficients","heading":"4.8 Interpret output and report results","text":"give following output:table gives correlation coefficient pair variables data frame. correlated variables murder assault r value 0.80. appears agree well set scatter plots produced earlier.","code":"## # A tibble: 4 × 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.8       0.07    0.56\n## 2 assault     0.8     1         0.26    0.67\n## 3 urban_pop   0.07    0.26      1       0.41\n## 4 robbery     0.56    0.67      0.41    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-pearson","chapter":"4 Correlation coefficients","heading":"4.9 Exercise: State data (Pearson)","text":"Exercise 4.1  Pearson’s correlation USA state dataWe use data file data/tidy/CS3-statedata.csv dataset exercise. rather benign dataset contains information general properties US state, population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (’s getting away ), percentage population high-school graduates, average number days minimum temperature freezing 1931 1960, state area square miles. dataset contains 50 rows 8 columns, column names: population, income, illiteracy, life_exp, murder, hs_grad, frost area.Load data (remembering tell R first column CSV file used specify row names dataset) use pairs() command visually identify 3 different pairs variables appear bethe positively correlatedthe negatively correlatednot correlated allCalculate Pearson’s r variable pairs see well able identify correlation visually.get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 × 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # … with 40 more rows\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 × 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # … with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 × 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # … with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"read-in-the-data","chapter":"4 Correlation coefficients","heading":"4.9.1 Read in the data","text":"","code":"\nUSAstate <- read_csv(\"data/tidy/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate## # A tibble: 50 × 9\n##    state       population income illiteracy life_exp murder hs_grad frost   area\n##    <chr>            <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n##  1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n##  2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n##  3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n##  4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n##  5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n##  6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n##  7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n##  8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n##  9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n## 10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n## # … with 40 more rows"},{"path":"correlation-coefficients.html","id":"pair-wise-comparisons-visual","chapter":"4 Correlation coefficients","heading":"4.9.2 Pair-wise comparisons (visual)","text":"","code":"\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"calculate-the-correlation-coefficients","chapter":"4 Correlation coefficients","heading":"4.9.3 Calculate the correlation coefficients","text":"get correlation coefficients format allows us manipulate , use cor_test() function. something similar cor_mat() function - calculates pairwise correlation coefficients. However, outputs results table format, instead matrix.two variables compared given var1 var2 columns. correlation coefficient given cor column.extract maximum, minimum least correlated pairs, easy filter correlation table bit , pair now appears twice (orientation, murder & assault, assault & murder).Now unique pairs corresponding correlation coefficients, can extract information need:taken together:positively correlated variables illiteracy murderThe negatively correlated variables life_exp murderThe uncorrelated variables population area","code":"\n# calculate Pearson's correlation coefficients\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\")## # A tibble: 64 × 8\n##    var1       var2          cor     statistic      p conf.low conf.high method \n##    <chr>      <chr>       <dbl>         <dbl>  <dbl>    <dbl>     <dbl> <chr>  \n##  1 population population  1           Inf     0        1         1      Pearson\n##  2 population income      0.21          1.47  0.147   -0.0744    0.460  Pearson\n##  3 population illiteracy  0.11          0.750 0.457   -0.176     0.375  Pearson\n##  4 population life_exp   -0.068        -0.473 0.639   -0.340     0.214  Pearson\n##  5 population murder      0.34          2.54  0.0146   0.0722    0.568  Pearson\n##  6 population hs_grad    -0.098        -0.686 0.496   -0.367     0.185  Pearson\n##  7 population frost      -0.33         -2.44  0.0184  -0.559    -0.0593 Pearson\n##  8 population area        0.023         0.156 0.877   -0.257     0.299  Pearson\n##  9 income     population  0.21          1.47  0.147   -0.0744    0.460  Pearson\n## 10 income     income      1     464943848.    0        1         1      Pearson\n## # … with 54 more rows\n# calculate the correlation coefficients\n# select the unique pairs\n# and store in a new object\nUSAstate_cor <- USAstate %>% \n  column_to_rownames(var = \"state\") %>%\n  cor_test(method = \"pearson\") %>% \n  # filter out the self-pairs (e.g. murder & murder)\n  filter(cor != 1) %>% \n  # arrange the data by correlation coefficient\n  arrange(cor) %>% \n  # each correlation appears twice\n  # because the pairs are duplicated\n  group_by(cor) %>% \n  # slice the first row of each group\n  slice(seq(1, n(), by = 2)) %>% \n  # remove the grouping\n  ungroup()\n\n# have a look at the ouput\nUSAstate_cor## # A tibble: 28 × 8\n##    var1       var2         cor statistic        p conf.low conf.high method \n##    <chr>      <chr>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <chr>  \n##  1 life_exp   murder     -0.78    -8.66  2.26e-11   -0.870   -0.642  Pearson\n##  2 illiteracy frost      -0.67    -6.29  9.16e- 8   -0.801   -0.484  Pearson\n##  3 illiteracy hs_grad    -0.66    -6.04  2.17e- 7   -0.791   -0.464  Pearson\n##  4 illiteracy life_exp   -0.59    -5.04  6.97e- 6   -0.745   -0.371  Pearson\n##  5 murder     frost      -0.54    -4.43  5.4 e- 5   -0.711   -0.307  Pearson\n##  6 murder     hs_grad    -0.49    -3.87  3.25e- 4   -0.675   -0.243  Pearson\n##  7 income     illiteracy -0.44    -3.37  1.51e- 3   -0.638   -0.181  Pearson\n##  8 population frost      -0.33    -2.44  1.84e- 2   -0.559   -0.0593 Pearson\n##  9 income     murder     -0.23    -1.64  1.08e- 1   -0.478    0.0516 Pearson\n## 10 life_exp   area       -0.11    -0.748 4.58e- 1   -0.374    0.176  Pearson\n## # … with 18 more rows\n# get most positively correlated pair\nUSAstate_cor %>%\n  filter(cor == max(cor))\n\n# get most negatively correlated pair\nUSAstate_cor %>%\n  filter(cor == min(cor))\n\n# get least correlated pair\nUSAstate_cor %>%\n  # abs() computes the absolute value\n  filter(cor == min(abs(cor)))"},{"path":"correlation-coefficients.html","id":"spearmans-rank-correlation-coefficient","chapter":"4 Correlation coefficients","heading":"4.10 Spearman’s rank correlation coefficient","text":"test first calculates rank numerical data (.e. position smallest (negative) largest (positive)) two variables calculates Pearson’s product moment correlation coefficient using ranks. consequence, test less sensitive outliers distribution.","code":""},{"path":"correlation-coefficients.html","id":"implement-test-1","chapter":"4 Correlation coefficients","heading":"4.11 Implement test","text":"using USArrests data set , run command:Remember cor_mat() requires matrix, use state column row namesThe argument method tells R correlation coefficient use","code":"\nUSArrests %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")"},{"path":"correlation-coefficients.html","id":"interpret-output-and-report-results-1","chapter":"4 Correlation coefficients","heading":"4.12 Interpret output and report results","text":"gives following output:table gives correlation coefficient pair variables data frame. Slightly annoyingly, pair occurs twice opposite direction.","code":"## # A tibble: 4 × 5\n##   rowname   murder assault urban_pop robbery\n## * <chr>      <dbl>   <dbl>     <dbl>   <dbl>\n## 1 murder      1       0.82      0.11    0.68\n## 2 assault     0.82    1         0.28    0.71\n## 3 urban_pop   0.11    0.28      1       0.44\n## 4 robbery     0.68    0.71      0.44    1"},{"path":"correlation-coefficients.html","id":"exercise-state-data-spearman","chapter":"4 Correlation coefficients","heading":"4.13 Exercise: State data (Spearman)","text":"Exercise 4.2  Spearman’s correlation USA state dataCalculate Spearman’s correlation coefficient data/tidy/CS3-statedata.csv dataset.variable’s correlations affected use Spearman’s rank compared Pearson’s r?reference scatter plot produced earlier, can explain might ?Remember use column_to_rownames(var = \"state\") argument load data matrixInstead eye-balling differences, think can determine difference two correlation matricesThe cor_plot() function can useful visualise matricesIn order determine variables affected choice Spearman vs Pearson just plot matrices side side try spot going , one reasons ’re using R can bit programmatic things. Also, eyes aren’t good processing parsing sort information display. better way somehow visualise data.Let’s calculate difference two correlation matrices. create correlation matrix using cor_mat(). Next remove rowname, ’re left just data frame containing numbers. way can subtract values two data frames.Lastly, use cor_plot() function plot heatmap differences.one cases using tidyverse actually necessarily easiest way. similar thing using base R syntax:plot coloured blue red, indicating biggest positive differences correlation coefficients blue. biggest negative differences coloured red, whereas least difference indicated white.plot symmetric along leading diagonal (hopefully obvious reasons) can see majority squares light blue light red colour, means isn’t much difference Spearman Pearson vast majority variables. squares appear darkest look along area row/column suggesting ’s big difference correlation coefficients .can now revisit pairwise scatter plot see makes sense:can see clearly correspond plots noticeable outliers. example, Alaska twice big next biggest state, Texas. Big outliers data can large impact Pearson coefficient, whereas Spearman coefficient robust effects outliers. can see detail look area vs income graph coefficients. Pearson gives value 0.36, slight positive correlation, whereas Spearman gives value 0.057, basically uncorrelated. single outlier (Alaska) top-right scatter plot big effect Pearson practically ignored Spearman.Well done, Mr. Spearman.","code":"\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\")## # A tibble: 8 × 9\n##   rowname    population income illiteracy life_exp murder hs_grad frost   area\n## * <chr>           <dbl>  <dbl>      <dbl>    <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n## 1 population       1     0.12        0.31    -0.1    0.35   -0.38 -0.46 -0.12 \n## 2 income           0.12  1          -0.31     0.32  -0.22    0.51  0.2   0.057\n## 3 illiteracy       0.31 -0.31        1       -0.56   0.67   -0.65 -0.68 -0.25 \n## 4 life_exp        -0.1   0.32       -0.56     1     -0.78    0.52  0.3   0.13 \n## 5 murder           0.35 -0.22        0.67    -0.78   1      -0.44 -0.54  0.11 \n## 6 hs_grad         -0.38  0.51       -0.65     0.52  -0.44    1     0.4   0.44 \n## 7 frost           -0.46  0.2        -0.68     0.3   -0.54    0.4   1     0.11 \n## 8 area            -0.12  0.057      -0.25     0.13   0.11    0.44  0.11  1\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_pear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"pearson\") %>% \n  # remove the row names\n  select(-rowname)\n\n# create a data frame that contains all the Pearson's coefficients\nUSAstate_spear <- USAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  cor_mat(method = \"spearman\") %>% \n  # remove the row names\n  select(-rowname)\n\n# calculate the difference between Pearson's and Spearman's\nUSAstate_diff <- USAstate_pear - USAstate_spear\n\n# use the column names of the data set as rownames\nrownames(USAstate_diff) <- names(USAstate_diff)\n\nUSAstate_diff %>%\n  cor_plot()\n# read in the data with the base R read.csv function\n# and assign the first column as row names\nUSAstate_base <- read.csv(\"data/tidy/CS3-statedata.csv\", row.names = 1)\n\n# calculate a correlation matrix using Pearson's\ncorPear <- cor(USAstate_base, method = \"pearson\")\n\n# calculate a correlation matrix using Spearman\ncorSpea <- cor(USAstate_base, method = \"spearman\")\n\n# calculate the difference between the two matrices\ncorDiff <- corPear - corSpea\n\n# and plot it, like before\ncorDiff %>% \n  cor_plot()\n# visual comparisons of variables\nUSAstate %>% \n  column_to_rownames(var = \"state\") %>% \n  pairs(lower.panel = NULL)"},{"path":"correlation-coefficients.html","id":"key-points","chapter":"4 Correlation coefficients","heading":"4.14 Key points","text":"Correlation degree two variables linearly relatedCorrelation imply causationWe can visualise correlations using pairs() cor_plot() functionsUsing cor_mat() cor_test() functions can calculate correlation matricesTwo main correlation coefficients Pearson’s r Spearman’s rank, Spearman’s rank less sensitive outliers","code":""},{},{"path":"linear-regression.html","id":"linear-regression","chapter":"5 Linear regression","heading":"5 Linear regression","text":"","code":""},{"path":"linear-regression.html","id":"objectives-2","chapter":"5 Linear regression","heading":"5.1 Objectives","text":"QuestionsWhen use linear regression?interpret results?ObjectivesBe able perform linear regression RUse ANOVA check slope regression differs zeroUnderstand underlying assumptions linear regression analysisUse diagnostic plots check assumptions","code":""},{"path":"linear-regression.html","id":"purpose-and-aim-1","chapter":"5 Linear regression","heading":"5.2 Purpose and aim","text":"Regression analysis tests association two variables, also allows one investigate quantitatively nature relationship present, thus determine whether one variable may used predict values another.\nSimple linear regression essentially models dependence scalar dependent variable (y) independent (explanatory) variable (x) according relationship:\\[\\begin{equation*} \ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\\(\\beta_0\\) value intercept \\(\\beta_1\\) slope fitted line. aim simple linear regression analysis assess whether coefficient slope, \\(\\beta_1\\), actually different zero. different zero can say \\(x\\) significant effect \\(y\\) (since changing \\(x\\) leads predicted change \\(y\\)), whereas isn’t significantly different zero, say isn’t sufficient evidence relationship. course, order assess whether slope significantly different zero first need calculate values \\(\\beta_0\\) \\(\\beta_1\\).","code":""},{"path":"linear-regression.html","id":"section-commands-1","chapter":"5 Linear regression","heading":"5.3 Section commands","text":"new commands used section.","code":""},{"path":"linear-regression.html","id":"data-and-hypotheses-1","chapter":"5 Linear regression","heading":"5.4 Data and hypotheses","text":"perform simple linear regression analysis two variables murder assault USArrests dataset. wish determine whether assault variable significant predictor murder variable. means need find coefficients \\(\\beta_0\\) \\(\\beta_1\\) best fit following macabre equation:\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\cdot Assault\n\\end{equation*}\\]testing following null alternative hypotheses:\\(H_0\\): assault significant predictor murder, \\(\\beta_1 = 0\\)\\(H_1\\): assault significant predictor murder, \\(\\beta_1 \\neq 0\\)","code":""},{"path":"linear-regression.html","id":"summarise-and-visualise-1","chapter":"5 Linear regression","heading":"5.5 Summarise and visualise","text":"can visualise data :appears relatively strong positive relationship two variables whilst reasonable scatter points around trend line, probably expect significant result case.","code":"\n# create scatterplot of the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point()"},{"path":"linear-regression.html","id":"assumptions","chapter":"5 Linear regression","heading":"5.6 Assumptions","text":"order linear regression analysis valid 4 key assumptions need met:data must linear (entirely possible calculate straight line data straight - doesn’t mean !)residuals must normally distributedThe residuals must correlated fitted valuesThe fit depend overly much single point (point high leverage).Whether assumptions met can easily checked visually producing four key diagnostic plots.First need define linear model:first argument lm formula saying murder depends assaults. seen , syntax generally dependent variable ~ independent variable.second argument specifies dataset useNext, can create diagnostic plots model:top left graph plots Residuals plot. data best explained straight line uniform distribution points horizontal blue line (sufficient points red line, smoother line, top blue line). plot pretty good.top right graph shows Q-Q plot allows visual inspection normality. residuals normally distributed, points lie diagonal dotted line. isn’t bad slight snaking towards upper end appears outlier.bottom left Location-scale graph allows us investigate whether correlation residuals predicted values whether variance residuals changes significantly. , red line horizontal. correlation change variance red line horizontal. plot fine.last graph shows Cook’s distance tests one point unnecessarily large effect fit. important aspect see points larger 0.5 (meaning ’d careful) 1.0 (meaning ’d definitely check point large effect model). , point undue influence. plot good.Formally, concern looking diagnostic plots, linear regression valid. However, disappointingly, people ever check whether linear regression assumptions met quoting results.Let’s change leading example!","code":"\nlm_1 <- lm(murder ~ assault,\n           data = USArrests)\nlm_1 %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression.html","id":"implement-test-2","chapter":"5 Linear regression","heading":"5.7 Implement test","text":"already defined linear model, can closer look :function lm returns linear model (lm) object essentially list containing everything necessary understand analyse linear model. However, just type model name () just prints screen actual coefficients model .e. intercept slope line.found line best fit given :\\[\\begin{equation*}\nMurder = 0.63 + 0.042 \\cdot Assault\n\\end{equation*}\\]Assess whether slope significantly different zero:, use anova() command assess significance. shouldn’t surprising stage introductory lectures made sense. mathematical perspective, one-way ANOVA simple linear regression exactly makes sense use command analyse R.","code":"\n# show the linear model\nlm_1## \n## Call:\n## lm(formula = murder ~ assault, data = USArrests)\n## \n## Coefficients:\n## (Intercept)      assault  \n##     0.63168      0.04191\nanova(lm_1)"},{"path":"linear-regression.html","id":"interpret-output-and-report-results-2","chapter":"5 Linear regression","heading":"5.8 Interpret output and report results","text":"exactly format table saw one-way ANOVA:1st line just tells ANOVA testThe 2nd line tells response variable (case Murder)3rd, 4th 5th lines ANOVA table contain useful values:\nDf column contains degrees freedom values row, 1 48 (’ll need reporting)\nF value column contains F statistic, 86.454 (’ll need reporting).\np-value 2.596e-12 number directly Pr(>F) 4th line.\nvalues table (Sum Sq Mean Sq) column used calculate F statistic don’t need know .\nDf column contains degrees freedom values row, 1 48 (’ll need reporting)F value column contains F statistic, 86.454 (’ll need reporting).p-value 2.596e-12 number directly Pr(>F) 4th line.values table (Sum Sq Mean Sq) column used calculate F statistic don’t need know ., p-value ’re interested shows us probability getting data null hypothesis actually true slope line actually zero.\nSince p-value excruciatingly tiny can reject null hypothesis state :simple linear regression showed assault rate US states significant predictor number murders (F = 86.45, df = 1,48, p = 2.59x10-12).Plotting regression lineIt can helpful plot regression line original data see far data predicted linear values. can :plot data using geom_point()Next, add linear model using geom_smooth(method = \"lm\"), hiding confidence intervals (se = FALSE)","code":"## Analysis of Variance Table\n## \n## Response: murder\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## assault    1 597.70  597.70  86.454 2.596e-12 ***\n## Residuals 48 331.85    6.91                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nUSArrests %>% \n  ggplot(aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"},{"path":"linear-regression.html","id":"exercise","chapter":"5 Linear regression","heading":"5.9 Exercise","text":"Exercise 5.1  Linear regressionCalculate two simple linear regressions using data/tidy/CS3-statedata.csv dataset, first variable life_exp variable murder variable hs_grad frost.following cases:Find value slope intercept coefficients regressionsDetermine slope significantly different zero (.e. relationship two variables)Produce scatter plot data line best fit superimposed top.Produce diagnostic plots discuss (virtual) neighbour carried simple linear regression caseMurder Life ExpectancyLet’s see murder variable can used predict life_exp variable. Let’s plot first .visualise reasons:check data aren’t obviously wrong. sensible values life expectancy (nothing massively large small), plausible values murder rates (’m au fait US murder rates 1976 small positive numbers seem plausible).check see expect statistical analysis. appear reasonable downward trend data. surprised didn’t get significant result given amount data spread data lineWe check assumptions (roughly though ’ll properly minute). Nothing immediately gives cause concern; data appear linear, spread data around line appears homogeneous symmetrical. outliers either.Now, let’s check assumptions diagnostic plots.Residuals plot appears symmetric enough (similar distribution points horizontal blue line) happy linearity. Similarly red line Location-Scale plot looks horizontal enough happy homogeneity variance. aren’t influential points Cook’s distance plot. plot give bit concern Q-Q plot. see clear evidence snaking, although degree snaking isn’t actually bad. just means can pretty certain distribution residuals isn’t normal, also isn’t non-normal. situation? Well, three possible options:Appeal Central Limit Theorem. states large enough sample size don’t worry whether distribution residuals normally distributed. Large enough bit moving target honest depends non-normal underlying data . data little bit non-normal can get away using smaller sample data massively skewed (example). exact science, anything 30 data points considered lot mild moderate non-normality (case). data skewed looking data points (50-100). , example can legitimately just carry analysis without worrying.Try transforming data. try applying mathematical functions response variable (life_exp) hope repeating analysis transformed variable make things better. honest might work won’t know try. Dealing transformed variables legitimate approach can make interpreting model bit challenging. particular example none traditional transformations (log, square-root, reciprocal) anything fix slight lack normality (can take word try using; lm(log(LifeExp ~ Murder, data = USAstate)) example.Go permutation methods / bootstrapping. approach definitely work. don’t time explain (’s subject entire practical). approach also requires us reasonably large sample size work well assume distribution sample good approximation distribution entire dataset.case, large enough sample size deviation normality isn’t bad, can just crack standard analysis., let’s actually analysis:find murder rate statistically significant predictor life expectancy US states. Woohoo!High School Graduation Frosty DaysNow let’s investigate relationship proportion High School Graduates state (hs_grad) mean number days freezing (frost) within state., look data.doesn’t appear ridiculous errors data; High School graduation proportions 0-100% range mean number sub-zero days state 0 365, numbers plausible.Whilst trend upwards, wouldn’t surprise came back significant, ’m bit concerned …assumptions. ’m mainly concerned data aren’t linear. appears noticeable pattern data sort minimum around 50-60 Frost days. means ’s hard assess assumptions.Let’s check properlyNow, let’s check assumptions diagnostic plots.can see suspected backed Residuals plot. data aren’t linear appears sort odd -pattern . Given lack linearity just isn’t worth worrying plots model misspecified: straight line just doesn’t represent data .Just reference, practice looking diagnostic plots, ignore lack linearity can say thatNormality pretty good Q-Q plotHomogeneity variance isn’t good appears noticeable drop variance go left right (consideration Location-Scale plot)don’t appear influential points (looking Cook’s distance plot)However, none relevant particular case since data aren’t linear straight line wrong model fit.situation?Well actually, bit tricky aren’t easy fixes . two broad solutions dealing misspecified model.common solution need predictor variables model. ’re trying explain/predict high school graduation using number frost days. Obviously many things affect proportion high school graduates just cold State (weird potential predictor think ) need statistical approach allows us look multiple predictor variables. ’ll cover approach next two sessions.potential solution say high school graduation can fact predicted number frost days relationship isn’t linear. need specify relationship (curve basically) try fit data new, non-linear, curve. process called, unsurprisingly, non-linear regression don’t cover course. process best used already strong theoretical reason non-linear relationship two variables (sigmoidal dose-response curves pharmacology exponential relationships cell growth). case don’t preconceived notions wouldn’t really appropriate case.Neither solutions can tackled knowledge far course can definitely say based upon data set, isn’t linear relationship (significant otherwise) frosty days high school graduation rates.","code":"\n# plot the data and add the regression line\nUSAstate %>% \n  ggplot(aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n# create a linear model\nlm_murder <- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nlm_murder %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\nanova(lm_murder)## Analysis of Variance Table\n## \n## Response: life_exp\n##           Df Sum Sq Mean Sq F value   Pr(>F)    \n## murder     1 53.838  53.838  74.989 2.26e-11 ***\n## Residuals 48 34.461   0.718                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# plot the data\nUSAstate %>% \n  ggplot(aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n# create a linear model\nlm_frost <- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nlm_frost %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)"},{"path":"linear-regression.html","id":"key-points-1","chapter":"5 Linear regression","heading":"5.10 Key points","text":"Linear regression tests linear relationship exists two variablesIf , can use one variable predict anotherA linear model intercept slope test slope differs zeroWe create linear models R lm() function use anova() assess slope coefficientWe can use linear regression four assumptions met:\ndata linear\nResiduals normally distributed\nResiduals correlated fitted values\nsingle point large influence linear model\ndata linearResiduals normally distributedResiduals correlated fitted valuesNo single point large influence linear modelWe can use resid_panel() get diagnostic plots R, help evaluate assumptions","code":""}]
